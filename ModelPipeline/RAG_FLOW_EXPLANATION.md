# RAG System Flow: What Gets Sent to the LLM?

## Quick Answer

**YES** - The LLM receives **only the retrieved hit-related sentences**, NOT the entire S3 vector embeddings database.

The S3 vector embeddings are used **only for retrieval/search** to find relevant sentences. Only the top-K most relevant sentences (after expansion and deduplication) are sent to the LLM as context.

---

## Detailed Flow

### Step 1: Query Processing & Variant Generation

```
User Query: "What was NVIDIA's revenue in 2023?"
    â†“
Entity Extraction (companies, years, sections)
    â†“
Query Embedding (1024-d vector)
    â†“
[Optional] Variant Generation (semantic rephrasings)
    - Variant 1: "NVIDIA's 2023 revenue"
    - Variant 2: "How much revenue did NVIDIA generate in 2023?"
    - Variant 3: "NVIDIA 2023 financial revenue"
```

### Step 2: S3 Vector Retrieval

The retriever searches the S3 Vectors database (which contains embeddings for **all sentences** in the SEC filings):

```
Base Query â†’ S3 Vectors Search
    â”œâ”€ Filtered Search (strict filters: company, year, section)
    â”‚   â””â”€ Returns: ~30 top-K hits (sentence-level)
    â”‚
    â””â”€ Global Search (relaxed filters: company, year >= threshold)
        â””â”€ Returns: ~20 top-K hits (sentence-level)

[If variants enabled]
Variant 1 â†’ Filtered Search â†’ ~15 hits
Variant 2 â†’ Filtered Search â†’ ~15 hits
Variant 3 â†’ Filtered Search â†’ ~15 hits
```

**Key Point**: The S3 Vectors database contains embeddings for **millions of sentences**, but we only retrieve the **top-K most similar** (typically 30-50 hits total after deduplication).

### Step 3: Deduplication

```
Raw Hits: ~80-100 hits (from base + variants)
    â†“
Deduplicate by (sentence_id, embedding_id)
    â†“
Union Hits: ~30-50 unique sentence hits
```

### Step 4: Sentence Expansion (Window Context)

Each retrieved hit is expanded to include neighboring sentences for context:

```
30 S3Hits (core hits)
    â†“
Window Expansion (Â±3 sentences around each hit)
    â†“
~210 SentenceRecords (with overlapping windows)
    â†“
Deduplicate by sentenceID (keep best evidence)
    â†“
~140 unique SentenceRecords
```

**Example**:
- Core hit: Sentence #45 about "revenue growth"
- Expanded: Sentences #42, #43, #44, #45, #46, #47, #48
- This provides context around the hit

### Step 5: Context Assembly

The unique sentences are sorted and formatted:

```
~140 unique SentenceRecords
    â†“
Sort by: (company, year ASC, section, doc, position)
    â†“
Format with headers:
    === [NVDA] NVIDIA CORP | FY 2023 | Doc: nvda_2023_10k | Item 7: MD&A ===
    
    Sentence text 1
    
    Sentence text 2
    
    ...
```

### Step 6: Final Context Sent to LLM

The assembled context string is combined with KPI data and sent to the LLM:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KPI SNAPSHOT                                    â”‚
â”‚ (Structured financial metrics)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NARRATIVE CONTEXT - SEC FILINGS                â”‚
â”‚                                                 â”‚
â”‚ === [NVDA] NVIDIA CORP | FY 2023 | ... ===     â”‚
â”‚                                                 â”‚
â”‚ [~140 sentences from retrieved hits]          â”‚
â”‚                                                 â”‚
â”‚ === [MSFT] MICROSOFT CORP | FY 2023 | ... ===  â”‚
â”‚                                                 â”‚
â”‚ [More sentences...]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER QUESTION                                   â”‚
â”‚                                                 â”‚
â”‚ What was NVIDIA's revenue in 2023?             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Points

### âœ… What the LLM Receives

1. **Only retrieved sentences** (~140 sentences after expansion)
2. **Sorted and formatted** with provenance headers
3. **Combined with KPI data** (structured metrics)
4. **User's original question** at the end

### âŒ What the LLM Does NOT Receive

1. **NOT the entire S3 vector database** (millions of sentences)
2. **NOT all embeddings** (only the top-K most similar)
3. **NOT raw vector data** (only the sentence text)

### ğŸ” How Retrieval Works

- **S3 Vectors** = Search engine (like Google)
- **Embeddings** = Index for semantic search
- **Retrieval** = Find top-K most similar sentences
- **Expansion** = Add neighboring sentences for context
- **Assembly** = Format for LLM consumption

---

## Example: Query Flow

**Query**: "What were NVIDIA's revenue trends from 2018 to 2020?"

1. **Retrieval**: Searches S3 Vectors â†’ finds ~40 sentence hits about NVIDIA revenue 2018-2020
2. **Expansion**: Each hit expands to Â±3 neighbors â†’ ~280 sentences
3. **Deduplication**: Overlapping windows merged â†’ ~150 unique sentences
4. **Assembly**: Sorted by year, formatted with headers
5. **LLM Context**: ~150 sentences sent to LLM (NOT millions)

**Result**: LLM generates answer based on these ~150 relevant sentences, not the entire knowledge base.

---

## Configuration Parameters

You can control what gets retrieved:

- `top_k_filtered`: Max hits from filtered search (default: ~30)
- `top_k_global`: Max hits from global search (default: ~20)
- `top_k_filtered_variants`: Max hits per variant (default: ~15)
- `window_size`: Sentences around each hit (default: Â±3)
- `enable_variants`: Use query variants (default: false)
- `enable_global`: Use global search (default: true)

These parameters control the **size of context** sent to the LLM, not the size of the searchable database.

---

## Summary

- **S3 Vector Embeddings**: Used for **retrieval/search** only
- **LLM Context**: Contains **only the top-K retrieved sentences** (after expansion)
- **Not included**: The entire knowledge base or all embeddings
- **Result**: Efficient, focused context that's relevant to the query

The RAG system is like a librarian: it searches the entire library (S3 vectors) but only brings you the most relevant books (retrieved sentences) to answer your question.

