{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65996277",
   "metadata": {},
   "source": [
    "## RAG Pipeline Modules Notes\n",
    "- Config + wiring, Query → embedding + Supply1 / Supply 2.\n",
    "- Embedding → S3 Vectors search wrapper: search_s3_vectors(query_emb, top_k) -> pl.DataFrame\n",
    "- Reranker stub: rerank(query, candidates_df) -> candidates_df_sorted.\n",
    "- Evidence → answer synthesis\n",
    "- Eval harness integration\n",
    "- Gold set → RAG pipeline → Measurable metrics.\n",
    "\n",
    "**Potential Ideas**:\n",
    "1. Flow below or a better version.\n",
    "    ```\n",
    "    User question → embed → ANN search → 100 hits\n",
    "            ↓\n",
    "    Hybrid/BM25 filter (optional)\n",
    "            ↓\n",
    "    Cross-encoder reranker → 10 best passages\n",
    "            ↓\n",
    "    Context-window expansion ±N ?? ( Rethink. Contexts might vary. )\n",
    "            ↓\n",
    "    LLM prompt assembly\n",
    "    ```\n",
    "2. config-driven crews, prompt versioning, auto-refinement, and central logging. possibly.\n",
    "\n",
    "**Notes**:\n",
    "- LLMs are extremely sensitive to the quality and diversity of retrieved context.\n",
    "- Top-10 good chunks → coherent answer.\n",
    "- Top-10 semantically-close-but-irrelevant → hallucination.\n",
    "\n",
    "**Ideas for Sub Directory/Package designs**:\n",
    "- RouterPipeline/ – the 1/0 decision maker and entity detection.\n",
    "- AnalyticsPipeline/ – the “true” KPI / JSON layer you highlighted in yellow.\n",
    "- RAGPipeline/ – embedding, S3 Vectors, similarity + re-rank.\n",
    "- LLMPipeline/ – synthesis / explanation crews.\n",
    "\n",
    "\n",
    "A) “Range queries (2020–2023) should be basic — can S3 Vectors do this?”\n",
    "- Yes. Use numeric report_year and filter with $gte/$lte. Example: {\"$and\": [{\"cik_int\": 320193}, {\"report_year\": {\"$gte\": 2020, \"$lte\": 2023}}]}. \n",
    "\n",
    "B) “Bring the most related sentences around this query, top-k around this query.”\n",
    "- S3 Vectors gives the top-K hits. “Around this query” in the semantic sense is what ANN already returns; \n",
    "- “around each hit” in the document context sense (±N sentences, same paragraph/section) is your post-processing. That is not performed inside S3 Vectors. \n",
    "\n",
    "**About SOTA CE Bi-Encoders and Hybrid stacks**: \n",
    "\n",
    "C) State-of-the-art and practical models\n",
    "- Cross-encoders: ms-marco-MiniLM-L-6-v2, bge-reranker-large, monoT5-3B, E5-Reranker.\n",
    "- Bi-encoders: bge-base, e5-large-v2.\n",
    "- Hybrid stacks: hybrid_fusion(bm25, dense, crossencoder) for robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7e93b",
   "metadata": {},
   "source": [
    "## Recap on N4 operations and index.\n",
    "\n",
    "- single, sentence-level vector index where each vector record corresponds to exactly one row from lean vector table.\n",
    "- index granularity is determined entirely by what you PUT - there is no automatic aggregation or hierarchy creation by S3 Vectors.\n",
    "- each vector has: a unique key (sentenceID), 1024-d embedding, and metadata (cik_int, report_year, section_code, etc.)\n",
    "\n",
    "**Does S3 Vectors Create Internal Shards or Sub-Indexes?**\n",
    "- system handles internal partitioning to scale, but you never manage or address these shards separately.\n",
    "- AWS handles internal distribution across their infrastructure\n",
    "- interact with one logical index via the API\n",
    "- no user-visible \"shards\" or \"secondary indexes\" on metadata columns like report_year or section_code \n",
    "\n",
    "#### Prod level- How many logical indexes to create? and necessity.\n",
    "- Global index: one index for all companies/years\n",
    "- Company-specific: one index per CIK\n",
    "- Hybrid: one index per (company, year) combination\n",
    "\n",
    "### About indexing recommendations and strategies:\n",
    "1. metadata filters to simulate local recall: narrow by cik_int, report_year, section_code.\n",
    "2. current index as the “global” FinRAG index: narrow by cik_int, report_year, section_code, etc.\n",
    "3. multiple indexes when you have a real need: sentence-level vs section-level embeddings.\n",
    "4. S3 Vectors doesn’t solve rerank/context logic. \"Use it as the ANN + metadata filter engine.\n",
    "- one S3 Vectors index is enough to support “global vs local recall + rerank” design\n",
    "- not for the reason my intuition is currently worried about (i.e., “my open-regime hit rates are low, so I must need another index”).\n",
    "- low score is more about evaluation regime and chunk granularity than about “not enough indexes”.\n",
    "\n",
    "### same index, different “lens” via filters and rerank\n",
    "5. K and reranking strategy\n",
    "   1. Local: k=50, strong focus; rerank within small candidate set.\n",
    "   2. Global: k=200+, over-fetch; use reranker + windowing to find cross-company narrative.\n",
    "\n",
    "### low hits on open-regime queries explained:\n",
    "- low open-regime hit@k does not mean “we need more indexes.\n",
    "- it means: “global search over a large, heterogeneous space is hard; we need rerankers, filters, and careful eval definitions.”\n",
    "\n",
    "### multi-level / (Sentence vs Section/Paragraph):\n",
    "- Document → Section → Paragraph → Sentence.\n",
    "- we store and embed only sentence-level units, we do context reconstruction (±2 windows) after retrieval possibly.\n",
    "- RAG questions are analytical, narrative, risk-based, trend-based, cross-company\n",
    "  \n",
    "### when we need multi level and dataset regrain, reset:\n",
    "- narrative spans 10–20+ sentences\n",
    "- topic is clear but any single sentence is weak: embeddings of individual sentences may scatter in vector space\n",
    "- need “semantic neighborhoods” at the section level: entire MD&A section, entire Risk Factor section\n",
    "- cross-year narrative shifts across LARGE-SCALE chunks, docs, sections. \n",
    "- fine grained atomic things: cant do discourse-level meaning, topic continuity, or section-level intention.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66632a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step plan maps like this:\n",
    "\n",
    "1. Intent classification – dev coding\n",
    "2. Entity extraction – dev coding\n",
    "3. Query embedding – Bedrock/cohere does this. v4.\n",
    "    - Result: Send Supply1 (Analytical Metric Info from Metric tables), Supply2 (User Query as Cohere Embedding) & Extracted parameters\n",
    "\n",
    "4. Metadata filter construction – dev coding, using S3 Vectors filter syntax\n",
    "5. S3 Vectors ANN search – this is exactly the QueryVectors call on index, with filters\n",
    "6. Context window expansion – dev coding, using the meta parquet / sentenceID neighbors\n",
    "7. Fetch full text – dev coding, join on sentenceID\n",
    "8. Hybrid reranking – dev coding, external reranker\n",
    "\n",
    "9. Deduplication – dev coding - check on this, study.\n",
    "\n",
    "10. Context assembly – dev coding\n",
    "11. Prepare Config Prompt, LLM synthesis – dev coding + Bedrock Claude calls.\n",
    "12. Evaluation harness integration – dev coding - on P3 Gold. or P2 Gold.\n",
    "13. MLFlow or ELK integration - logging - dev coding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d068e",
   "metadata": {},
   "source": [
    "### Study on BM25, Needs, Complexity - Planning it for Phase 2, Not now:\n",
    "\n",
    "- local BM25 index is very feasible on a laptop with 16–32 GB RAM:\n",
    "  - Index build time: on the order of seconds to a minute once, if you use a simple Python BM25 library and store just tokenized sentences.\n",
    "  - RAM footprint: think a few hundred MB, not tens of GB, if you keep representation lean (IDs + postings + term frequencies).\n",
    "  - Query latency: typically tens of milliseconds per query for 50–100 hits.\n",
    "\n",
    "- Easiest route is a small Python library (rank_bm25, whoosh, or even elasticlunr-style).\n",
    "    - Load sentenceID + sentence from local parquet cache.\n",
    "    - Tokenize once.\n",
    "    - Build a BM25 index object.\n",
    "    - Serialize it to disk (pickle) so it doesn’t rebuild each run.\n",
    "- Query side is trivial: bm25.get_top_n(query_tokens, sentences, n=70) plus mapping back to sentenceID.\n",
    "\n",
    "### Minimal Enhancement Plans:\n",
    "\n",
    "- Window expansion / Multi-hop: for each hit sentenceID, pull neighbors ±N and group into paragraphs/sections. That’s just Polars + sentenceID prefix logic. It massively improves context without new infra.\n",
    "- Semantic query variants into S3 Vectors: cheap to implement:\n",
    "  - Call LLM once to generate 2–4 paraphrases of the user question.\n",
    "  - Embed each and query S3 Vectors with k=30 for each variant.\n",
    "  - Union by sentenceID, then window-expand + rerank.\n",
    "\n",
    "- “phase 2” bump: BM25 hybrid layer, Facet filters “generator” ?\n",
    "\n",
    "### About routing:\n",
    "- When should we trigger global tricks? (Routing).\n",
    "   - not wise to run all “global recall” machinery for every query. You want a small routing layer that decides:\n",
    "   - “This is a narrow, local question → just use filters + one S3 call.”\n",
    "   - “This is a broad analytic question → fire semantic variants, possibly BM25.”\n",
    "\n",
    "- Decision tree ideas:\n",
    "  - Detect explicit CIK/year in the query or in UI parameters.\n",
    "  - If query has company but no year → “broad within company”.\n",
    "  - If query has neither company nor year → global analytic.\n",
    "  - small “mode selector” in your query module: mode = local | company_broad | global. (just basic ideas for now.)\n",
    "\n",
    "- Window expansion (multi-hop): not optional if we care about narrative quality.\n",
    "- Without windowing, the LLM sees disjoint, contextless fragments. even if we use semantic variants, we still need window expansion to build coherent context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b35ebb",
   "metadata": {},
   "source": [
    "### First concrete plans:\n",
    "\n",
    "- Implement window expansion cleanly and make it a reusable helper: given a set of sentenceIDs, return expanded, deduped, grouped contexts.\n",
    "  \n",
    "- Implement semantic query variants → S3 Vectors:\n",
    "    - N = 2–3 variants per query.\n",
    "    - Each variant uses the same filter logic you already have.\n",
    "    - Merge candidates, window-expand, rerank.\n",
    "- \n",
    "- Add a tiny mode switch:\n",
    "    - mode=\"local\" → no variants, strong filters.\n",
    "    - mode=\"global\" → variants, broader filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46884f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f906e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7dad0bf",
   "metadata": {},
   "source": [
    "### Big pic analysis on Vishak's Code:\n",
    "- metric_pipeline/ → query → [ticker, year, metrics] → look up numeric values in a precomputed JSON table → return structured results and a formatted string.\n",
    "- synthesis_pipeline/ → orchestrator coordinates:\n",
    "  - metric pipeline (if available), Cohere-via-Bedrock query embedding, vector search (currently mocked), Claude-via-Bedrock answer generation.\n",
    "\n",
    "### Metric_pipeline.py:\n",
    "- Decides if a query is “numeric metric-style”.\n",
    "- Extracts ticker, year, metric names (with fuzzy typo handling).\n",
    "- Looks them up in a metrics JSON table.\n",
    "- Returns structured results and/or a formatted English string.\n",
    "\n",
    "**COMPANY_TO_TICKER:**\n",
    "- Lowercase company names → ticker codes, e.g. \"nvidia\": \"NVDA\", \"apple\": \"AAPL\".\n",
    "**METRIC_MAPPINGS:**\n",
    "- Natural-language phrases → canonical metric codes \n",
    "  - (e.g. \"income_stmt_Revenue\", \"balance_sheet_Total Assets\", \"Return on Assets (ROA) %\").\n",
    "- \"revenue\", \"sales\", \"total revenue\", \"total sales\" → income_stmt_Revenue.\n",
    "- \"net income\", \"profit\", \"earnings\", \"bottom line\" → income_stmt_Net Income.\n",
    "- Asset, liability, equity, cash flow, gross profit, operating expenses, COGS, interest expense, tax, ROA, margins.\n",
    "**METRIC_KEYWORDS:**\n",
    "- A smaller set of generic terms: revenue, income, profit, loss, assets, liabilities, equity, debt, cash flow, ....\n",
    "**QUANTITATIVE_INDICATORS:**\n",
    "- Phrases like \"how much\", \"what is\", \"total\", \"amount\", used to detect “how much is X” style queries.\n",
    "\n",
    "### FilterExtractor – turning text into filters:\n",
    "- src/filter_extractor.py. uses COMPANY_TO_TICKER and METRIC_MAPPINGS, plus a built-in fuzzy matcher.\n",
    "- simple_fuzzy_match(word, choices, threshold=0.8): no external fuzzywuzzy dependency; just internal edit distance.\n",
    "- FilterExtractor.extract(query: str) -> Dict:  \n",
    "  - ` { \"ticker\": ..., \"year\": ..., \"metrics\": [...], \"query\": original_query, \"confidence\": 0.0–1.0 }`\n",
    "- _extract_ticker(query): tries to find any 2–5 letter token and treat it as a ticker (uppercasing it), filtering out obvious English words (IN, IT, ARE, THE, etc.).\n",
    "- _extract_year(query): Regex for 4-digit years in 1900–2030, returns int or None.\n",
    "- _extract_metrics(query): Lowercases query, Sorts METRIC_MAPPINGS by key length to match longer phrases first.\n",
    "- Confidence: simply counts how many of [ticker, year, metrics] were found, divides by 3.\n",
    "\n",
    "\n",
    "### Analysis Takeaways:\n",
    "- externalize domain knowledge into metric_mappings.py instead of hard-coding strings all over.\n",
    "- implement fuzzy matching with a clean, local Levenshtein distance so you don’t drag in extra dependencies.\n",
    "- have a clear, narrow interface: FilterExtractor.extract(query) and MetricPipeline.process(query); the latter returns a dict with a data field that you can pass directly into the LLM side.\n",
    "- 'needs_metric_layer' is, in practice, a very usable first-pass intent classifier for “should I even bother hitting numeric tables”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc841a97",
   "metadata": {},
   "source": [
    "## Extracting full constants or better keys:\n",
    "\n",
    "#### p3_candidates_kpi.json → numeric / KPI / MD&A metric language\n",
    "#### p3_candidates_risk.json → risk / uncertainty / regulatory / liquidity language\n",
    "\n",
    "aiming for two upgraded vocabularies:\n",
    "Metric vocabulary / mappings\n",
    "- richer set of natural phrases to map into METRIC_MAPPINGS and METRIC_KEYWORDS\n",
    "Section / scope vocabulary\n",
    "- specific sections: MD&A, Risk Factors, Liquidity & Capital Resources, Notes, etc.,\n",
    "- risk vs MD&A vs Business vs Footnotes cues in the sentence text, not just in your hard section_name field.\n",
    "- for FX/supply chain/COVID/defs: goldp3_v5_def_verify_candidates.json (to extend into more exotic cues).\n",
    "\n",
    "## If you see:\n",
    "    - a quantitative phrase, plus at least one metric keyword, plus a valid ticker / company from dim_companies, a year or range,\n",
    "    - high confidence that: you must invoke the numeric KPI layer (Supply1) and not just RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7de2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KPI tokens] rows=6426 -> d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\analysis_exports\\goldp3_views\\analysis_keywords_kpi_tokens.json\n",
      "[KPI tokens by label] rows=12258 -> d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\analysis_exports\\goldp3_views\\analysis_keywords_kpi_by_label.json\n",
      "[Risk tokens] rows=6639 -> d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\analysis_exports\\goldp3_views\\analysis_keywords_risk_tokens.json\n",
      "[Risk tokens by topic] rows=19162 -> d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\analysis_exports\\goldp3_views\\analysis_keywords_risk_by_topic.json\n",
      "[DEBUG] Sections path: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\dimensions\\finrag_dim_sec_sections.parquet\n",
      "[DEBUG] Sections schema: Schema([('sec_item_canonical', String), ('hf_section_code', Int32), ('api_section_code', String), ('section_code', String), ('section_name', String), ('section_description', String), ('section_category', String), ('part_number', Int32), ('priority', String), ('has_sub_items', Boolean)])\n",
      "[Section tokens by section] rows=209 -> d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\analysis_exports\\goldp3_views\\analysis_keywords_section_by_section.json\n",
      "[Section tokens global] rows=152 -> d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\analysis_exports\\goldp3_views\\analysis_keywords_section_global.json\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Setup: paths\n",
    "# ---------------------------------------------------------\n",
    "project_root = Path.cwd().parent  # assuming notebook is in .../finrag_ml_tg1/notebooks/*\n",
    "export_root   = project_root / \"data_cache\" / \"analysis_exports\" / \"goldp3_views\"\n",
    "\n",
    "kpi_path      = export_root / \"p3_candidates_kpi.json\"\n",
    "risk_path     = export_root / \"p3_candidates_risk.json\"\n",
    "\n",
    "# ModelPipeline\\finrag_ml_tg1\\data_cache\\dimensions\\finrag_dim_sec_sections.parquet\n",
    "sections_path = project_root / \"data_cache\" / \"dimensions\" / \"finrag_dim_sec_sections.parquet\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helper: tokenize sentences into lowercase tokens\n",
    "# - strips non-alphanumerics to space\n",
    "# - splits on space\n",
    "# - filters out very short tokens (len <= 2)\n",
    "# ---------------------------------------------------------\n",
    "def tokenize_column(df: pl.DataFrame, text_col: str, out_col: str = \"token\") -> pl.DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .with_columns(\n",
    "            pl.col(text_col)\n",
    "              .str.to_lowercase()\n",
    "              .str.replace_all(r\"[^a-z0-9]+\", \" \")\n",
    "              .str.strip_chars()\n",
    "              .alias(\"_norm_text\")\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(\"_norm_text\").str.split(\" \").alias(\"_tokens\")\n",
    "        )\n",
    "        # FIX: avoid selecting _tokens twice\n",
    "        .select(\n",
    "            pl.all().exclude([\"_norm_text\", \"_tokens\"]),\n",
    "            pl.col(\"_tokens\"),\n",
    "        )\n",
    "        .explode(\"_tokens\")\n",
    "        .rename({\"_tokens\": out_col})\n",
    "        .filter(pl.col(out_col).str.len_chars() > 2)\n",
    "        .filter(pl.col(out_col) != \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) KPI candidates: distinct tokens + counts\n",
    "# =========================================================\n",
    "df_kpi = pl.read_json(kpi_path)\n",
    "\n",
    "# tokenize based on sentence_text\n",
    "df_kpi_tokens = tokenize_column(df_kpi, text_col=\"sentence_text\", out_col=\"token\")\n",
    "\n",
    "# overall distinct KPI tokens with counts\n",
    "df_kpi_token_counts = (\n",
    "    df_kpi_tokens\n",
    "    .group_by(\"token\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"freq\"),\n",
    "        pl.n_unique(\"sentenceID\").alias(\"num_sentences\"),\n",
    "        pl.n_unique(\"kpi_label\").alias(\"num_kpi_labels\"),\n",
    "    ])\n",
    "    .sort(\"freq\", descending=True)\n",
    ")\n",
    "\n",
    "# optional: save to JSON for inspection\n",
    "kpi_tokens_out = export_root / \"analysis_keywords_kpi_tokens.json\"\n",
    "df_kpi_token_counts.write_json(kpi_tokens_out)\n",
    "print(f\"[KPI tokens] rows={df_kpi_token_counts.height} -> {kpi_tokens_out}\")\n",
    "\n",
    "# per-KPI label token counts (words used around each KPI label)\n",
    "df_kpi_by_label = (\n",
    "    df_kpi_tokens\n",
    "    .group_by([\"kpi_label\", \"token\"])\n",
    "    .agg([\n",
    "        pl.len().alias(\"freq\"),\n",
    "        pl.n_unique(\"sentenceID\").alias(\"num_sentences\"),\n",
    "    ])\n",
    "    .sort([\"kpi_label\", \"freq\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "kpi_tokens_by_label_out = export_root / \"analysis_keywords_kpi_by_label.json\"\n",
    "df_kpi_by_label.write_json(kpi_tokens_by_label_out)\n",
    "print(f\"[KPI tokens by label] rows={df_kpi_by_label.height} -> {kpi_tokens_by_label_out}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) Risk candidates: distinct tokens + counts\n",
    "# =========================================================\n",
    "df_risk = pl.read_json(risk_path)\n",
    "\n",
    "df_risk_tokens = tokenize_column(df_risk, text_col=\"sentence_text\", out_col=\"token\")\n",
    "\n",
    "df_risk_token_counts = (\n",
    "    df_risk_tokens\n",
    "    .group_by(\"token\")\n",
    "    .agg([\n",
    "        pl.len().alias(\"freq\"),\n",
    "        pl.n_unique(\"sentenceID\").alias(\"num_sentences\"),\n",
    "        pl.n_unique(\"risk_topic\").alias(\"num_risk_topics\"),\n",
    "    ])\n",
    "    .sort(\"freq\", descending=True)\n",
    ")\n",
    "\n",
    "risk_tokens_out = export_root / \"analysis_keywords_risk_tokens.json\"\n",
    "df_risk_token_counts.write_json(risk_tokens_out)\n",
    "print(f\"[Risk tokens] rows={df_risk_token_counts.height} -> {risk_tokens_out}\")\n",
    "\n",
    "# per-risk-topic tokens\n",
    "df_risk_by_topic = (\n",
    "    df_risk_tokens\n",
    "    .group_by([\"risk_topic\", \"token\"])\n",
    "    .agg([\n",
    "        pl.len().alias(\"freq\"),\n",
    "        pl.n_unique(\"sentenceID\").alias(\"num_sentences\"),\n",
    "    ])\n",
    "    .sort([\"risk_topic\", \"freq\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "risk_tokens_by_topic_out = export_root / \"analysis_keywords_risk_by_topic.json\"\n",
    "df_risk_by_topic.write_json(risk_tokens_by_topic_out)\n",
    "print(f\"[Risk tokens by topic] rows={df_risk_by_topic.height} -> {risk_tokens_by_topic_out}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) Section-dimension keywords:\n",
    "#    - from section_name + section_description\n",
    "# =========================================================\n",
    "\n",
    "# =========================================================\n",
    "# 3) Section-dimension keywords:\n",
    "#    - from section_name + section_description\n",
    "# =========================================================\n",
    "import json\n",
    "\n",
    "# If your sections dimension is stored as PARQUET (as per your debug path):\n",
    "sections_path = project_root / \"data_cache\" / \"dimensions\" / \"finrag_dim_sec_sections.parquet\"\n",
    "\n",
    "print(f\"[DEBUG] Sections path: {sections_path}\")\n",
    "\n",
    "# Read depending on extension\n",
    "if sections_path.suffix == \".parquet\":\n",
    "    df_sections = pl.read_parquet(sections_path)\n",
    "elif sections_path.suffix == \".json\":\n",
    "    with open(sections_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sections_raw = json.load(f)  # JSON array -> list[dict]\n",
    "    df_sections = pl.DataFrame(sections_raw)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported sections file type: {sections_path}\")\n",
    "\n",
    "# Basic schema sanity check (optional but helpful once)\n",
    "print(\"[DEBUG] Sections schema:\", df_sections.schema)\n",
    "\n",
    "# unify text: section_name + section_description\n",
    "df_sec_text = df_sections.with_columns(\n",
    "    (pl.col(\"section_name\") + pl.lit(\" \") + pl.col(\"section_description\"))\n",
    "      .str.to_lowercase()\n",
    "      .alias(\"sec_text\")\n",
    ")\n",
    "\n",
    "# tokenize\n",
    "df_sec_tokens = tokenize_column(df_sec_text, text_col=\"sec_text\", out_col=\"token\")\n",
    "\n",
    "# distinct tokens per section_code (for token -> section mapping)\n",
    "df_sec_token_by_section = (\n",
    "    df_sec_tokens\n",
    "    .group_by([\"section_code\", \"sec_item_canonical\", \"section_category\", \"token\"])\n",
    "    .agg([\n",
    "        pl.len().alias(\"freq\"),\n",
    "    ])\n",
    "    .sort([\"section_code\", \"freq\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "sec_tokens_by_section_out = export_root / \"analysis_keywords_section_by_section.json\"\n",
    "df_sec_token_by_section.write_json(sec_tokens_by_section_out)\n",
    "print(f\"[Section tokens by section] rows={df_sec_token_by_section.height} -> {sec_tokens_by_section_out}\")\n",
    "\n",
    "# global distinct section tokens\n",
    "df_sec_token_global = (\n",
    "    df_sec_tokens\n",
    "    .group_by(\"token\")\n",
    "    .agg([\n",
    "        pl.n_unique(\"section_code\").alias(\"num_sections\"),\n",
    "        pl.n_unique(\"section_category\").alias(\"num_categories\"),\n",
    "    ])\n",
    "    .sort(\"num_sections\", descending=True)\n",
    ")\n",
    "\n",
    "sec_tokens_global_out = export_root / \"analysis_keywords_section_global.json\"\n",
    "df_sec_token_global.write_json(sec_tokens_global_out)\n",
    "print(f\"[Section tokens global] rows={df_sec_token_global.height} -> {sec_tokens_global_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "PART_III_ITEM_10 : ['governance', 'officers', 'directors', 'executive', 'structure', 'board', 'corporate', 'item', 'and']\n",
    "PART_III_ITEM_11 : ['executive', 'compensation', 'analysis', 'pay', 'grants', 'discussion', 'and', 'equity', 'item']\n",
    "PART_III_ITEM_12 : ['ownership', 'management', 'security', 'share', 'plans', 'equity', 'major', 'owners', 'directors', 'certain', 'and', 'item', 'beneficial', 'officers', 'shareholders']\n",
    "PART_III_ITEM_13 : ['related', 'transactions', 'and', 'relationships', 'item', 'certain', 'party', 'independence', 'director']\n",
    "PART_III_ITEM_14 : ['fees', 'and', 'audit', 'services', 'auditor', 'accountant', 'principal', 'non', 'item']\n",
    "PART_II_ITEM_5 : ['stock', 'market', 'repurchases', 'item', 'dividends', 'equity', 'for', 'registrant', 'performance', 'share', 'data', 'common']\n",
    "PART_II_ITEM_6 : ['financial', 'deprecated', 'selected', 'filers', '2020', 'item', 'after', 'reserved', 'data', 'summary', 'multi', 'year', 'for', 'smaller']\n",
    "PART_II_ITEM_7 : ['analysis', 'trends', 'results', 'critical', 'item', 'outlook', 'revenue', 'operating', 'discussion', 'capital', 'liquidity', 'management', 'resources']\n",
    "PART_II_ITEM_7A : ['risk', 'disclosures', 'item', 'about', 'interest', 'qualitative', 'foreign', 'commodity', 'currency', 'rate', 'exposures', 'market', 'and', 'quantitative']\n",
    "PART_II_ITEM_8 : ['statements', 'financial', 'cash', 'notes', 'data', 'income', 'statement', 'balance', 'critical', 'supplementary', 'audited', 'flow', 'and', 'sheet', 'item']\n",
    "PART_II_ITEM_9 : ['disagreements', 'with', 'auditors', 'changes', 'item', 'accountants', 'and', 'accounting', 'none', 'usually', 'rare']\n",
    "PART_II_ITEM_9A : ['controls', 'internal', 'item', 'effectiveness', 'and', 'sox', 'icfr', '404', 'disclosures', 'procedures']\n",
    "PART_II_ITEM_9B : ['information', 'item', 'other', 'disclosed', 'not', 'previously', 'material']\n",
    "PART_IV_ITEM_15 : ['financial', 'exhibits', 'schedules', 'list', 'exhibit', 'index', 'statement', 'item', 'and']\n",
    "PART_IV_ITEM_16 : ['summary', 'item', 'form', 'used', 'optional', 'rarely']\n",
    "PART_I_ITEM_1 : ['business', 'item', 'competition', 'services', 'segments', 'products', 'description', 'strategy', 'market']\n",
    "PART_I_ITEM_1A : ['factors', 'risk', 'risks', 'statement', 'item', 'looking', 'uncertainties', 'forward']\n",
    "PART_I_ITEM_1B : ['staff', 'comments', 'none', 'unresolved', 'outstanding', 'sec', 'item', 'typically']\n",
    "PART_I_ITEM_2 : ['properties', 'holdings', 'item', 'physical', 'facilities', 'real', 'estate']\n",
    "PART_I_ITEM_3 : ['legal', 'proceedings', 'item', 'litigation', 'material', 'matters']\n",
    "PART_I_ITEM_4 : ['safety', 'mine', 'companies', 'disclosures', 'only', 'mining', 'statistics', 'item', 'for']\n",
    "\n",
    "\n",
    "=== topic liquidity_credit ===\n",
    "['liquidity', 'financial', 'cash', 'results', 'default', 'operations', 'have', 'other', 'capital', 'company', 'credit', 'business', 'which', 'debt', 'condition', 'risk', 'adversely', 'adverse', 'insurance', 'including', 'any', 'flows', 'ability', 'obligations', 'affect', 'result', 'material', 'mbia', 'mortgage', 'effect', 'risks', 'additional', 'market', 'terms', 'investment', 'investments', 'subsidiaries', 'future', 'interest', 'impact']\n",
    "\n",
    "=== topic regulatory ===\n",
    "['regulatory', 'other', 'business', 'including', 'which', 'products', 'regulation', 'have', 'requirements', 'financial', 'subject', 'data', 'operations', 'result', 'changes', 'legal', 'insurance', 'laws', 'any', 'regulations', 'compliance', 'company', 'adverse', 'new', 'costs', 'actions', 'results', 'ability', 'risks', 'services', 'impact', 'related', 'litigation', 'affect', 'adversely', 'risk', 'practices', 'additional', 'government', 'capital']\n",
    "\n",
    "=== topic market_competitive ===\n",
    "['volatility', 'price', 'market', 'business', 'other', 'which', 'stock', 'have', 'financial', 'including', 'products', 'markets', 'company', 'economic', 'competitors', 'securities', 'impact', 'future', 'affect', 'result', 'factors', 'adverse', 'results', 'credit', 'risk', 'competition', 'litigation', 'customers', 'global', 'changes', 'experience', 'conditions', 'adversely', 'has', 'any', 'operations', 'significant', 'risks', 'insurance', 'ability']\n",
    "\n",
    "=== topic operational_supply_chain ===\n",
    "['operations', 'business', 'results', 'have', 'financial', 'adverse', 'condition', 'material', 'effect', 'which', 'other', 'impact', 'any', 'disruption', 'including', 'result', 'adversely', 'risk', 'affect', 'company', 'systems', 'products', 'insurance', 'future', 'risks', 'security', 'significant', 'litigation', 'services', 'loss', 'factors', 'pandemic', 'costs', 'information', 'mortgage', 'failure', 'materially', 'ability', 'events', 'operating']\n",
    "\n",
    "=== topic cybersecurity_tech ===\n",
    "['security', 'data', 'information', 'cyber', 'other', 'attacks', 'systems', 'access', 'have', 'third', 'unauthorized', 'breaches', 'including', 'threats', 'measures', 'breach', 'business', 'which', 'parties', 'service', 'use', 'networks', 'party', 'continue', 'customers', 'user', 'subject', 'products', 'incidents', 'technology', 'risks', 'services', 'result', 'financial', 'cybersecurity', 'failures', 'detect', 'significant', 'privacy', 'any']\n",
    "\n",
    "=== topic legal_ip_litigation ===\n",
    "['litigation', 'claims', 'other', 'have', 'business', 'which', 'products', 'property', 'result', 'intellectual', 'risk', 'company', 'adverse', 'rights', 'certain', 'subject', 'financial', 'any', 'related', 'significant', 'costs', 'adversely', 'against', 'impact', 'legal', 'including', 'future', 'liability', 'management', 'class', 'insurance', 'investigations', 'stock', 'security', 'proceedings', 'also', 'product', 'material', 'data', 'there']\n",
    "\n",
    "=== topic general_risk ===\n",
    "['risk', 'adverse', 'have', 'security', 'other', 'business', 'which', 'company', 'financial', 'products', 'insurance', 'including', 'investment', 'factors', 'data', 'impact', 'credit', 'mortgage', 'result', 'any', 'capital', 'effect', 'information', 'material', 'significant', 'services', 'changes', 'funds', 'risks', 'tax', 'results', 'subject', 'ability', 'systems', 'pandemic', 'certain', 'new', 'also', 'future', 'economic']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba383cb0",
   "metadata": {},
   "source": [
    "### Three axes of filtering / extraction:\n",
    "- Section axis – “Where in the 10-K?” (ITEM_1, ITEM_1A, ITEM_7, ITEM_8, …)\n",
    "- Metric axis – “Which KPI / numeric concept?” (revenue, net income, cash from ops, debt, etc.)\n",
    "- Topic axis – “What semantic theme?” (liquidity risk, regulatory risk, FX risk, supply chain disruption, etc.)\n",
    "- (The risk-topic keywords live on axis 3. They’re useful even if you don’t use them as hard filters, and they’re not “competing” with section keywords.)\n",
    "  \n",
    "### Under the hood:\n",
    "- Use analysis_keywords_kpi_by_label.json to ground revenue / net income / operating income / cash-from-ops / EPS synonyms in what actually appears in filings.\n",
    "- Use analysis_keywords_section_by_section.json and the dim-sections table to decide which natural phrases should map to PART_I_ITEM_1, PART_II_ITEM_7, PART_II_ITEM_8, etc.\n",
    "- Use analysis_keywords_risk_by_topic.json to build a clean RISK_TOPIC_KEYWORDS map keyed by your topic labels (liquidity_credit, regulatory, …).\n",
    "\n",
    "### Even if the initial v1 filter only uses section_name and company/year, have multiple hooks:\n",
    "- Risk language is not confined to ITEM 1A\n",
    "- Companies often talk about “liquidity risk”, “market risk”, “regulatory risk” in MD&A, 7A, or even notes.\n",
    "- Risk-topic keywords can fire even in ITEM_7 or ITEM_7A, not just ITEM_1A.\n",
    "- extra semantic axis to be usable: Section alone (ITEM_1A) wasn’t enough: it’s all risk, but what risk? \n",
    "\n",
    "### MD&A vs Risk Factors:\n",
    "- MD&A is a free-form narrative about operations, trends, liquidity, capital resources, strategic commentary, expenses, revenue drivers, restructuring, acquisitions, product updates, macro commentary, guidance, and infinitely more.\n",
    "- MD&A ≠ a small, enumerable ontology.\n",
    "- Risk ≈ a taxonomizable, clusterable ontology.\n",
    "- Risk-topic keywords matter because “risk” is the single section of a 10-K where semantic themes are highly interpretable, highly clustered, and highly stable across companies and across years.\n",
    "- MD&A and other sections are not like that — they are semantically broad, multi-topic, and do not benefit from manual keyword catalogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fa19d",
   "metadata": {},
   "source": [
    "## entity_adapter/\n",
    "\n",
    "1. models.py – shared dataclasses / plain structs describing the outputs.\n",
    "2. *_universe.py – loaders that talk to dim / parquet / JSON tables and build in-memory lookup structures.\n",
    "3. *_extractor.py – pure logic: “given a query string and a universe, return lists of entities”.\n",
    "4. entity_adapter.py – one high-level class that orchestrates the four extractors and produces a single, clean “parsed query” object.\n",
    "\n",
    "#### Conceptually, QueryEntities (in models.py) would hold:\n",
    "\n",
    "- ciks: list of ints (primary thing S3 will use)\n",
    "- tickers: list of strings\n",
    "- company_names: list of canonical company names\n",
    "- years: list of ints (unique, unsorted or sorted)\n",
    "- sections: list of standard section codes (e.g. PART_I_ITEM_1A, PART_II_ITEM_7, not “md&a”)\n",
    "- metrics: list of canonical metric IDs from metric_mapping_v2 / metrics dim\n",
    "- (later) risk_topics, mdna_topics if ever use them\n",
    "- maybe a debug / raw field if to stash intermediate info\n",
    "\n",
    "#### you have to think of this: central abstraction: “parsed query entities” should entity-level information needed for downstream filters.\n",
    "\n",
    "#### Final Flow:\n",
    "- Create a CompanyUniverse (load dim file).\n",
    "- Instantiate CompanyExtractor, YearExtractor, SectionExtractor, MetricExtractorAdapter.\n",
    "- Expose a method along the lines of:\n",
    "  - “given a query string, run all extractors, combine outputs, produce a QueryEntities object”.\n",
    "\n",
    "#### Extractor details: CompanyExtractor:\n",
    "- dynamic alias index,\n",
    "- exact alias matching (apple → Apple Inc.),\n",
    "- conservative fuzzy alias matching (microsft → MICROSOFT CORP),\n",
    "- handling of possessives like \"nvidia's\", \"apple's\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c88fe",
   "metadata": {},
   "source": [
    "#### Section Intelligence:\n",
    "\n",
    "- Embeddings + ANN already solve 70–80% of “section intelligence”\n",
    "- Embedding vectors already contain: semantics, sentence meaning, section tone, ling features.\n",
    "- RAG sys rely on vector retrieval to pick the right sections implicitly..\n",
    "\n",
    "#### User side filter triggers:\n",
    "- They return the correct canonical filter value → ITEM_7, ITEM_1A, etc.\n",
    "- They accept many NL variations → “Item 7”, “ITEM-7”, “item_7”, “7A”, “7-A”, etc.\n",
    "- They allow fuzzy phrasing → “management discussion”, “ops results”, “liquidity resources”.\n",
    "- They map all SECTION_KEYWORDS → sec_item_canonical, not section_code.\n",
    "\n",
    "#### Patterns to cover:\n",
    "- PATTERNS: item 7 item-7 item_7 item7 \"7\" \"7A\" \"item 7a\" \"item7A\" \"ITEM 7A\"\n",
    "```\n",
    "| Input         | Group | Canonical |\n",
    "| ------------- | ----- | --------- |\n",
    "| “item 7”      | 7     | ITEM_7    |\n",
    "| “item-7”      | 7     | ITEM_7    |\n",
    "| “item 7A”     | 7A    | ITEM_7A   |\n",
    "| “item-1a”     | 1A    | ITEM_1A   |\n",
    "| “item 8”      | 8     | ITEM_8    |\n",
    "| “7A”          | 7A    | ITEM_7A   |\n",
    "| “see item 12” | 12    | ITEM_12   |\n",
    "| “ITEM_13”     | 13    | ITEM_13   |\n",
    "```\n",
    "\n",
    "\n",
    "### Plan:\n",
    "- Step 1: keyword phase : Pull all ITEM_* from SECTION_KEYWORDS_v2\n",
    "- Step 2: regex phase : Extract any explicit item mention → ITEM_#\n",
    "- Step 3: semantic signals : If risk topic is present → add ITEM_1A\n",
    "    - If financial metrics present → add ITEM_7 and ITEM_8\n",
    "    - If cash flow metrics → add ITEM_7 and ITEM_8\n",
    "    - If query metric is only revenue → either ITEM_7 OR rely solely on ANN (both OK)\n",
    "- Step 4: default : If empty → use ITEM_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0164d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 4-12+ design ideas, Retrieval Spine Designs:\n",
    "\n",
    "- design the whole retrieval spine now, with concrete decisions about:\n",
    "  - local vs “global” retrieval, how semantic variants fit in, what S3 hits look like for us, \n",
    "  - and how that flows into windowing, full text, rerank, dedup, assembly.\n",
    "\n",
    "#### Ground facts\n",
    "- Index is 1024-d Cohere v4 (“bedrock_cohere_v4_1024d_…”) and S3 Vectors index finrag-sentence-fact-embed-1024d has dimension = 1024.\n",
    "- All doc embeddings were created with output_dimension=1024.\n",
    "- Query side is now also Cohere v4 via Bedrock, explicitly forcing output_dimension = cfg.dimensions = 1024, so we are geometrically aligned.\n",
    "- S3 Vectors QueryVectors response (from AWS docs):\n",
    "```\n",
    "{\n",
    "  \"vectors\": [\n",
    "    {\n",
    "      \"key\": \"string\",\n",
    "      \"data\": { ... },          // only if returnMetadata / returnData\n",
    "      \"distance\": number,       // only if returnDistance = true\n",
    "      \"metadata\": { ... }       // our fields live here\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "- querying, we will call query_vectors with: `returnDistance=True, returnMetadata=True`\n",
    "\n",
    "#### ! Filtering design:\n",
    "- Local (filtered) retrieval: S3 query with strong metadata filters - cik_int, report_year, sec_item_canonical\n",
    "- Company-global retrieval: Relax years but keep company when we have it. replace exact year set with report_year >= recent_years_threshold. \n",
    "- Truly global retrieval: only makes sense when: no company was detected, or you explicitly want multi-company analytic behavior.\n",
    "- If query has companies:\n",
    "  - Filtered call: strong local {cik_int ∈ C, report_year ∈ Y}.\n",
    "  - Global call: company-global {cik_int ∈ C, report_year >= threshold}.\n",
    "- **So we never ignore company in v1; “global” is “global in time”, not “global across issuers”.**\n",
    "- If query has no companies:\n",
    "  - Filtered call: whatever filters we can build (years / sections).\n",
    "  - Global call: truly global with recency only.\n",
    "\n",
    "#### Semantic variants: filters or open?\n",
    "- Purpose of variants = phrase coverage, not “turn the retriever into random noise”. \n",
    "- Variants should use the same filters as the base query !!\n",
    "- **different embeddings for the same conceptual question, not a different scope.**\n",
    "  - Base embedding: filtered call, global call\n",
    "  - variant embedding: filtered call only\n",
    "- Base global call already gives cross-year / cross-company support.\n",
    "- Variants are mainly for “did the embedding miss some synonyms?” within the intended scope; filtered calls capture that well.\n",
    "- Doing global+variants everywhere multiplies noise and rerank effort for relatively small marginal gain.\n",
    "- **add toggles for:** enable_global: bool (default True), enable_variants: bool (default False initially).\n",
    "---\n",
    "\n",
    "### Files, modules etc:\n",
    "- map steps 4–10 to concrete modules under rag_modules_src/rag_pipeline/:\n",
    "  - metadata_filters.py – Step 4: build S3 Vectors filter JSON(s) from EntityExtractionResult.\n",
    "  - s3_retriever.py – Step 5: call S3 Vectors (filtered + global, base + variants) and produce structured hits.\n",
    "  - context_window.py – Step 6: map sentence-level hits to windowed spans.\n",
    "  - text_fetcher.py – Step 7: join spans to text (Stage1/Stage2 parquet).\n",
    "  - reranker.py – Step 8: hybrid reranking (MVP simple).\n",
    "  - deduplicator.py – Step 9: drop duplicates/boilerplate.\n",
    "  - context_assembler.py – Step 10: final prompt-ready context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef20ec",
   "metadata": {},
   "source": [
    "---\n",
    "### Thoughts on DEDUPS:\n",
    "\n",
    "1. duplication is almost guaranteed once you move past “one ANN call → one sentence”:\n",
    "2. hit the same logical sentence or paragraph via different paths: filtered vs global S3 Vectors calls or variants.\n",
    "3. obvious repetition from SEC boilerplate: forward-looking statements, safe-harbor disclaimers\n",
    "4. Context budget: repeated text eats tokens and shrinks the room for genuinely diverse evidence.\n",
    "5. Answer quality: Claude gets a distorted signal because one paragraph is over-represented compared with others; it tends to over-anchor on boilerplate and repeat it back.\n",
    "6. Evaluation noise: your P3 gold harness will see apparent “multiple supporting passages” that are actually the same paragraph duplicated through different retrieval paths; that muddies recall/precision diagnostics.\n",
    "---\n",
    "\n",
    "1. Do all the complex stuff first: filtered + global + semantic variants + window expansion + reranking.\n",
    "2. Before truncating to the top N chunks for the prompt, run a dedup pass that: \n",
    "   1. canonical key per chunk, (cik_int, report_year, sec_item_canonical, start_sentence_id, end_sentence_id) if operating at spans or windowing.\n",
    "   2. normalized_text (lowercased, stripped, maybe trimmed to 1–2k chars) if you operate pure text-wise. (not recommended)\n",
    "3. single highest-score version of any chunk sharing that key. (??)\n",
    "4. sits naturally right at the end of your retriever module, just before you hand rag_context to the LLM prompt builder.\n",
    "---\n",
    "\n",
    "1. BedrockClient is a focused wrapper around Claude on Bedrock: Owns the bedrock-runtime client and model_id, max_tokens, temperature.\n",
    "2. pure string prompt from: user_query, a compact analytical string (metric pipeline output), a flat list of RAG chunks (rag_context), each being a dict with keys like text, company, year, section, similarity_score.\n",
    "3. QueryOrchestrator is the high-level “glue”: \n",
    "   1. self.metric_pipeline (from metric_pipeline/src/pipeline.py) to run KPI extraction over your Stage 2/warehouse tables.\n",
    "   2. self.embedder (currently wired to the old QueryEmbedder that takes (query, input_type) and hits Bedrock for embeddings).\n",
    "   3. self.rag_search (intended to be a S3 Vectors search client, but currently None with a mock branch in _search_documents).\n",
    "   4. self.llm_client as a BedrockClient instance.\n",
    "4. Extract S3 filters from the analytic string in _extract_s3_filters.. (??)\n",
    "5. final answer via _generate_response.\n",
    "\n",
    "--- \n",
    "#### orchestrator already has slots for:\n",
    "- “Step 3: query embedding” → _generate_embedding.\n",
    "- “Step 4–5: metadata filters + S3 Vectors ANN” → _extract_s3_filters + _search_documents.\n",
    "- “Step 10: context assembly + LLM synthesis” → _generate_response + BedrockClient._build_prompt.\n",
    "\n",
    "\n",
    "```\n",
    "Dedup policy (MVP): Primary key: (cik_int, report_year, sec_item_canonical, sentenceID) → keep highest score_final.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66a51f",
   "metadata": {},
   "source": [
    "---\n",
    "## H level plan - 1117, Quick:\n",
    "\n",
    "````\n",
    "EntityExtractionResult\n",
    "  ↓\n",
    "[Step 4] MetadataFilterBuilder → {filtered_filters, global_filters}\n",
    "  ↓\n",
    "[Step 5] S3VectorsRetriever → RetrievalBundle\n",
    "         ├─ base_embedding: filtered_call + global_call\n",
    "         └─ variant_embeddings: filtered_call only (if enabled)\n",
    "  ↓\n",
    "         Deduplicate by sentence_id → union_hits (List[S3Hit])\n",
    "         Minimal state, early reduction, clear early semantics, and- cheaper to deduplicate @sentenceID here. \n",
    "         Second deduplication will happen later.\n",
    "  ↓\n",
    "[Step 6] ContextWindowExpander → List[ContextSpan]\n",
    "         (±N sentence window, merge overlaps)\n",
    "  ↓\n",
    "[Step 7] TextFetcher → List[ContextBlock]\n",
    "         (join to Stage1/2 parquet for full text)\n",
    "  ↓\n",
    "[Step 8] HybridReranker → List[ContextBlock] (scored)\n",
    "         (ANN score + keyword overlap + source bonuses)\n",
    "  ↓\n",
    "[Step 9] BlockDeduplicator → List[ContextBlock] (cleaned)\n",
    "         Primary key: (cik_int, report_year, sec_item, sentenceID)\n",
    "         Keep highest score_final per key\n",
    "  ↓\n",
    "[Step 10] ContextAssembler → str (formatted context)\n",
    "          Format: === NVDA 2021 ITEM_7 ===\\n<text>\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384393c",
   "metadata": {},
   "source": [
    "### S3 Retriever Analysis:\n",
    "\n",
    "- **Evidence:**\n",
    "- All thresholds (0.0 → 0.5): 45 hits, NO rejections\n",
    "- Similarity range: [0.674, 0.737]\n",
    "- Mean: 0.693, Median: 0.687\n",
    "- Conclusion: Even the \"weakest\" hit (similarity=0.674) is actually quite strong. S3 Vectors' ANN algorithm already returns high-quality matches.\n",
    "- 64% of hits in 0.6-0.7 range\n",
    "- 36% of hits in 0.7-0.8 range\n",
    "- Zero hits below 0.6 similarity\n",
    "- Zero hits above 0.8 similarity\n",
    "- The 45th best result (0.674) is still semantically relevant\n",
    "- **no \"long tail\" of weak matches to filter out** AT ALL.\n",
    "\n",
    "\n",
    "### Rerank Corrections:\n",
    "- S3 returns 45 hits (filtered + global)\n",
    "- Window expansion: 45 hits × 7 sentences/window = 315 sentence lookups in Stage 2 meta\n",
    "- Text fetch: Materialize 315 full text blocks\n",
    "- Rerank: Score all 315 blocks\n",
    "- BAD DESIGN.\n",
    "- expensive I/O (315 parquet row fetches + string concatenations) on data you'll throw away.. \n",
    "- S3 returns 45 hits -> Rerank -> Take top 15-20 hits only -> Window expansion: 20 hits × 7 = 140 sentence lookups \n",
    "- Stage-2 dedup: Remove overlapping windows from the small set\n",
    "- Talk to LLM\n",
    "\n",
    "\n",
    "```\n",
    "# Step 4-5: Retrieval (DONE)!! YIPPIEIEIEE. \n",
    "filtered_hits, global_hits = s3_retriever.retrieve(...)\n",
    "union_hits = deduplicate_stage1(filtered_hits + global_hits)  # by (sentence_id, embedding_id)\n",
    "\n",
    "# Step 6: IMMEDIATE RERANKING (sentence-level)\n",
    "scored_hits = reranker.score_hits(query, union_hits)  \n",
    "# Input: List[S3Hit], Output: List[S3Hit] with .final_score\n",
    "\n",
    "# Step 7: TOP-K SELECTION\n",
    "top_hits = sorted(scored_hits, key=lambda h: h.final_score, reverse=True)[:config.top_k_for_expansion]\n",
    "# e.g., keep top 20 hits for window expansion\n",
    "\n",
    "# Step 8: WINDOW EXPANSION (only on survivors)\n",
    "spans = window_expander.expand(top_hits)\n",
    "# Creates ContextSpan objects with sentence ranges\n",
    "\n",
    "# Step 9: TEXT FETCH (only for expanded spans)\n",
    "blocks = text_fetcher.materialize_blocks(spans)\n",
    "# Joins to Stage 2 meta, concatenates text\n",
    "\n",
    "# Step 10: STAGE-2 DEDUPLICATION (remove overlapping windows)\n",
    "unique_blocks = deduplicator.dedup(blocks)\n",
    "# Key: (cik, year, section, min_sentence_id, max_sentence_id)\n",
    "\n",
    "# Step 11: FINAL SELECTION & ASSEMBLY\n",
    "final_blocks = unique_blocks[:config.max_context_blocks]  # e.g., top 10\n",
    "context_str = assembler.assemble(final_blocks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f875213",
   "metadata": {},
   "source": [
    "### Retrieval variants analysis.. :\n",
    "1. combining results from different retrieval contexts:\n",
    "    - Filtered hits: \"What's most similar within (NVDA, 2021-2023, ITEM_7)?\"\n",
    "    - Global hits: \"What's most similar within (NVDA, any year ≥2015)?\"\n",
    "    - Variant hits: \"What's most similar to a rephrased version of the query?\"\n",
    "```\n",
    "2. Filtered pool: 5,000 sentences (NVDA 2021-2023 ITEM_7)\n",
    "  → Top hit: distance=0.15 (very close match)\n",
    "    Global pool: 50,000 sentences (NVDA all years)\n",
    "  → Top hit: distance=0.25 (less close, but from larger pool)\n",
    "  \n",
    "  !! Filtered hit is closer (0.15 < 0.25). But global hit competed against 10x more candidates, hm?\n",
    "```\n",
    "3. S3 Vectors doesn't tell: \"This was rank 1 out of 5,000\" vs \"This was rank 1 out of 50,000\"\n",
    "4. Simple reranking would go bad: we need awareness. Provenance-Aware Scoring.. \"filtered hits match user intent by definition\".\n",
    "5. injecting domain knowledge ??\n",
    "6. Dense embeddings can retrieve semantically similar but lexically misaligned results; Lexical overlap catches this: \"Does this sentence actually SAY the words I care about?\"\n",
    "\n",
    "---\n",
    "### WINDOW EXPANSION- but with limits:\n",
    "- ! Take only top 20 BEFORE window expansion. top_hits = sorted_hits[:20]\n",
    "- Example: Total after union: 30 + 15 + 45 = 90 hits\n",
    "- After Stage-1 dedup: ~60 unique hits (assuming some overlap)\n",
    "- Assume 60, 60 hits × 7 sentences/window = 420 sentence lookups in Stage 2 meta parquet\n",
    "- most of those 60 hits won't make it to the final context anyway. !! expensive.\n",
    "- Set it high initially (e.g., 40-50) so you're not too aggressive\n",
    "\n",
    "\n",
    "### Note quick on who's doing the hit calling:\n",
    "```python\n",
    "class S3VectorsRetriever:\n",
    "    def retrieve(self, embeddings, entities, mode, filters):\n",
    "        # Call S3 for filtered\n",
    "        filtered_hits = self._query_s3(embedding, filtered_filters, top_k_filtered)\n",
    "        \n",
    "        # Call S3 for global\n",
    "        global_hits = self._query_s3(embedding, global_filters, top_k_global)\n",
    "        \n",
    "        # Call S3 for each variant\n",
    "        variant_hits = []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345b8d6",
   "metadata": {},
   "source": [
    "### Again, Embed things- Drop or not, TopK or not?\n",
    "\n",
    "```\n",
    "# Why this is WRONG:\n",
    "Filtered hit: distance=0.25 (from pool of 5,000 candidates)\n",
    "Global hit:   distance=0.20 (from pool of 50,000 candidates)\n",
    "```\n",
    "- Which is \"better\"?\n",
    "- Global has LOWER distance (0.20 < 0.25)\n",
    "- But filtered competed in a SMALLER pool (harder to rank high)\n",
    "- Distances are NOT comparable across different search spaces!\n",
    "\n",
    "1. S3 Vectors returns cosine distance relative to the query embedding, but:\n",
    "   - Pool size affects distance distribution:\n",
    "   - Small pool (5K): Top-30 distances might be [0.15, 0.30]\n",
    "   - Large pool (50K): Top-30 distances might be [0.20, 0.40]\n",
    "   - Same embedding, different distributions!\n",
    "\n",
    "2. Don't do minmax normalization or even scale normalization, despite how it might seem OK or standard. I mean, technically, we use standard scalers and such things in dataset handlers and use it on a continuous range of very fine specific float value, in-bit features in datasets. But here, especially for semantic and similarity scores, I'm not really sure what is the correct thing to do. ?? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a50bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b70525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48b134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2ab49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d04d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4baf7ae1",
   "metadata": {},
   "source": [
    "### High level End to End Skelly v1:\n",
    "\n",
    "```python\n",
    "\n",
    "# 1–3 already done:\n",
    "entities = adapter.extract(query)\n",
    "embedding = embedder.embed_query(query, entities)\n",
    "\n",
    "# 4. Filters\n",
    "local_filters = filter_builder.build_local_filters(entities)\n",
    "global_filters = filter_builder.build_global_filters(entities)\n",
    "\n",
    "# 5. Retrieval (base + variants later)\n",
    "bundle = retriever.retrieve_many(\n",
    "    embeddings=[embedding],      # later: [embedding] + variant_embeddings\n",
    "    entities=entities,\n",
    "    enable_global=True,\n",
    "    top_k_filtered=30,\n",
    "    top_k_global=15,\n",
    "    local_filters=local_filters,\n",
    "    global_filters=global_filters,\n",
    ")\n",
    "\n",
    "# 6. Context windows\n",
    "spans = window_expander.expand(bundle.union_hits)\n",
    "\n",
    "# 7. Full text\n",
    "blocks = text_fetcher.materialize_blocks(spans)\n",
    "\n",
    "# 8. Rerank\n",
    "reranked_blocks = reranker.rerank(query, blocks)\n",
    "\n",
    "# 9. Dedup\n",
    "clean_blocks = deduplicator.dedup(reranked_blocks)\n",
    "\n",
    "# 10. Assemble final context for LLM\n",
    "context_str = assembler.assemble(clean_blocks)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc440b7e",
   "metadata": {},
   "source": [
    "### Window Expansion + Hit Merging Logic -- Tracking IS true hit or not, etc.\n",
    "\n",
    "```python\n",
    "\n",
    "Hit A (pos=44, distance=0.18) → window [41, 42, 43, 44, 45, 46, 47]\n",
    "  Creates 7 rows:\n",
    "    sentenceID=\"doc_1A_0041\", is_core_hit=False, parent_hit_distance=0.18\n",
    "    sentenceID=\"doc_1A_0042\", is_core_hit=False, parent_hit_distance=0.18\n",
    "    sentenceID=\"doc_1A_0043\", is_core_hit=False, parent_hit_distance=0.18\n",
    "    sentenceID=\"doc_1A_0044\", is_core_hit=True,  parent_hit_distance=0.18  ← Core\n",
    "    sentenceID=\"doc_1A_0045\", is_core_hit=False, parent_hit_distance=0.18\n",
    "    sentenceID=\"doc_1A_0046\", is_core_hit=False, parent_hit_distance=0.18\n",
    "    sentenceID=\"doc_1A_0047\", is_core_hit=False, parent_hit_distance=0.18\n",
    "\n",
    "Hit B (pos=46, distance=0.22) → window [43, 44, 45, 46, 47, 48, 49]\n",
    "  Creates 7 rows:\n",
    "    sentenceID=\"doc_1A_0043\", is_core_hit=False, parent_hit_distance=0.22\n",
    "    sentenceID=\"doc_1A_0044\", is_core_hit=False, parent_hit_distance=0.22\n",
    "    sentenceID=\"doc_1A_0045\", is_core_hit=False, parent_hit_distance=0.22\n",
    "    sentenceID=\"doc_1A_0046\", is_core_hit=True,  parent_hit_distance=0.22  ← Core\n",
    "    sentenceID=\"doc_1A_0047\", is_core_hit=False, parent_hit_distance=0.22\n",
    "    sentenceID=\"doc_1A_0048\", is_core_hit=False, parent_hit_distance=0.22\n",
    "    sentenceID=\"doc_1A_0049\", is_core_hit=False, parent_hit_distance=0.22\n",
    "\n",
    "Total: 14 rows (with duplicates)\n",
    "\n",
    "Sentence \"doc_1A_0043\":\n",
    "  - From Hit A: distance=0.18\n",
    "  - From Hit B: distance=0.22\n",
    "  → Keep Hit A's version (0.18 < 0.22) ✅\n",
    "\n",
    "Sentence \"doc_1A_0044\":\n",
    "  - From Hit A: distance=0.18, is_core_hit=True\n",
    "  - From Hit B: distance=0.22, is_core_hit=False\n",
    "  → Keep Hit A's version ✅\n",
    "\n",
    "Sentence \"doc_1A_0046\":\n",
    "  - From Hit A: distance=0.18, is_core_hit=False\n",
    "  - From Hit B: distance=0.22, is_core_hit=True\n",
    "  → Keep Hit A's version (better score, even though B's core) ✅\n",
    "\n",
    "# After dedup, we have individual sentences\n",
    "# Group them into contiguous runs by (doc, section, sentence_pos)\n",
    "\n",
    "Sentences: [41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
    "           ↑_________________________↑  ↑______↑\n",
    "           Block 1: [41-47]              Block 2: [48-49]\n",
    "\n",
    "contiguous? Merge into ONE block: [41-49]\n",
    "```\n",
    "---\n",
    "### Algorithm Idea for now:\n",
    "\n",
    "```\n",
    "Step 6-7: ContextBuilder.build_blocks()\n",
    "  ↓\n",
    "  For each S3Hit:\n",
    "    1. Calculate window [pos-3, pos+3]\n",
    "    2. Fetch sentences from Stage 2 meta (by sentence_pos range)\n",
    "    3. For each sentence in window:\n",
    "       - Mark is_core_hit (True if sentence_id == hit.sentence_id)\n",
    "       - Tag with parent_hit_distance\n",
    "       - Tag with parent_hit_sources, parent_hit_variant_ids\n",
    "  ↓\n",
    "  Flatten to: List[SentenceRecord] (one row per sentence, many duplicates)\n",
    "\n",
    "Step 8: Sentence-Level Deduplication\n",
    "  ↓\n",
    "  Group by: (sentenceID, cik_int, report_year, section_name)\n",
    "  Keep: Best parent_hit_distance\n",
    "  Preserve: is_core_hit, sources, variant_ids\n",
    "  ↓\n",
    "  Result: List[SentenceRecord] (deduplicated, each sentence appears once)\n",
    "\n",
    "Step 9: Contiguous Grouping\n",
    "  ↓\n",
    "  Group by: (cik_int, report_year, section_name)\n",
    "  Within each group:\n",
    "    - Sort by sentence_pos\n",
    "    - Find contiguous runs (pos[i+1] == pos[i] + 1)\n",
    "    - Each run becomes a ContextBlock\n",
    "  ↓\n",
    "  Result: List[ContextBlock] (natural paragraphs)\n",
    "\n",
    "Step 10: Final Selection & Assembly\n",
    "  ↓\n",
    "  Sort blocks by: best sentence distance in block (or first core_hit distance)\n",
    "  Take top 10 blocks\n",
    "  Format for LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "# rag_pipeline/context_builder.py\n",
    "\n",
    "class ContextBuilder:\n",
    "    def build_blocks(self, hits):\n",
    "        # Step 1: Expand all hits to sentence records\n",
    "        sentence_records = self._expand_to_sentences(hits)\n",
    "        \n",
    "        # Step 2: Deduplicate at sentence level\n",
    "        unique_sentences = self._deduplicate_sentences(sentence_records)\n",
    "        \n",
    "        # Step 3: Group into contiguous blocks\n",
    "        blocks = self._group_into_blocks(unique_sentences)\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def _expand_to_sentences(self, hits):\n",
    "        # For each hit, create SentenceRecord for core + neighbors\n",
    "        \n",
    "    def _deduplicate_sentences(self, records):\n",
    "        # Group by sentenceID, keep best parent_hit_distance\n",
    "        \n",
    "    def _group_into_blocks(self, sentences):\n",
    "        # Find contiguous runs, create ContextBlocks\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7879293",
   "metadata": {},
   "source": [
    "### Score Purpose Analysis.\n",
    "```\n",
    "Steps 1-5: Score determines WHICH sentences to retrieve\n",
    "  ↓\n",
    "Step 6-7: Score determines WHICH version to keep during dedup\n",
    "  ↓\n",
    "Step 8-9: Score determines PRIORITY for topK selection\n",
    "  ↓\n",
    "Step 10: Score is IRRELEVANT - just format text cleanly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448c6e6",
   "metadata": {},
   "source": [
    "---\n",
    "### Functional order; Sorting; NO custom-forced hop-window context blocks.\n",
    "\n",
    "```\n",
    "Multiple hits create overlapping windows:\n",
    "  Hit A: [s1, s2, s3, s4, s10, s12, s20]\n",
    "  Hit B: [s20, s22, s28, s36]\n",
    "  Hit C: [s100, s22, s1, s2, s20]\n",
    "\n",
    "After expansion: ~210 SentenceRecords (with duplicates)\n",
    "  s1 appears 2 times (from Hit A and Hit C)\n",
    "  s2 appears 2 times (from Hit A and Hit C)\n",
    "  s20 appears 3 times (from A, B, C)\n",
    "  etc.\n",
    "\n",
    "After sentence dedup: ~140 unique sentences\n",
    "  s1 (once, best distance)\n",
    "  s2 (once, best distance)\n",
    "  s3 (once)\n",
    "  s4 (once)\n",
    "  s10 (once)\n",
    "  s12 (once)\n",
    "  ...\n",
    "\n",
    "# After sentence dedup, just SORT by functional order:\n",
    "sorted_sentences = sorted(unique_sentences, key=lambda s: (\n",
    "    s.cik_int,\n",
    "    s.report_year,      # Descending? (most recent first)\n",
    "    s.section_name,\n",
    "    s.doc_id,\n",
    "    s.sentence_pos      # Within document, natural order\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4439443",
   "metadata": {},
   "source": [
    "\n",
    "- Result: One clean ordered list !!\n",
    "- ALWAYSS FOCUS NATURAL ORDER\n",
    "\n",
    "### Ideal ASSEMBLY:\n",
    "```\n",
    "    \"\"\"\n",
    "    === NVIDIA CORP | 2020 | ITEM_1A ===\n",
    "    We face supply chain risks... (pos 45)\n",
    "    Competition in data centers... (pos 89)\n",
    "    Our manufacturing partners... (pos 102)\n",
    "\n",
    "    === MICROSOFT CORP | 2020 | ITEM_7 ===\n",
    "    Cloud revenue grew 50%... (pos 12)\n",
    "    Azure subscriptions increased... (pos 45)\n",
    "\n",
    "    === NVIDIA CORP | 2019 | ITEM_1A ===\n",
    "    GPU demand fluctuations... (pos 34)\n",
    "    Cryptocurrency mining impact... (pos 67)\n",
    "    \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e6014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRAG ML (Python 3.11)",
   "language": "python",
   "name": "finrag_ml_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
