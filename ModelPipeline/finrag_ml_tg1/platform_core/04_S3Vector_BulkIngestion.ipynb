{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04754a4e",
   "metadata": {},
   "source": [
    "## Operation 4!\n",
    "### - Pushes the Stage 3 table prepped for S3 vector bucket using `PutVector` API\n",
    "\n",
    "````\n",
    "Regular S3 Bucket: sentence-data-ingestion\n",
    "└── ML_EMBED_ASSETS/S3_VECTORS_STAGING/cohere_1024d/\n",
    "    └── finrag_embeddings_s3vectors_cohere_1024d.parquet (365 MB)\n",
    "\n",
    "Purpose: Staging/preparation area\n",
    "Type: Standard S3 storage\n",
    "\n",
    "S3 Vectors Bucket: <new-bucket-name>-vectors\n",
    "└── Managed by S3 Vectors service\n",
    "    └── Indexes (not raw parquet files)\n",
    "        └── finrag-cohere-1024d-index\n",
    "            ├── Optimized vector storage\n",
    "            ├── HNSW index structure\n",
    "            └── Metadata tables\n",
    "\n",
    "Purpose: Production vector search\n",
    "Type: S3 Vectors managed storage\n",
    "Cost: $0.30/million vectors/month + query costs\n",
    "````\n",
    "\n",
    "---\n",
    "#### Code Auth: Joel Markapudi. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52985f",
   "metadata": {},
   "source": [
    "\n",
    "### Flow the service expects:\n",
    "- S3 console screens you used only create the container for vectors (a vector bucket + a vector index) and set immutable index settings (dimension, distance metric, and any “non-filterable” metadata keys).\n",
    "- S3 Vectors is a vector store interface layered into S3, not a Parquet-aware service. It doesn’t parse tabular files to auto-create vectors; you must present already-embedded float32 arrays with keys and metadata via the S3 Vectors APIs.\n",
    "- Then you QueryVectors with a query embedding, optionally filtered by metadata. \n",
    "- S3 Vectors stores items of the form: { key, data: {float32: [..dim..]}, metadata: {k:v,..} }. You insert them with the PutVectors API. You then search with QueryVectors(topK, queryVector, filter=…).\n",
    "\n",
    "### Pricing Model\n",
    "- $0.30 per million vectors per month\n",
    "- 203,076 vectors = 0.203 million vectors\n",
    "- Monthly cost: 0.203 × $0.30 = $0.061/month (~$0.73/year)\n",
    "- Vector data: float32[1024] arrays (~4 KB each)\n",
    "- Filterable metadata: cik_int, report_year, section_name, sic, sentence_pos\n",
    "- Non-filterable metadata: sentenceID, embedding_id, section_sentence_count\n",
    "- Index structures: HNSW graph, metadata indexes\n",
    "- Redundancy: S3 standard durability (11 9's)\n",
    "\n",
    "```\n",
    "AWS charges:\n",
    "- PutVectors API calls: $0.00 (FREE)\n",
    "- Data ingress to S3: $0.00 (FREE - standard AWS policy)\n",
    "- Index building: $0.00 (FREE - handled automatically)\n",
    "Insertion:\n",
    "- 203,076 vectors ÷ 500 per batch = 407 API calls\n",
    "- Cost: $0.00\n",
    "Even at scale:\n",
    "- 71.8 million vectors ÷ 500 = 143,600 API calls, Free.\n",
    "```\n",
    "\n",
    "### Pricing: QueryVectors API (Search)\n",
    "\n",
    "$1.00 per million vector comparisons\n",
    "\n",
    "What's a \"vector comparison\"?\n",
    "- One distance calculation between query vector and stored vector\n",
    "``` \n",
    "Example query:\n",
    "topK = 10 (return 10 results)\n",
    "Actual comparisons: ~100-500 vectors (HNSW efficiency)\n",
    "Cost: 0.0001 to 0.0005 × $1.00 = $0.0001 to $0.0005 per query\n",
    "```\n",
    "- With metadata filters:\n",
    "- topK = 10, filter = {cik_int: 1318605}\n",
    "- Candidates: ~5,000 Tesla vectors (filtered first)\n",
    "- Comparisons: ~100-500 (HNSW on filtered set)\n",
    "- Cost: Same ~$0.0001 to $0.0005 per query\n",
    "\n",
    "### Safety Shrinking exists but never triggers.\n",
    "- Typical payload per vector:\n",
    "- Embedding: 1024 × 4 bytes (float32) = 4,096 bytes = 4 KB\n",
    "- Metadata: ~200 bytes (cik_int, report_year, short strings)\n",
    "- Total: ~4.3 KB per vector\n",
    "- 500 vectors × 4.3 KB = 2.15 MB ✓ (well under 20 MiB)\n",
    "- **arn**: arn:aws:s3vectors:us-east-1:729472661729:bucket/finrag-embeddings-s3vectors/index/finrag-sentence-fact-embed-1024d\n",
    "- **console**: console view is index-centric. see vector bucket and the vector indexes  created (name, ARN, creation time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f64da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config loader ready\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Now import works\n",
    "from loaders.ml_config_loader import MLConfig\n",
    "print(\"✓ Config loader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a63019",
   "metadata": {},
   "source": [
    "## Check Available methods on S3Vector's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d718ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available S3 Vectors methods:\n",
      "  - can_paginate\n",
      "  - close\n",
      "  - create_index\n",
      "  - create_vector_bucket\n",
      "  - delete_index\n",
      "  - delete_vector_bucket\n",
      "  - delete_vector_bucket_policy\n",
      "  - delete_vectors\n",
      "  - exceptions\n",
      "  - generate_presigned_url\n",
      "  - get_index\n",
      "  - get_paginator\n",
      "  - get_vector_bucket\n",
      "  - get_vector_bucket_policy\n",
      "  - get_vectors\n",
      "  - get_waiter\n",
      "  - list_indexes\n",
      "  - list_vector_buckets\n",
      "  - list_vectors\n",
      "  - meta\n",
      "  - put_vector_bucket_policy\n",
      "  - put_vectors\n",
      "  - query_vectors\n",
      "  - waiter_names\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3vectors = boto3.client('s3vectors', region_name='us-east-1')\n",
    "\n",
    "# List all available methods\n",
    "methods = [m for m in dir(s3vectors) if not m.startswith('_')]\n",
    "print(\"Available S3 Vectors methods:\")\n",
    "for m in sorted(methods):\n",
    "    print(f\"  - {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056084e3",
   "metadata": {},
   "source": [
    "## S3 Vectors get_index Response Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fe11bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ Found ModelPipeline via file path: D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "S3 VECTORS INDEX INSPECTOR\n",
      "======================================================================\n",
      "\n",
      "Vector Bucket: finrag-embeddings-s3vectors\n",
      "Index Name:    finrag-sentence-fact-embed-1024d\n",
      "Region:        us-east-1\n",
      "\n",
      "[Full Response]\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"afc8cd9a-6b24-4282-b45f-3002f7429b58\",\n",
      "    \"HostId\": \"\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 09 Dec 2025 16:52:38 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"481\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amz-request-id\": \"afc8cd9a-6b24-4282-b45f-3002f7429b58\",\n",
      "      \"access-control-allow-origin\": \"*\",\n",
      "      \"vary\": \"origin, access-control-request-method, access-control-request-headers\",\n",
      "      \"access-control-expose-headers\": \"*\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"index\": {\n",
      "    \"vectorBucketName\": \"finrag-embeddings-s3vectors\",\n",
      "    \"indexName\": \"finrag-sentence-fact-embed-1024d\",\n",
      "    \"indexArn\": \"arn:aws:s3vectors:us-east-1:729472661729:bucket/finrag-embeddings-s3vectors/index/finrag-sentence-fact-embed-1024d\",\n",
      "    \"creationTime\": \"2025-11-10 14:52:15-05:00\",\n",
      "    \"dataType\": \"float32\",\n",
      "    \"dimension\": 1024,\n",
      "    \"distanceMetric\": \"cosine\",\n",
      "    \"metadataConfiguration\": {\n",
      "      \"nonFilterableMetadataKeys\": [\n",
      "        \"sentenceID\",\n",
      "        \"embedding_id\",\n",
      "        \"section_sentence_count\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "[Top-level Keys]\n",
      "Available keys: ['ResponseMetadata', 'index']\n",
      "\n",
      "[Common Paths]\n",
      "✓ Found: response['index']\n",
      "  Keys: ['vectorBucketName', 'indexName', 'indexArn', 'creationTime', 'dataType', 'dimension', 'distanceMetric', 'metadataConfiguration']\n",
      "✓ Found: response['ResponseMetadata'] (AWS metadata)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Inspect S3 Vectors Index Configuration\n",
    "# ============================================================================\n",
    "\n",
    "from s3vectors_index_inspector import inspect_s3vectors_index, get_index_status\n",
    "\n",
    "# Option 1: Full verbose inspection (debugging)\n",
    "response = inspect_s3vectors_index(\n",
    "    vector_bucket=\"finrag-embeddings-s3vectors\",\n",
    "    index_name=\"finrag-sentence-fact-embed-1024d\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af59a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ Found ModelPipeline via file path: D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "Index Status: {'exists': True, 'dimension': 1024, 'metric': 'cosine', 'created': datetime.datetime(2025, 11, 10, 14, 52, 15, tzinfo=tzlocal()), 'arn': 'arn:aws:s3vectors:us-east-1:729472661729:bucket/finrag-embeddings-s3vectors/index/finrag-sentence-fact-embed-1024d', 'non_filterable_keys': ['sentenceID', 'embedding_id', 'section_sentence_count']}\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Quick status check (production)\n",
    "status = get_index_status()\n",
    "print(f\"Index Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3380d9",
   "metadata": {},
   "source": [
    "---\n",
    "## S3 VECTORS DATA INSERTION\n",
    "### Features: Preflight validation, retry logic, backoff, payload guards\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "814cf5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ Found ModelPipeline via file path: D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "S3 VECTORS BULK INSERTION PIPELINE\n",
      "======================================================================\n",
      "Provider: cohere_1024d\n",
      "Vector Bucket: finrag-embeddings-s3vectors\n",
      "Index Name: finrag-sentence-fact-embed-1024d\n",
      "Batch Size: 500 vectors/request\n",
      "\n",
      "[Preflight Check - Index Validation]\n",
      "  Index: finrag-sentence-fact-embed-1024d\n",
      "  ARN: arn:aws:s3vectors:us-east-1:729472661729:bucket/finrag-embeddings-s3vectors/index/finrag-sentence-fact-embed-1024d\n",
      "  Created: 2025-11-10 14:52:15-05:00\n",
      "  Dimension: 1024d\n",
      "  Data Type: float32\n",
      "  Distance Metric: cosine\n",
      "  Non-filterable Keys: {'section_sentence_count', 'sentenceID', 'embedding_id'}\n",
      "  ✓ Non-filterable keys match expected configuration\n",
      "  ✓ Preflight validation passed\n",
      "\n",
      "[Loading Stage 3 Data]\n",
      "  Source: finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "  Filter: ALL companies\n",
      "  Filter: Years in [2021, 2022, 2023, 2024, 2025, 2012, 2013, 2014]\n",
      "  ✓ Filtered Stage 3: 203,972 vectors\n",
      "\n",
      "[Batch Insertion with Retry Logic]\n",
      "  Total batches: 408\n",
      "  Retry strategy: Exponential backoff (max 7 attempts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting: 100%|██████████| 203972/203972 [49:49<00:00, 68.23vectors/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✓ INSERTION COMPLETE\n",
      "======================================================================\n",
      "  Vectors inserted: 203,972 / 203,972\n",
      "  Success rate: 100.0%\n",
      "  Duration: 2993.8s\n",
      "\n",
      "[Index Ready for Queries]\n",
      "  Query example:\n",
      "  response = s3vectors.query_vectors(\n",
      "      vectorBucketName='finrag-embeddings-s3vectors',\n",
      "      indexName='finrag-sentence-fact-embed-1024d',\n",
      "      queryVector=[...1024d embedding...],\n",
      "      topK=10,\n",
      "      filter={'cik_int': 1318605, 'report_year': 2021})\n",
      "======================================================================\n",
      "\n",
      "[Programmatic Summary]\n",
      "  {'provider': 'cohere_1024d', 'cik_filter': None, 'year_filter': [2021, 2022, 2023, 2024, 2025, 2012, 2013, 2014], 'total_in_stage3': 203972, 'filtered_count': 203972, 'total_inserted': 203972, 'failed_batches': 0, 'shrunk_batches': 0, 'success_rate': 100.0, 'duration_seconds': 2993.8055713176727, 'batches_processed': 408}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK 04: S3 VECTORS BULK INSERTION\n",
    "# ============================================================================\n",
    "\n",
    "# Execution Parameters\n",
    "INSERT_VECTORS = True\n",
    "PROVIDER = \"cohere_1024d\"\n",
    "\n",
    "# Test parameters: 1 company × 1 year\n",
    "# CIK_FILTER = [1318605]  # Tesla\n",
    "# YEAR_FILTER = [2016]     # Early year, smaller dataset\n",
    "\n",
    "# Incremental insertion filters\n",
    "CIK_FILTER = None  # None = all companies\n",
    "YEAR_FILTER = [2021, 2022, 2023, 2024, 2025, 2012, 2013, 2014]  # Or None for all\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "MAX_RETRIES = 7\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if INSERT_VECTORS:\n",
    "    from s3vectors_bulk_insertion import S3VectorsBulkInserter\n",
    "    \n",
    "    inserter = S3VectorsBulkInserter(\n",
    "        provider=PROVIDER,\n",
    "        cik_filter=CIK_FILTER,\n",
    "        year_filter=YEAR_FILTER,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_retries=MAX_RETRIES\n",
    "    )\n",
    "    \n",
    "    summary = inserter.run()\n",
    "    \n",
    "    # Summary already printed by class, but dict available for logging\n",
    "    print(f\"\\n[Programmatic Summary]\")\n",
    "    print(f\"  {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091bbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06fd3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "S3 VECTORS DATA INSERTION -  VERSION\n",
      "======================================================================\n",
      "Vector Bucket: finrag-embeddings-s3vectors\n",
      "Index Name: finrag-sentence-fact-embed-1024d\n",
      "Provider: cohere_1024d\n",
      "Batch Size: 500 (AWS max)\n",
      "\n",
      "[Preflight Check - Index Validation]\n",
      "  Index: finrag-sentence-fact-embed-1024d\n",
      "  ARN: arn:aws:s3vectors:us-east-1:729472661729:bucket/finrag-embeddings-s3vectors/index/finrag-sentence-fact-embed-1024d\n",
      "  Created: 2025-11-10 14:52:15-05:00\n",
      "  Dimension: 1024\n",
      "  Data Type: float32\n",
      "  Distance Metric: cosine\n",
      "  Non-filterable Keys: {'embedding_id', 'section_sentence_count', 'sentenceID'}\n",
      "  ✓ Non-filterable keys match expected configuration\n",
      "  ✓ Preflight validation passed\n",
      "\n",
      "[Loading Stage 3 Data]\n",
      "  Source: finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "  Loaded: 203,076 vectors\n",
      "\n",
      "[Inserting Vectors with Retry Logic]\n",
      "  Total batches: 407\n",
      "  Retry strategy: Exponential backoff (max 7 attempts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting: 100%|██████████| 203076/203076 [17:38<00:00, 191.93vectors/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✓ INSERTION COMPLETE\n",
      "======================================================================\n",
      "  Vectors inserted: 203,076\n",
      "  Success rate: 100.0%\n",
      "\n",
      "[Index Ready for Queries]\n",
      "  Query example:\n",
      "  response = s3vectors.query_vectors(\n",
      "      vectorBucketName='finrag-embeddings-s3vectors',\n",
      "      indexName='finrag-sentence-fact-embed-1024d',\n",
      "      queryVector=[...1024d embedding...],\n",
      "      topK=10,\n",
      "      filter={'cik_int': 1318605, 'report_year': 2020})\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# S3 VECTORS DATA INSERTION\n",
    "# Features: Preflight validation, retry logic, backoff, payload guards\n",
    "# ============================================================================\n",
    "\n",
    "# Execution Parameters\n",
    "INSERT_VECTORS_TO_S3VECTORS = True\n",
    "BATCH_SIZE = 500                       # Optimal (AWS max per request)\n",
    "S3VECTORS_PROVIDER = \"cohere_1024d\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'loaders'))\n",
    "\n",
    "from loaders.ml_config_loader import MLConfig\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "config = MLConfig()\n",
    "\n",
    "VECTOR_BUCKET = \"finrag-embeddings-s3vectors\"\n",
    "INDEX_NAME = \"finrag-sentence-fact-embed-1024d\"\n",
    "\n",
    "DIM = config.s3vectors_dimensions(S3VECTORS_PROVIDER)\n",
    "REGION = config.region\n",
    "\n",
    "s3vectors = boto3.client(\"s3vectors\", region_name=REGION,\n",
    "                         aws_access_key_id=config.aws_access_key,\n",
    "                         aws_secret_access_key=config.aws_secret_key)\n",
    "\n",
    "# ============================================================================\n",
    "#  FEATURE 1: PREFLIGHT INDEX VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def describe_and_validate_index(s3v_client, bucket, index, expected_dim):\n",
    "    \"\"\"\n",
    "    Validate index configuration matches our data before insertion\n",
    "    \n",
    "    Checks:\n",
    "    1. Dimension matches (1024)\n",
    "    2. Distance metric appropriate for embeddings\n",
    "    3. Non-filterable metadata keys declared correctly\n",
    "    \n",
    "    Prevents: Failed inserts due to config mismatch\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Preflight Check - Index Validation]\")\n",
    "    \n",
    "    try:\n",
    "        response = s3v_client.get_index(\n",
    "            vectorBucketName=bucket,\n",
    "            indexName=index\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"Failed to get index: {e}\")\n",
    "    \n",
    "    # Extract configuration (flat structure, lowercase keys)\n",
    "    index_data = response['index']\n",
    "    actual_dim = index_data['dimension']\n",
    "    distance_metric = index_data['distanceMetric']\n",
    "    data_type = index_data['dataType']\n",
    "    \n",
    "    # Get non-filterable metadata keys\n",
    "    meta_cfg = index_data.get('metadataConfiguration', {})\n",
    "    nonfilterable_keys = set(meta_cfg.get('nonFilterableMetadataKeys', []))\n",
    "    \n",
    "    print(f\"  Index: {index}\")\n",
    "    print(f\"  ARN: {index_data['indexArn']}\")\n",
    "    print(f\"  Created: {index_data['creationTime']}\")\n",
    "    print(f\"  Dimension: {actual_dim}\")\n",
    "    print(f\"  Data Type: {data_type}\")\n",
    "    print(f\"  Distance Metric: {distance_metric}\")\n",
    "    print(f\"  Non-filterable Keys: {nonfilterable_keys or 'None'}\")\n",
    "    \n",
    "    # Validate dimension\n",
    "    if actual_dim != expected_dim:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch!\\n\"\n",
    "            f\"  Index configured: {actual_dim}d\\n\"\n",
    "            f\"   embeddings: {expected_dim}d\\n\"\n",
    "            f\"  → Cannot insert mismatched dimensions\"\n",
    "        )\n",
    "    \n",
    "    # Validate data type\n",
    "    if data_type != 'float32':\n",
    "        print(f\"  ⚠️  Warning: Index expects '{data_type}', we're sending float32\")\n",
    "    \n",
    "    # Check distance metric (informational)\n",
    "    if distance_metric not in ['cosine', 'euclidean', 'dotProduct']:\n",
    "        print(f\"  ⚠️  Warning: Unusual distance metric '{distance_metric}'\")\n",
    "    \n",
    "    # Validate non-filterable metadata alignment\n",
    "    expected_nonfilterable = {'sentenceID', 'embedding_id', 'section_sentence_count'}\n",
    "    \n",
    "    if nonfilterable_keys:\n",
    "        if nonfilterable_keys == expected_nonfilterable:\n",
    "            print(f\"  ✓ Non-filterable keys match expected configuration\")\n",
    "        else:\n",
    "            missing = expected_nonfilterable - nonfilterable_keys\n",
    "            extra = nonfilterable_keys - expected_nonfilterable\n",
    "            if missing:\n",
    "                print(f\"  Note: Expected non-filterable keys not configured: {missing}\")\n",
    "            if extra:\n",
    "                print(f\"  Note: Additional non-filterable keys: {extra}\")\n",
    "    else:\n",
    "        print(f\"  Note: No non-filterable keys configured\")\n",
    "        print(f\"     All metadata will be filterable (may increase query costs)\")\n",
    "    \n",
    "    print(f\"  ✓ Preflight validation passed\")\n",
    "    \n",
    "    return distance_metric, nonfilterable_keys\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  FEATURE 2: PAYLOAD SIZE GUARD\n",
    "# ============================================================================\n",
    "\n",
    "MAX_PAYLOAD_SIZE = 20 * 1024 * 1024  # 20 MiB (AWS hard limit)\n",
    "\n",
    "def estimate_payload_size(vectors_batch):\n",
    "    \"\"\"\n",
    "    Estimate JSON payload size to stay under 20 MiB limit\n",
    "    \n",
    "    AWS limit: PutVectors request ≤ 20 MiB\n",
    "    Our typical: 500 vectors × 4KB each = ~2 MiB (safe)\n",
    "    \n",
    "    Risk: Large non-filterable metadata (text fields)\n",
    "    \"\"\"\n",
    "    # Quick estimation via JSON serialization\n",
    "    sample_payload = {\"vectors\": vectors_batch}\n",
    "    estimated_bytes = len(json.dumps(sample_payload, default=str))\n",
    "    return estimated_bytes\n",
    "\n",
    "def shrink_batch_if_needed(vectors_batch, max_size=MAX_PAYLOAD_SIZE):\n",
    "    \"\"\"\n",
    "    Dynamically reduce batch size if payload too large\n",
    "    \n",
    "    Returns: (potentially_smaller_batch, was_shrunk)\n",
    "    \"\"\"\n",
    "    size = estimate_payload_size(vectors_batch)\n",
    "    \n",
    "    if size <= max_size:\n",
    "        return vectors_batch, False\n",
    "    \n",
    "    # Binary search for acceptable batch size\n",
    "    print(f\"  Payload too large ({size/1024/1024:.1f} MB), shrinking batch...\")\n",
    "    \n",
    "    while size > max_size and len(vectors_batch) > 1:\n",
    "        vectors_batch = vectors_batch[:len(vectors_batch) // 2]\n",
    "        size = estimate_payload_size(vectors_batch)\n",
    "    \n",
    "    print(f\"     → Reduced to {len(vectors_batch)} vectors ({size/1024/1024:.1f} MB)\")\n",
    "    return vectors_batch, True\n",
    "\n",
    "# ============================================================================\n",
    "#  FEATURE 3: RESILIENT RETRY WITH EXPONENTIAL BACKOFF\n",
    "# ============================================================================\n",
    "\n",
    "def put_vectors_with_retry(s3v_client, bucket, index, vectors_batch, max_attempts=7):\n",
    "    \"\"\"\n",
    "    Insert vectors with intelligent retry logic\n",
    "    \n",
    "    Handles:\n",
    "    - 429 TooManyRequestsException (throttling) → Retry with backoff\n",
    "    - 503 ServiceUnavailableException (capacity) → Retry with backoff\n",
    "    - 4xx client errors → Fail fast (bad request, don't retry)\n",
    "    - 5xx server errors → Retry with backoff\n",
    "    \n",
    "    Backoff strategy:\n",
    "    - Initial: 0.5s + jitter\n",
    "    - Exponential: doubles each retry (capped at 4s)\n",
    "    - Jitter: ±0.25s randomization (prevents thundering herd)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Guard against oversized payloads\n",
    "    vectors_batch, was_shrunk = shrink_batch_if_needed(vectors_batch)\n",
    "    \n",
    "    attempt = 0\n",
    "    delay = 0.5  # Start with 500ms\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            # Attempt insertion\n",
    "            response = s3v_client.put_vectors(\n",
    "                vectorBucketName=bucket,\n",
    "                indexName=index,\n",
    "                vectors=vectors_batch\n",
    "            )\n",
    "            \n",
    "            # Success!\n",
    "            return len(vectors_batch), was_shrunk\n",
    "        \n",
    "        except ClientError as e:\n",
    "            error_code = e.response.get('Error', {}).get('Code', 'Unknown')\n",
    "            http_status = e.response.get('ResponseMetadata', {}).get('HTTPStatusCode', 500)\n",
    "            \n",
    "            # Classify error type\n",
    "            is_throttle = error_code == 'TooManyRequestsException'\n",
    "            is_capacity = error_code == 'ServiceUnavailableException'\n",
    "            is_client_error = 400 <= http_status < 500\n",
    "            is_server_error = http_status >= 500\n",
    "            \n",
    "            # Client errors (4xx) except throttle → Fail fast\n",
    "            if is_client_error and not is_throttle:\n",
    "                print(f\"\\n  ❌ Client error (non-retryable): {error_code}\")\n",
    "                print(f\"     Message: {e.response.get('Error', {}).get('Message', 'No details')}\")\n",
    "                raise  # Don't retry bad requests\n",
    "            \n",
    "            # Retryable errors: throttle, capacity, server errors\n",
    "            if is_throttle or is_capacity or is_server_error:\n",
    "                attempt += 1\n",
    "                \n",
    "                if attempt >= max_attempts:\n",
    "                    print(f\"\\n  ❌ Max retries ({max_attempts}) exceeded\")\n",
    "                    raise\n",
    "                \n",
    "                # Calculate backoff with jitter\n",
    "                jitter = random.random() * 0.25  # ±0.25s randomization\n",
    "                sleep_time = min(delay + jitter, 4.0)  # Cap at 4s\n",
    "                \n",
    "                retry_reason = \"throttled\" if is_throttle else (\"capacity\" if is_capacity else \"server error\")\n",
    "                print(f\"  ⏳ Retry {attempt}/{max_attempts} ({retry_reason}), waiting {sleep_time:.2f}s...\")\n",
    "                \n",
    "                time.sleep(sleep_time)\n",
    "                delay *= 2  # Exponential backoff\n",
    "                continue\n",
    "            \n",
    "            # Unknown error type\n",
    "            print(f\"\\n  Unknown error: {error_code}\")\n",
    "            raise\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Unexpected error\n",
    "            print(f\"\\n  ❌ Unexpected error: {type(e).__name__}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Should never reach here\n",
    "    raise RuntimeError(\"Retry logic error\")\n",
    "\n",
    "# ============================================================================\n",
    "#  FEATURE 4: IDEMPOTENCY LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "def log_batch_boundaries(batch_num, vectors_batch):\n",
    "    \"\"\"\n",
    "    Log first/last keys for idempotency tracking\n",
    "    \n",
    "    Use case: If insertion fails mid-run, you can resume from last successful batch\n",
    "    without re-inserting earlier vectors (S3 Vectors upserts on duplicate keys)\n",
    "    \"\"\"\n",
    "    if not vectors_batch:\n",
    "        return\n",
    "    \n",
    "    first_key = vectors_batch[0]['key']\n",
    "    last_key = vectors_batch[-1]['key']\n",
    "    \n",
    "    print(f\"  Batch {batch_num}: [{first_key}...{last_key}] ({len(vectors_batch)} vectors)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CORE HELPER FUNCTIONS \n",
    "# ============================================================================\n",
    "\n",
    "def validate_embedding(embedding, expected_dim):\n",
    "    \"\"\"Convert embedding to float32 and validate dimensions\"\"\"\n",
    "    arr = np.asarray(embedding, dtype=np.float32)\n",
    "    if arr.shape[0] != expected_dim:\n",
    "        raise ValueError(f\"Dimension mismatch: got {arr.shape[0]}, expected {expected_dim}\")\n",
    "    return arr.tolist()\n",
    "\n",
    "def batch_to_s3vectors_format(batch_df):\n",
    "    \"\"\"Convert Polars DataFrame batch to S3 Vectors PutVectors format\"\"\"\n",
    "    vectors = []\n",
    "    \n",
    "    for row in batch_df.iter_rows(named=True):\n",
    "        embedding_float32 = validate_embedding(row['embedding'], DIM)\n",
    "        \n",
    "        vector_item = {\n",
    "            \"key\": str(row['sentenceID_numsurrogate']),\n",
    "            \"data\": {\"float32\": embedding_float32},\n",
    "            \"metadata\": {\n",
    "                # Filterable (5 fields)\n",
    "                \"cik_int\": int(row['cik_int']),\n",
    "                \"report_year\": int(row['report_year']),\n",
    "                \"section_name\": str(row['section_name']),\n",
    "                \"sic\": str(row['sic']),\n",
    "                \"sentence_pos\": int(row['sentence_pos']),\n",
    "                \n",
    "                # Non-filterable (3 fields)\n",
    "                \"sentenceID\": str(row['sentenceID']),\n",
    "                \"embedding_id\": str(row['embedding_id']),\n",
    "                \"section_sentence_count\": int(row['section_sentence_count'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        vectors.append(vector_item)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN INSERTION LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "if INSERT_VECTORS_TO_S3VECTORS:\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"S3 VECTORS DATA INSERTION -  VERSION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Vector Bucket: {VECTOR_BUCKET}\")\n",
    "    print(f\"Index Name: {INDEX_NAME}\")\n",
    "    print(f\"Provider: {S3VECTORS_PROVIDER}\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE} (AWS max)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: PREFLIGHT VALIDATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    distance_metric, nonfilterable_keys = describe_and_validate_index(\n",
    "        s3vectors, VECTOR_BUCKET, INDEX_NAME, DIM\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: LOAD STAGE 3 DATA\n",
    "    # ========================================================================\n",
    "    \n",
    "    cache_path = config.get_s3vectors_cache_path(S3VECTORS_PROVIDER)\n",
    "    \n",
    "    if not cache_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Stage 3 data not found: {cache_path}\\n\"\n",
    "            f\"Run BUILD_S3VECTORS_TABLE=True first\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n[Loading Stage 3 Data]\")\n",
    "    print(f\"  Source: {cache_path.name}\")\n",
    "    df_stage3 = pl.read_parquet(cache_path)\n",
    "    total_rows = len(df_stage3)\n",
    "    print(f\"  Loaded: {total_rows:,} vectors\")\n",
    "    \n",
    "    # Validate schema\n",
    "    required_cols = ['sentenceID_numsurrogate', 'embedding', 'cik_int', 'report_year',\n",
    "                     'section_name', 'sic', 'sentence_pos', 'sentenceID',\n",
    "                     'embedding_id', 'section_sentence_count']\n",
    "    missing = [c for c in required_cols if c not in df_stage3.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: INSERT WITH RESILIENT RETRY\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n[Inserting Vectors with Retry Logic]\")\n",
    "    num_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    print(f\"  Total batches: {num_batches}\")\n",
    "    print(f\"  Retry strategy: Exponential backoff (max 7 attempts)\")\n",
    "    \n",
    "    total_inserted = 0\n",
    "    failed_batches = 0\n",
    "    shrunk_batches = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"Inserting\", unit=\"vectors\") as pbar:\n",
    "        for i in range(0, total_rows, BATCH_SIZE):\n",
    "            batch_num += 1\n",
    "            \n",
    "            # Get batch\n",
    "            batch_df = df_stage3[i:i+BATCH_SIZE]\n",
    "            vectors_batch = batch_to_s3vectors_format(batch_df)\n",
    "            \n",
    "            # Log batch boundaries (idempotency tracking)\n",
    "            # log_batch_boundaries(batch_num, vectors_batch)  # Uncomment for debugging\n",
    "            \n",
    "            try:\n",
    "                # Insert with retry\n",
    "                inserted, was_shrunk = put_vectors_with_retry(\n",
    "                    s3vectors, VECTOR_BUCKET, INDEX_NAME, vectors_batch\n",
    "                )\n",
    "                \n",
    "                total_inserted += inserted\n",
    "                if was_shrunk:\n",
    "                    shrunk_batches += 1\n",
    "                \n",
    "                pbar.update(inserted)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ❌ Batch {batch_num} failed permanently: {e}\")\n",
    "                failed_batches += 1\n",
    "                continue\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: SUMMARY\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✓ INSERTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Vectors inserted: {total_inserted:,}\")\n",
    "    print(f\"  Success rate: {total_inserted/total_rows*100:.1f}%\")\n",
    "    \n",
    "    if failed_batches > 0:\n",
    "        print(f\"  Failed batches: {failed_batches}/{num_batches}\")\n",
    "    \n",
    "    if shrunk_batches > 0:\n",
    "        print(f\"  Batches auto-shrunk (payload > 20MB): {shrunk_batches}\")\n",
    "    \n",
    "    print(f\"\\n[Index Ready for Queries]\")\n",
    "    print(f\"  Query example:\")\n",
    "    print(f\"  response = s3vectors.query_vectors(\")\n",
    "    print(f\"      vectorBucketName='{VECTOR_BUCKET}',\")\n",
    "    print(f\"      indexName='{INDEX_NAME}',\")\n",
    "    print(f\"      queryVector=[...1024d embedding...],\")\n",
    "    print(f\"      topK=10,\")\n",
    "    print(f\"      filter={{'cik_int': 1318605, 'report_year': 2020}})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"INSERT_VECTORS_TO_S3VECTORS = False. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75db556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae7d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4cf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070424ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b7625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51586a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a6f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746e211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f623602d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eba4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098299d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0a8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8914a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95b335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b524727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b693fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb701513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b30ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1af1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f91834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a0ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd191665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79dd046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744adce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b3449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d8196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efc1e21e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065ea35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd84be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b48a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8b379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8364529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
