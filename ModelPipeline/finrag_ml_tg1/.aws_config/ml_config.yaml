# ============================================================================
# FinRAG ML Pipeline - Path Configuration
# Purpose: S3 paths for data inputs/outputs
# ============================================================================

# AWS S3 Bucket Configuration
s3:
  bucket_name: sentence-data-ingestion
  region: us-east-1
  base_path_etl: DATA_MERGE_ASSETS
  base_path_ml: ML_EMBED_ASSETS

# ============================================================================
# DATA PATHS - ETL LAYER (Reference/Input)
# ============================================================================
data_etl:
  # Historical fact table (base dataset)
  historical:
    path: DATA_MERGE_ASSETS/HISTORICAL_DATA
    filename: finrag_sec_fact_historical.parquet
    description: "Base historical dataset from DuckDB export"
  
  # Incremental staging data (new/updated rows)
  crawled_incremental_stg:
    path: DATA_MERGE_ASSETS/INCREMENTAL_DATA
    filename: finrag_sec_incremental_stg_data.parquet
    description: "New data from API ingestion pipeline"

  # Final merged fact table (Stage 1 - Input for ML)
  sentence_fact:
    path: DATA_MERGE_ASSETS/FINRAG_FACT_SENTENCES
    filename: finrag_fact_sentences.parquet
    description: "Stage 1 - Original ETL output (24 columns)"
    compression: zstd

# ============================================================================
# DATA PATHS - ML LAYER (Output)
# ============================================================================
data_ml:
  # Enhanced fact table with embedding metadata
  meta_embeds:
    path: ML_EMBED_ASSETS/EMBED_META_FACT
    filename: finrag_fact_sentences_meta_embeds.parquet
    description: "Stage 2 - With embedding metadata + RAG helpers (35 columns)"
    compression: zstd

  
  # S3 Vectors staging (Stage 3 - Ready for ingestion)
  s3_vectors_staging:
    base_path: ML_EMBED_ASSETS/S3_VECTORS_STAGING  # for consistency
    
    cohere_1024d:
      path: ML_EMBED_ASSETS/S3_VECTORS_STAGING/cohere_1024d
      filename: finrag_embeddings_s3vectors_cohere_1024d.parquet
      dimensions: 1024
      description: "Stage 3 - Cohere 1024d vectors prepared for S3 Vectors"
      compression: zstd
    
    titan_1024d:
      path: ML_EMBED_ASSETS/S3_VECTORS_STAGING/titan_1024d
      filename: finrag_embeddings_s3vectors_titan_1024d.parquet
      dimensions: 1024
      description: "Stage 3 - Titan 1024d vectors prepared for S3 Vectors"
      compression: zstd

  
  # Embeddings: Separate storage per provider
  embeddings:
    base_path: ML_EMBED_ASSETS/EMBED_VECTORS
    
    cohere_768d:
      path: ML_EMBED_ASSETS/EMBED_VECTORS/cohere_768d
      filename: finrag_embeddings_cohere_768d.parquet
      metadata_file: embedding_metadata.json
      dimensions: 768
    
    cohere_1024d:  
      path: ML_EMBED_ASSETS/EMBED_VECTORS/cohere_1024d
      filename: finrag_embeddings_cohere_1024d.parquet
      metadata_file: embedding_metadata.json
      dimensions: 1024

    titan_1024d:
      path: ML_EMBED_ASSETS/EMBED_VECTORS/titan_1024d
      filename: finrag_embeddings_titan_1024d.parquet
      metadata_file: embedding_metadata.json
      dimensions: 1024


  ## DATA_MERGE_ASSETS/DIMENSION_TABLES/finrag_dim_companies_21.parquet
  ## DATA_MERGE_ASSETS/DIMENSION_TABLES/finrag_dim_sec_sections.parquet
  # Dimension tables
  dimensions:
    path: DATA_MERGE_ASSETS/DIMENSION_TABLES
    companies:
      filename: finrag_dim_companies_21.parquet
      description: "21 curated companies with CIK, ticker, name"
    sections:
      filename: finrag_dim_sec_sections.parquet
      description: "SEC 10-K section taxonomy (20 sections)"

  # KPI Fact Table (Metric Pipeline)
  kpi_facts:
    path: DATA_MERGE_ASSETS/FINRAG_FACT_METRICS
    filename: KPI_FACT_DATA_EDGAR.parquet
    description: "Structured KPI extraction from 10-K filings"

  # Query Logs & Exports (QueryLogger persistence)
  query_logging:
    base_path: DATA_MERGE_ASSETS/LOGS/FINRAG
    logs:
      path: DATA_MERGE_ASSETS/LOGS/FINRAG/logs
      description: "Query execution logs (metadata, cost, performance)"
    contexts:
      path: DATA_MERGE_ASSETS/LOGS/FINRAG/contexts
      description: "Assembled contexts sent to LLM"
    responses:
      path: DATA_MERGE_ASSETS/LOGS/FINRAG/responses
      description: "Full query responses with metadata"

      
# ============================================================================
# EMBEDDING CONFIGURATION
# ============================================================================

  # filters:
  #   cik_int: [ 34088, 59478, 104169, 200406, 320193, 789019, 813762, 814585, 890926, 909832, 
  #               1018724, 1045810, 1065280, 1141391, 1273813, 1276520, 1318605, 1326801, 1341439, 1403161,
  #                1652044 ]     
  #   year:  [2015, 2016, 2017, 2018, 2019, 2020]             
  #   sections: null

  # Tesla and Microsoft for 2016 and 2017
  # cik_int: [1318605, 789019]     # (list)
  # year: [2016, 2017]             

  # Batch across 5 at a time.
  # cik_int: [ 34088, 59478, 104169, 200406, 320193 ] 
  # cik_int: [ 789019, 813762, 814585, 890926, 909832 ] 
  # cik_int: [ 1018724, 1045810, 1065280, 1141391, 1273813 ] 
  # cik_int: [ 1276520, 1318605, 1326801, 1341439, 1403161, 1652044 ]     

embedding_execution:
  mode: parameterized
  
  filters:
    cik_int: [ 1276520, 1318605, 1326801, 1341439, 1403161, 1652044 ] 
    year:  [2015, 2016, 2017, 2018, 2019, 2020]             
    sections: null
  
  force_replace_vectors: false
  force_replace_meta: false



  # Direct Cohere API (commented - backup option)
  # cohere:
  #   model: embed-english-v3.0
  #   api_key_env: COHERE_API_KEY
  #   dimensions: 768
  #   input_type: search_document
  #   batch_size: 96
  #   max_retries: 3
  #   timeout_seconds: 30
  


embedding:
  default_provider: bedrock

  bedrock:
    region: us-east-1
    models:
      cohere_embed_v3: { model_id: cohere.embed-english-v3, dimensions: 1024, batch_size: 96, input_type: search_document }
      cohere_embed_v4: { model_id: cohere.embed-v4:0,      dimensions: 1024, batch_size: 96, input_type: search_document }
      titan_v2:        { model_id: amazon.titan-embed-text-v2:0, dimensions: 1024, batch_size: 25 }
    default_model: cohere_embed_v4

  filtering:
    min_char_length: 30
    max_char_length: 1000
    max_token_count: 500
    exclude_sections: [ITEM_15, ITEM_16]

      


# ============================================================================
# COST TRACKING
# ============================================================================
costs:
  embedding_budget_usd: 5.00
  alert_threshold_pct: 80
  
  rates:
    cohere_768d: 0.0001
    bedrock_titan_1024d: 0.0001

# ============================================================================
# LOGGING
# ============================================================================
logging:
  level: INFO
  log_to_s3: true
  log_path: ML_EMBED_ASSETS/LOGS
  log_filename_pattern: "ml_pipeline_{timestamp}.log"

# ============================================================================
# METADATA
# ============================================================================
metadata:
  pipeline_version: "1.0.0"
  last_updated: "2024-11-07"
  owner: "Joel Markapudi"
  project: "FinRAG IE7374 MLOps"





# ============================================================================
# RAG ORCHESTRATOR CONFIGURATION
# ============================================================================

## query_embedding - user submission query uses exactly this config.

rag_orchestrator:
  # Query embedding configuration (uses same Bedrock setup as document embeddings)
  query_embedding:
    provider: bedrock
    region: us-east-1
    model_id: cohere.embed-v4:0  
    dimensions: 1024
    input_type: search_query  
    batch_size: 96
  
  # LLM configuration for final response generation
  llm:
    provider: bedrock
    region: us-east-1
    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0
    max_tokens: 4096
    temperature: 0.7


  # Vector search configuration
  vector_search:
    method: s3_vectors  
    top_k: 10
    similarity_threshold: 0.7
    context_window: 2  # Include neighboring chunks
        
    # If using S3 Vectors
    s3_vectors:
      index_name: finrag-embeddings
      namespace: production

  # External pipeline paths
  external_pipelines:
    metric_pipeline_path: ../metric_pipeline
    rag_search_path: ../rag_pipeline
  
  # Execution settings
  execution:
    parallel_processing: true
    timeout_seconds: 30
    max_retries: 3
  
  # Response formatting
  response:
    include_sources: true
    max_context_chunks: 5
    include_analytical_data: true


# # ======================= Replaced. ══════════════════════════════════════
# # RETRIEVAL CONFIGURATION
# retrieval:
#   top_k: 10
#   similarity_metric: cosine
#   context_window: 2
  
#   priority_sections:
#     - ITEM_1A
#     - ITEM_7
#     - ITEM_8
#     - ITEM_1
  
#   recent_years_threshold: 2018



# ═══════════════════════════════════════════════════════════════════════════
# SEMANTIC VARIANTS CONFIG
# ═══════════════════════════════════════════════════════════════════════════
semantic_variants:
  enabled: true                                         # Toggle on/off
  model_id: "us.anthropic.claude-haiku-4-5-20251001-v1:0"
  max_tokens: 150                                       # Very short responses
  temperature: 0.7                                      # Some creativity for rephrasing
  count: 3                                              # Number of variants to generate

  ## model_id: "anthropic.claude-3-haiku-20240307-v1:0"  # Cheap model (~$0.00025 per call)
  ## model_id: "anthropic.claude-haiku-4-5-20251001-v1:0"

  # Prompt template (user can modify without touching code)
  prompt_template: |
    Rephrase this financial query {count} different ways while preserving the exact intent and entities.
    Keep the same companies, years, and sections mentioned.
    Return ONLY the {count} rephrased queries, one per line, with no numbering or extra text.
    
    Original query: "{query}"
    
    Rephrased versions:

# ═══════════════════════════════════════════════════════════════════════════
# RETRIEVAL PIPELINE CONFIG
# ═══════════════════════════════════════════════════════════════════════════
retrieval:
  # S3 Vectors query parameters
  top_k_filtered: 30  # Max results per filtered call
  top_k_global: 15    # Max results per global call
  top_k_filtered_variants: 15
  top_k_for_expansion: 20 
  
  window_size: 3
  max_context_blocks: 10
  
  # Retrieval strategy toggles
  enable_global: true       # Run global (time-relaxed) calls alongside filtered
  enable_variants: true     # Generate semantic query variants (requires Bedrock LLM)
  
  # Filter thresholds
  recent_year_threshold: 2015  # For "company-global" time relaxation (report_year >= this)
  min_similarity: 0.3          # Filter very weak hits (1 - distance/2 < this threshold)
  
  # S3 Vectors index configuration
  vector_bucket: "finrag-embeddings-s3vectors"
  index_name: "finrag-sentence-fact-embed-1024d"
  dimensions: 1024

  max_hits_before_expansion: 30     # Total hit limit before windowing
  filtered_proportion: 0.70         
  global_proportion: 0.30           

  # Similarity threshold - RESEARCH FINDING: Set very low
  # S3 Vectors' ANN already filters effectively. Threshold only useful for:
    # - Blocking truly off-topic queries (similarity < 0.4)
    # - Preventing accidental broad retrieval in edge cases
    # For normal queries, threshold=0.3 rejects nothing useful. !!!

  ## Filtered is precision layer - should still dominate
  ## 30% global is substantial - 9 hits can add real temporal diversity
 

# ============================================================================
# SERVING MODELS CONFIGURATION
# ============================================================================
# 
# CRITICAL: Cross-Region Inference (CRIS) Prefixes
# ─────────────────────────────────────────────────
# Newer Anthropic models REQUIRE regional prefixes (us./eu./apac.)
# These are "inference profiles" that route across multiple regions.
# 
# Benefits:
#   - 2x throughput (doubles quotas)
#   - Better availability (auto-failover)
#   - Required for latest models
# 
# Prefixes:
#   us.*   → Routes across US regions (us-east-1, us-west-2)
#   eu.*   → Routes across EU regions (eu-central-1, eu-west-1, eu-west-3)
#   apac.* → Routes across APAC regions
# 
# Without prefix: Single region only (may not be available in us-east-1)
# ============================================================================

serving_models:
  
  # ──────────────────────────────────────────────────────────────────────────
  # DEVELOPMENT MODELS (Fast iteration, lower cost)
  # ──────────────────────────────────────────────────────────────────────────
  
  development:
    model_id: "us.anthropic.claude-3-5-haiku-20241022-v1:0"  # CRIS prefix
    display_name: "Claude 3.5 Haiku (CRIS)"
    max_tokens: 4096
    temperature: 0.1
    top_p: 0.9
    cost_per_1k_input: 0.001   # $1 per 1M tokens
    cost_per_1k_output: 0.005  # $5 per 1M tokens
    context_window: 200000
    use_case: "Development, testing, iteration"
    notes: "Uses Cross-Region Inference - doubles throughput, better availability"
  
  development_CH45:
    model_id: "us.anthropic.claude-haiku-4-5-20251001-v1:0"  # CRIS prefix (Haiku 4.5)
    display_name: "Claude 4.5 Haiku (CRIS)"
    max_tokens: 8192
    temperature: 0.1
    top_p: 0.9
    cost_per_1k_input: 0.001   # $1 per 1M tokens (same as 3.5 Haiku)
    cost_per_1k_output: 0.005  # $5 per 1M tokens
    context_window: 200000
    use_case: "Latest Haiku features, fast development"
    notes: "Newest Haiku - REQUIRES CRIS prefix for availability"
  
  development_CL_SONN_4_5:
    model_id: "us.anthropic.claude-sonnet-4-5-20250929-v1:0"  # CRIS prefix (Sonnet 4.5)
    display_name: "Claude 4.5 Sonnet (CRIS)"
    max_tokens: 8192
    temperature: 0.1
    top_p: 0.9
    cost_per_1k_input: 0.003   # $3 per 1M tokens (3x Haiku)
    cost_per_1k_output: 0.015  # $15 per 1M tokens (3x Haiku)
    context_window: 200000
    use_case: "Complex reasoning, higher quality synthesis"
    notes: "Newest Sonnet - REQUIRES CRIS prefix for availability. 3x cost of Haiku but superior reasoning."
  

  production_balanced:
    model_id: "us.anthropic.claude-3-5-sonnet-20241022-v2:0"  # 
    display_name: "Claude 3.5 Sonnet v2 (CRIS)"
    max_tokens: 8192
    temperature: 0.1
    top_p: 0.9
    cost_per_1k_input: 0.003   # $3 per 1M tokens
    cost_per_1k_output: 0.015  # $15 per 1M tokens
    context_window: 200000
    use_case: "Production queries, evaluation gold set"
    notes: "CRITICAL: v2 model ONLY works with us. prefix in us-east-1"
  
  
  production_budget:
    model_id: "amazon.nova-micro-v1:0"  # 
    display_name: "Amazon Nova Micro"
    max_tokens: 5000
    temperature: 0.1
    top_p: 0.9
    cost_per_1k_input: 0.000035   # $0.035 per 1M tokens
    cost_per_1k_output: 0.00014   # $0.14 per 1M tokens
    context_window: 128000
    use_case: "High-volume testing, cost-sensitive scenarios"
    notes: "30x cheaper than Haiku - no CRIS prefix needed for Nova models"
  
  openai_compatible:
    model_id: "meta.llama3-1-70b-instruct-v1:0"  
    display_name: "Llama 3.1 70B"
    max_tokens: 4096
    temperature: 0.1
    top_p: 0.9
    cost_per_1k_input: 0.00099
    cost_per_1k_output: 0.00099
    context_window: 128000
    use_case: "Open source alternative for testing"
    notes: "No CRIS prefix needed for Meta models"

  default_serving_model: "development_CH45"  
