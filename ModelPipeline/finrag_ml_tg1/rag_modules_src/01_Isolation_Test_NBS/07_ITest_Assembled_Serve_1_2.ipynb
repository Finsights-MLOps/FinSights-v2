{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be017fe",
   "metadata": {},
   "source": [
    "## Final Serve. \n",
    "#### - Serve after S1 + S2 + Assembling them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da970294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook is in: ModelPipeline\\finrag_ml_tg1\\rag_modules_src\\01_Isolation_Test_NBS\\07_ITest_FinalServe.ipynb\n",
    "# supply_lines is in: ModelPipeline\\finrag_ml_tg1\\rag_modules_src\\synthesis_pipeline\\supply_lines.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model root: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\n",
      "\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "✓ FINAL COMBINED CONTEXT (KPI + RAG) saved to:\n",
      "  d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\rag_modules_src\\test_outputs\\combined_context_20251118_084005.txt\n",
      "  Size: 48,599 bytes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874a4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef20ea5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT INTEGRATION TEST - PRODUCTION QUERY\n",
      "================================================================================\n",
      "\n",
      "[Step 1] Initializing RAG components...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "  ✓ MLConfig loaded\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  ✓ RAG components initialized\n",
      "    - Entity Adapter: EntityAdapter\n",
      "    - Embedder: QueryEmbedderV2\n",
      "    - Retriever: S3VectorsRetriever\n",
      "    - Expander: SentenceExpander\n",
      "    - Assembler: ContextAssembler\n",
      "    - Metric Pipeline: MetricPipeline\n",
      "\n",
      "[Step 2] Building combined context with production query...\n",
      "  Query: For NVIDIA and Microsoft, what were revenue, operating income, and total assets in each year from 20...\n",
      "  ✓ Combined context built\n",
      "    - Total length: 43,483 characters\n",
      "    - Estimated tokens: ~10,870\n",
      "    - Has KPI block: True\n",
      "    - Has RAG block: True\n",
      "\n",
      "  [Structure Validation]\n",
      "    - KPI header present: True\n",
      "    - Narrative header present: True\n",
      "    - Query footer present: True\n",
      "    - Query position: 99.2% through context\n",
      "\n",
      "[Step 3] Loading prompt templates...\n",
      "  ✓ PromptLoader initialized\n",
      "  ✓ System prompt loaded\n",
      "    - Length: 4,050 characters\n",
      "    - Estimated tokens: ~1,012\n",
      "\n",
      "  [Recommended LLM Parameters]\n",
      "    - Temperature: 0.1\n",
      "    - Max tokens: 2,048\n",
      "    - Target models: claude-sonnet-3.5, claude-haiku-3.5, gpt-4o\n",
      "\n",
      "[Step 4] Formatting final user prompt...\n",
      "  ✓ User prompt formatted\n",
      "    - Length: 43,483 characters\n",
      "    - Estimated tokens: ~10,870\n",
      "\n",
      "[Step 5] Token budget analysis...\n",
      "  [Token Budget]\n",
      "    - System prompt: ~1,012 tokens\n",
      "    - User prompt (context): ~10,870 tokens\n",
      "    - Total input: ~11,882 tokens\n",
      "    - Response budget: 2,048 tokens\n",
      "    - Grand total: ~13,930 tokens\n",
      "\n",
      "  [Context Window Fits]\n",
      "    ✓ claude-sonnet-3.5: 7.0% utilized (13,930/200,000)\n",
      "    ✓ claude-haiku-3.5: 7.0% utilized (13,930/200,000)\n",
      "    ✓ gpt-4o: 10.9% utilized (13,930/128,000)\n",
      "\n",
      "[Step 6] Cost estimation...\n",
      "  [Cost Analysis - Claude 3.5 Sonnet]\n",
      "    - Input cost: $0.0356\n",
      "    - Output cost: $0.0307\n",
      "    - Total per query: $0.0664\n",
      "    - Cost for 100 queries: $6.64\n",
      "\n",
      "[Step 7] Context preview...\n",
      "\n",
      "  [Context Start - First 500 chars]\n",
      "  ----------------------------------------------------------------------------\n",
      "  ══════════════════════════════════════════════════════════════════════\n",
      "  KPI SNAPSHOT - METRIC PIPELINE OUTPUT\n",
      "  ══════════════════════════════════════════════════════════════════════\n",
      "  \n",
      "  Scope:\n",
      "    Companies (entities): MSFT, NVDA\n",
      "    Years (entities):     2016, 2017, 2018, 2019, 2020\n",
      "    Sections (entities):  ITEM_7, ITEM_1A\n",
      "    Companies (metrics):  MSFT, NVDA\n",
      "    Years (metrics):      2016, 2017, 2018, 2019, 2020\n",
      "    Metrics:              Operating Income, Total Assets, Revenue, Net Income\n",
      "    Coverage:         \n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "  [Query Footer - Last section]\n",
      "  ----------------------------------------------------------------------------\n",
      "  USER QUESTION\n",
      "  ══════════════════════════════════════════════════════════════════════\n",
      "  \n",
      "  For NVIDIA and Microsoft, what were revenue, operating income, and total assets in each year from 2016 to 2020, and how did management in the MD&A and Risk Factors sections explain these trends in terms of their AI\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "✓ ALL TESTS PASSED\n",
      "================================================================================\n",
      "\n",
      "[Summary]\n",
      "  ✓ RAG components initialized\n",
      "  ✓ Combined context built (43,483 chars)\n",
      "  ✓ System prompt loaded (4,050 chars)\n",
      "  ✓ User prompt formatted (43,483 chars)\n",
      "  ✓ Structure validated (KPI + Narrative + Query footer)\n",
      "  ✓ Token budget analyzed (~13,930 tokens)\n",
      "  ✓ Cost estimated ($0.0664 per query)\n",
      "\n",
      "[Ready for LLM]\n",
      "  Model: Claude 3.5 Sonnet\n",
      "  Temperature: 0.1\n",
      "  Max tokens: 2,048\n",
      "  Estimated cost: $0.0664\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\venv_ml_rag\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Integration test for PromptLoader with full RAG pipeline.\n",
    "Tests prompt loading and formatting with production-scale query.\n",
    "\n",
    "Run from ModelPipeline root:\n",
    "    cd ModelPipeline\n",
    "    python -m finrag_ml_tg1.rag_modules_src.prompts.test_prompt_integration\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find ModelPipeline root\n",
    "current = Path.cwd()\n",
    "model_root = None\n",
    "for parent in [current] + list(current.parents):\n",
    "    if parent.name == \"ModelPipeline\":\n",
    "        model_root = parent\n",
    "        break\n",
    "\n",
    "if model_root is None:\n",
    "    raise RuntimeError(\"Cannot find 'ModelPipeline' root in path tree\")\n",
    "\n",
    "if str(model_root) not in sys.path:\n",
    "    sys.path.insert(0, str(model_root))\n",
    "\n",
    "# Now import\n",
    "from finrag_ml_tg1.loaders.ml_config_loader import MLConfig\n",
    "from finrag_ml_tg1.rag_modules_src.prompts.prompt_loader import PromptLoader\n",
    "from finrag_ml_tg1.rag_modules_src.synthesis_pipeline.supply_lines import (\n",
    "    RAGComponents,\n",
    "    init_rag_components,\n",
    "    build_combined_context,\n",
    ")\n",
    "\n",
    "\n",
    "def test_prompt_integration():\n",
    "    \"\"\"\n",
    "    Full integration test:\n",
    "    1. Initialize RAG components\n",
    "    2. Build combined context with production query\n",
    "    3. Load prompts\n",
    "    4. Format final prompt\n",
    "    5. Validate structure\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"PROMPT INTEGRATION TEST - PRODUCTION QUERY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 1: Initialize Components\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 1] Initializing RAG components...\")\n",
    "    \n",
    "    try:\n",
    "        config = MLConfig()\n",
    "        print(\"  ✓ MLConfig loaded\")\n",
    "        \n",
    "        rag = init_rag_components(model_root)\n",
    "        print(\"  ✓ RAG components initialized\")\n",
    "        print(f\"    - Entity Adapter: {type(rag.adapter).__name__}\")\n",
    "        print(f\"    - Embedder: {type(rag.embedder).__name__}\")\n",
    "        print(f\"    - Retriever: {type(rag.retriever).__name__}\")\n",
    "        print(f\"    - Expander: {type(rag.expander).__name__}\")\n",
    "        print(f\"    - Assembler: {type(rag.assembler).__name__}\")\n",
    "        print(f\"    - Metric Pipeline: {type(rag.metric_pipeline).__name__}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Component initialization failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 2: Build Combined Context (Production Query)\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 2] Building combined context with production query...\")\n",
    "    \n",
    "    query = (\n",
    "        \"For NVIDIA and Microsoft, what were revenue, operating income, and total assets \"\n",
    "        \"in each year from 2016 to 2020, and how did management in the MD&A and \"\n",
    "        \"Risk Factors sections explain these trends in terms of their AI strategy, \"\n",
    "        \"competitive positioning, and supply chain risks?\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  Query: {query[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        combined_context, meta = build_combined_context(\n",
    "            query=query,\n",
    "            rag=rag,\n",
    "            include_kpi=True,\n",
    "            include_rag=True\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Combined context built\")\n",
    "        print(f\"    - Total length: {len(combined_context):,} characters\")\n",
    "        print(f\"    - Estimated tokens: ~{len(combined_context) // 4:,}\")\n",
    "        print(f\"    - Has KPI block: {bool(meta['kpi_block'])}\")\n",
    "        print(f\"    - Has RAG block: {bool(meta['rag_block'])}\")\n",
    "        \n",
    "        # Validate structure\n",
    "        has_kpi_header = \"KPI SNAPSHOT\" in combined_context\n",
    "        has_narrative_header = \"NARRATIVE CONTEXT\" in combined_context\n",
    "        has_query_footer = \"USER QUESTION\" in combined_context\n",
    "        \n",
    "        print(f\"\\n  [Structure Validation]\")\n",
    "        print(f\"    - KPI header present: {has_kpi_header}\")\n",
    "        print(f\"    - Narrative header present: {has_narrative_header}\")\n",
    "        print(f\"    - Query footer present: {has_query_footer}\")\n",
    "        \n",
    "        if not all([has_kpi_header, has_narrative_header, has_query_footer]):\n",
    "            print(f\"  ✗ Structure validation failed!\")\n",
    "            return False\n",
    "        \n",
    "        # Check query position (should be near end)\n",
    "        query_position = combined_context.find(\"USER QUESTION\")\n",
    "        context_length = len(combined_context)\n",
    "        query_at_end_pct = (query_position / context_length) * 100\n",
    "        \n",
    "        print(f\"    - Query position: {query_at_end_pct:.1f}% through context\")\n",
    "        \n",
    "        if query_at_end_pct < 80:\n",
    "            print(f\"    ⚠ Warning: Query should be near end (>80%), found at {query_at_end_pct:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Context building failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 3: Load Prompts\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 3] Loading prompt templates...\")\n",
    "    \n",
    "    try:\n",
    "        loader = PromptLoader(\n",
    "            system_prompt_version=\"v1\",\n",
    "            query_template_version=\"v1\"\n",
    "        )\n",
    "        print(\"  ✓ PromptLoader initialized\")\n",
    "        \n",
    "        system_prompt = loader.load_system_prompt()\n",
    "        print(f\"  ✓ System prompt loaded\")\n",
    "        print(f\"    - Length: {len(system_prompt):,} characters\")\n",
    "        print(f\"    - Estimated tokens: ~{len(system_prompt) // 4:,}\")\n",
    "        \n",
    "        # Get recommended LLM params\n",
    "        llm_params = loader.get_recommended_llm_params()\n",
    "        print(f\"\\n  [Recommended LLM Parameters]\")\n",
    "        print(f\"    - Temperature: {llm_params['temperature']}\")\n",
    "        print(f\"    - Max tokens: {llm_params['max_tokens']:,}\")\n",
    "        print(f\"    - Target models: {', '.join(llm_params['target_models'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Prompt loading failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 4: Format Final User Prompt\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 4] Formatting final user prompt...\")\n",
    "    \n",
    "    try:\n",
    "        user_prompt = loader.format_query_template(\n",
    "            combined_context=combined_context\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ User prompt formatted\")\n",
    "        print(f\"    - Length: {len(user_prompt):,} characters\")\n",
    "        print(f\"    - Estimated tokens: ~{len(user_prompt) // 4:,}\")\n",
    "        \n",
    "        # Validate it's the same as combined_context (simple pass-through template)\n",
    "        if user_prompt.strip() != combined_context.strip():\n",
    "            print(f\"  ⚠ Warning: Template modified context (expected pass-through)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Prompt formatting failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 5: Token Budget Analysis\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 5] Token budget analysis...\")\n",
    "    \n",
    "    system_tokens = len(system_prompt) // 4\n",
    "    user_tokens = len(user_prompt) // 4\n",
    "    total_input_tokens = system_tokens + user_tokens\n",
    "    response_tokens = llm_params['max_tokens']\n",
    "    total_tokens = total_input_tokens + response_tokens\n",
    "    \n",
    "    print(f\"  [Token Budget]\")\n",
    "    print(f\"    - System prompt: ~{system_tokens:,} tokens\")\n",
    "    print(f\"    - User prompt (context): ~{user_tokens:,} tokens\")\n",
    "    print(f\"    - Total input: ~{total_input_tokens:,} tokens\")\n",
    "    print(f\"    - Response budget: {response_tokens:,} tokens\")\n",
    "    print(f\"    - Grand total: ~{total_tokens:,} tokens\")\n",
    "    \n",
    "    # Context window check\n",
    "    context_limits = {\n",
    "        'claude-sonnet-3.5': 200_000,\n",
    "        'claude-haiku-3.5': 200_000,\n",
    "        'gpt-4o': 128_000,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  [Context Window Fits]\")\n",
    "    for model, limit in context_limits.items():\n",
    "        fits = total_tokens < limit\n",
    "        utilization = (total_tokens / limit) * 100\n",
    "        status = \"✓\" if fits else \"✗\"\n",
    "        print(f\"    {status} {model}: {utilization:.1f}% utilized ({total_tokens:,}/{limit:,})\")\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 6: Cost Estimation\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 6] Cost estimation...\")\n",
    "    \n",
    "    # Get serving model config\n",
    "    serving_config = config.get_default_serving_model()\n",
    "    model_name = serving_config['display_name']\n",
    "    cost_per_1k_input = serving_config['cost_per_1k_input']\n",
    "    cost_per_1k_output = serving_config['cost_per_1k_output']\n",
    "    \n",
    "    input_cost = (total_input_tokens / 1000) * cost_per_1k_input\n",
    "    output_cost = (response_tokens / 1000) * cost_per_1k_output\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    print(f\"  [Cost Analysis - {model_name}]\")\n",
    "    print(f\"    - Input cost: ${input_cost:.4f}\")\n",
    "    print(f\"    - Output cost: ${output_cost:.4f}\")\n",
    "    print(f\"    - Total per query: ${total_cost:.4f}\")\n",
    "    print(f\"    - Cost for 100 queries: ${total_cost * 100:.2f}\")\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Step 7: Preview Output\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n[Step 7] Context preview...\")\n",
    "    \n",
    "    # Show first 500 chars\n",
    "    print(f\"\\n  [Context Start - First 500 chars]\")\n",
    "    print(\"  \" + \"-\" * 76)\n",
    "    preview_start = combined_context[:500].replace(\"\\n\", \"\\n  \")\n",
    "    print(f\"  {preview_start}\")\n",
    "    print(\"  \" + \"-\" * 76)\n",
    "    \n",
    "    # Show query footer\n",
    "    if \"USER QUESTION\" in combined_context:\n",
    "        footer_start = combined_context.find(\"USER QUESTION\")\n",
    "        footer = combined_context[footer_start:][:300]\n",
    "        print(f\"\\n  [Query Footer - Last section]\")\n",
    "        print(\"  \" + \"-\" * 76)\n",
    "        footer_preview = footer.replace(\"\\n\", \"\\n  \")\n",
    "        print(f\"  {footer_preview}\")\n",
    "        print(\"  \" + \"-\" * 76)\n",
    "    \n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # Final Summary\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ ALL TESTS PASSED\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n[Summary]\")\n",
    "    print(f\"  ✓ RAG components initialized\")\n",
    "    print(f\"  ✓ Combined context built ({len(combined_context):,} chars)\")\n",
    "    print(f\"  ✓ System prompt loaded ({len(system_prompt):,} chars)\")\n",
    "    print(f\"  ✓ User prompt formatted ({len(user_prompt):,} chars)\")\n",
    "    print(f\"  ✓ Structure validated (KPI + Narrative + Query footer)\")\n",
    "    print(f\"  ✓ Token budget analyzed (~{total_tokens:,} tokens)\")\n",
    "    print(f\"  ✓ Cost estimated (${total_cost:.4f} per query)\")\n",
    "    \n",
    "    print(f\"\\n[Ready for LLM]\")\n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Temperature: {llm_params['temperature']}\")\n",
    "    print(f\"  Max tokens: {llm_params['max_tokens']:,}\")\n",
    "    print(f\"  Estimated cost: ${total_cost:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        success = test_prompt_integration()\n",
    "        sys.exit(0 if success else 1)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ FATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d02472",
   "metadata": {},
   "outputs": [],
   "source": [
    "## not an error. It's a successful exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966798fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0f385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc7b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a50c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
