{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a17af0",
   "metadata": {},
   "source": [
    "## LLM Answer Evals: \n",
    "#### Eval suite on final GOLDP3 Answers - LLM Synthesis using BERT Score and ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3bc1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPipeline root: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\n",
      "Notebook location: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\rag_modules_src\\01_Isolation_Test_NBS\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Setup - Path Resolution & Imports\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Suppress noisy logs for clean notebook output\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger(\"finrag_ml_tg1\").setLevel(logging.INFO)\n",
    "\n",
    "# Find ModelPipeline root and add to sys.path\n",
    "current = Path.cwd()\n",
    "for parent in [current] + list(current.parents):\n",
    "    if parent.name == \"ModelPipeline\":\n",
    "        model_root = parent\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Cannot find 'ModelPipeline' root in path tree\")\n",
    "\n",
    "if str(model_root) not in sys.path:\n",
    "    sys.path.insert(0, str(model_root))\n",
    "\n",
    "print(f\"ModelPipeline root: {model_root}\")\n",
    "print(f\"Notebook location: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45147e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BLEURT-20 checkpoint already exists at: C:\\Users\\joems\\.cache\\bleurt\\BLEURT-20\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# One-Time Setup: Download BLEURT-20 Checkpoint\n",
    "# ============================================================================\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "bleurt_cache = Path.home() / \".cache\" / \"bleurt\"\n",
    "checkpoint_dir = bleurt_cache / \"BLEURT-20\"\n",
    "\n",
    "if not checkpoint_dir.exists():\n",
    "    print(\"Downloading BLEURT-20 checkpoint (~1GB, one-time only)...\")\n",
    "    \n",
    "    # Download\n",
    "    url = \"https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\"\n",
    "    zip_path = bleurt_cache / \"BLEURT-20.zip\"\n",
    "    bleurt_cache.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(f\"Downloaded to: {zip_path}\")\n",
    "    \n",
    "    # Extract\n",
    "    print(\"Extracting checkpoint...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(bleurt_cache)\n",
    "    \n",
    "    # Cleanup\n",
    "    zip_path.unlink()\n",
    "    print(f\"✓ Checkpoint installed at: {checkpoint_dir}\")\n",
    "else:\n",
    "    print(f\"✓ BLEURT-20 checkpoint already exists at: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a03d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: C:\\Users\\joems\\.cache\\huggingface\\hub\n",
      "bert-base-uncased: 0.00 GB\n",
      "deepset/roberta-base-squad2: 0.50 GB\n",
      "distilbert-base-cased-distilled-squad: 0.26 GB\n",
      "distilbert-base-uncased: 0.00 GB\n",
      "mrm8488/bert-small-finetuned-squadv2: 0.23 GB\n",
      "roberta-large: 1.42 GB\n",
      "sentence-transformers/all-MiniLM-L6-v2: 0.09 GB\n",
      "sentence-transformers/all-mpnet-base-v2: 0.44 GB\n"
     ]
    }
   ],
   "source": [
    "## ===========================================================================\n",
    "## Cache Inspection - HuggingFace Model Cache Size\n",
    "## ===========================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# HuggingFace cache location\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "# List cached models\n",
    "for model_dir in cache_dir.glob(\"models--*\"):\n",
    "    model_name = model_dir.name.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "    size_gb = sum(f.stat().st_size for f in model_dir.rglob(\"*\") if f.is_file()) / 1e9\n",
    "    print(f\"{model_name}: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522aa7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING EVALUATION STACK\n",
      "================================================================================\n",
      "\n",
      "1/4 Loading BERTScore (DeBERTa-XLarge)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ BERTScore ready (F1: 1.000)\n",
      "\n",
      "2/4 Loading Sentence Transformer (MiniLM)...\n",
      "WARNING:tensorflow:From d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\venv_ml_rag\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "    ✓ Sentence Transformer ready (dim: 384)\n",
      "\n",
      "3/4 Loading ROUGE scorer...\n",
      "    ✓ ROUGE ready (F1: 1.000)\n",
      "\n",
      "4/4 Loading BLEURT-20...\n",
      "WARNING:tensorflow:From d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\venv_ml_rag\\Lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\venv_ml_rag\\Lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\joems\\.cache\\bleurt\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\joems\\.cache\\bleurt\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\joems\\.cache\\bleurt\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\joems\\.cache\\bleurt\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ BLEURT ready (score: 0.598)\n",
      "\n",
      "================================================================================\n",
      "ALL MODELS LOADED - EVALUATION STACK READY\n",
      "================================================================================\n",
      "\n",
      "Ready to evaluate answers!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Initialize Evaluation Stack\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Loads all evaluation models. Uses cached models from previous downloads.\n",
    "\n",
    "Models:\n",
    "- DeBERTa-XLarge-MNLI (1.4GB) - BERTScore\n",
    "- all-MiniLM-L6-v2 (80MB) - Cosine Similarity  \n",
    "- BLEURT-20 (1GB) - BLEURT scores\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING EVALUATION STACK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. BERTScore\n",
    "print(\"\\n1/4 Loading BERTScore (DeBERTa-XLarge)...\")\n",
    "from bert_score import score\n",
    "P, R, F1 = score([\"test\"], [\"test\"], lang='en', verbose=False)\n",
    "print(f\"    ✓ BERTScore ready (F1: {F1.item():.3f})\")\n",
    "\n",
    "# 2. Sentence Transformer\n",
    "print(\"\\n2/4 Loading Sentence Transformer (MiniLM)...\")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "test_emb = sentence_model.encode(\"test\")\n",
    "print(f\"    ✓ Sentence Transformer ready (dim: {len(test_emb)})\")\n",
    "\n",
    "# 3. ROUGE\n",
    "print(\"\\n3/4 Loading ROUGE scorer...\")\n",
    "from rouge_score import rouge_scorer\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "test_rouge = rouge_scorer_obj.score(\"test\", \"test\")\n",
    "print(f\"    ✓ ROUGE ready (F1: {test_rouge['rougeL'].fmeasure:.3f})\")\n",
    "\n",
    "# 4. BLEURT\n",
    "print(\"\\n4/4 Loading BLEURT-20...\")\n",
    "from bleurt import score as bleurt_score\n",
    "checkpoint_path = str(Path.home() / \".cache\" / \"bleurt\" / \"BLEURT-20\")\n",
    "bleurt_scorer = bleurt_score.BleurtScorer(checkpoint_path)  # ← Pass full path, not just 'BLEURT-20'\n",
    "test_bleurt = bleurt_scorer.score(references=[\"test\"], candidates=[\"test\"])\n",
    "print(f\"    ✓ BLEURT ready (score: {test_bleurt[0]:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS LOADED - EVALUATION STACK READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReady to evaluate answers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e724bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good paraphrase: 0.833\n",
      "Unrelated text: 0.184\n"
     ]
    }
   ],
   "source": [
    "## ===========================================================================\n",
    "## Proper DEMO test for BlEURT loading\n",
    "\"\"\"\n",
    "BLEURT Score Interpretation Guide:\n",
    "Excellent synthesis: 0.6 - 0.9\n",
    "Good synthesis: 0.3 - 0.6\n",
    "Weak synthesis: 0.0 - 0.3\n",
    "Poor synthesis: Below 0.0\n",
    "\"\"\"\n",
    "## ===========================================================================\n",
    "\n",
    "# Test with actual sentences\n",
    "good_match = bleurt_scorer.score(\n",
    "    references=[\"The company's revenue increased significantly\"],\n",
    "    candidates=[\"The firm's revenue grew substantially\"]\n",
    ")\n",
    "print(f\"Good paraphrase: {good_match[0]:.3f}\")  # Should be ~0.8-0.9\n",
    "\n",
    "bad_match = bleurt_scorer.score(\n",
    "    references=[\"The company's revenue increased significantly\"],\n",
    "    candidates=[\"I like pizza and ice cream\"]\n",
    ")\n",
    "print(f\"Unrelated text: {bad_match[0]:.3f}\")  # Should be negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99e5bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Eval !\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b5f27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ModelPipeline root: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\n",
      "✓ Notebook location: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\rag_modules_src\\01_Isolation_Test_NBS\n",
      "\n",
      "✓ Gold test suite: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\qa_manual_exports\\goldp3_analysis\\p3_gold_test_suite_31q.json\n",
      "\n",
      "================================================================================\n",
      "LOADED 11 TEST QUESTIONS\n",
      "================================================================================\n",
      "\n",
      "P3V3-Q001:\n",
      "  Companies: Walmart Inc.\n",
      "  Years: [2018, 2019, 2020]\n",
      "  Scope: cross_year | Difficulty: medium | Evidence: 4 sentences\n",
      "\n",
      "P3V2-Q006:\n",
      "  Companies: MICROSOFT CORP\n",
      "  Years: [2017]\n",
      "  Scope: local | Difficulty: medium | Evidence: 1 sentences\n",
      "\n",
      "P3V3-Q004:\n",
      "  Companies: RADIAN GROUP INC, NETFLIX INC, Mastercard Inc\n",
      "  Years: [2009]\n",
      "  Scope: cross_company | Difficulty: medium | Evidence: 3 sentences\n",
      "\n",
      "P3V3-Q007:\n",
      "  Companies: Tesla, Inc.\n",
      "  Years: [2022]\n",
      "  Scope: local | Difficulty: easy | Evidence: 1 sentences\n",
      "\n",
      "P3V3-Q002:\n",
      "  Companies: Meta Platforms, Inc.\n",
      "  Years: [2019, 2022, 2024]\n",
      "  Scope: cross_year | Difficulty: hard | Evidence: 4 sentences\n",
      "\n",
      "P3V2-Q015:\n",
      "  Companies: Walmart Inc.\n",
      "  Years: [2021]\n",
      "  Scope: local | Difficulty: medium | Evidence: 1 sentences\n",
      "\n",
      "P3V2-Q007:\n",
      "  Companies: GENWORTH FINANCIAL INC\n",
      "  Years: [2019]\n",
      "  Scope: local | Difficulty: medium | Evidence: 1 sentences\n",
      "\n",
      "P3V2-Q013:\n",
      "  Companies: Walmart Inc.\n",
      "  Years: [2011]\n",
      "  Scope: local | Difficulty: hard | Evidence: 1 sentences\n",
      "\n",
      "P3V2-Q001:\n",
      "  Companies: EXXON MOBIL CORP\n",
      "  Years: [2008]\n",
      "  Scope: local | Difficulty: easy | Evidence: 1 sentences\n",
      "\n",
      "P3V2-Q002:\n",
      "  Companies: ELI LILLY & Co\n",
      "  Years: [2006]\n",
      "  Scope: local | Difficulty: easy | Evidence: 1 sentences\n",
      "\n",
      "P3V2-Q004:\n",
      "  Companies: JOHNSON & JOHNSON\n",
      "  Years: [2016]\n",
      "  Scope: local | Difficulty: easy | Evidence: 1 sentences\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Setup - Path Resolution & Load Gold Test Suite\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Suppress noisy logs for clean notebook output\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger(\"finrag_ml_tg1\").setLevel(logging.INFO)\n",
    "\n",
    "# Find ModelPipeline root and add to sys.path\n",
    "current = Path.cwd()\n",
    "for parent in [current] + list(current.parents):\n",
    "    if parent.name == \"ModelPipeline\":\n",
    "        model_root = parent\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Cannot find 'ModelPipeline' root in path tree\")\n",
    "\n",
    "if str(model_root) not in sys.path:\n",
    "    sys.path.insert(0, str(model_root))\n",
    "\n",
    "print(f\"✓ ModelPipeline root: {model_root}\")\n",
    "print(f\"✓ Notebook location: {Path.cwd()}\\n\")\n",
    "\n",
    "# Construct absolute path to gold test suite\n",
    "gold_path = model_root / \"finrag_ml_tg1\" / \"data_cache\" / \"qa_manual_exports\" / \"goldp3_analysis\" / \"p3_gold_test_suite_31q.json\"\n",
    "\n",
    "if not gold_path.exists():\n",
    "    raise FileNotFoundError(f\"Gold test suite not found at: {gold_path}\")\n",
    "\n",
    "print(f\"✓ Gold test suite: {gold_path}\\n\")\n",
    "\n",
    "# Load all questions\n",
    "with gold_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    all_questions = json.load(f)\n",
    "\n",
    "# Selected question IDs for testing\n",
    "SELECTED_IDS = [\n",
    "    \"P3V3-Q001\",  # Walmart Debt Strategy 2018-2020 (cross-year, medium, 4 evidence)\n",
    "    \"P3V2-Q006\",  # Microsoft Intelligent Cloud 2017 (local, medium, 1 evidence)\n",
    "    \"P3V3-Q004\",  # Cross-Company Cyber 2009 (cross-company, medium, 3 evidence)\n",
    "    \"P3V3-Q007\",  # Tesla Adjusted EBITDA 2022 (local, easy, 1 evidence)\n",
    "    \"P3V3-Q002\",  # Meta Regulatory Evolution 2019-2024 (cross-year, hard, 4 evidence)\n",
    "\n",
    "\n",
    "    \"P3V2-Q015\",  # Walmart Market/Competitive Risks 2021\n",
    "                  # local, medium, 1 evidence # COVID-related risks - comprehensive but single long paragraph\n",
    "    \"P3V2-Q007\",  # Genworth Regulatory Risks 2019\n",
    "                  # local, medium, 1 evidence # Massive bullet list (14+ risk cues) - overwhelming detail\n",
    "    \"P3V2-Q013\",  # Walmart Operational/Supply Chain Risks 2011\n",
    "                  # local, hard, 1 evidence # Long narrative about natural disasters and disruptions\n",
    "    \n",
    "    \"P3V2-Q001\",  # Exxon Mobil Total Revenue 2008\n",
    "                  # local, easy, 1 evidence\n",
    "                  # BAD: Asks for revenue, answer is cross-reference boilerplate\n",
    "    \"P3V2-Q002\",  # Eli Lilly Net Income 2006\n",
    "                  # local, easy, 1 evidence\n",
    "                  # BAD: Asks for net income, answer discusses valuation allowance\n",
    "    \"P3V2-Q004\",  # Johnson & Johnson Cash Flow 2016\n",
    "                  # local, easy, 1 evidence\n",
    "                  # BAD: Asks for cash flow, answer is auditor's opinion statement\n",
    "]\n",
    "\n",
    "# Extract selected questions into structured dictionary\n",
    "test_suite = {}\n",
    "for q in all_questions:\n",
    "    qid = q[\"question_id\"]\n",
    "    if qid in SELECTED_IDS:\n",
    "        test_suite[qid] = {\n",
    "            \"question_text\": q[\"question_text\"],\n",
    "            \"gold_answer\": q[\"answer_text\"],\n",
    "            \"answer_type\": q[\"answer_type\"],\n",
    "            \"companies\": q[\"company_name\"],\n",
    "            \"years\": q[\"years\"],\n",
    "            \"retrieval_scope\": q[\"retrieval_scope\"],\n",
    "            \"difficulty\": q[\"difficulty\"],\n",
    "            \"evidence_count\": len(q[\"evidence_sentence_ids\"]),\n",
    "            \"evidence_ids\": q[\"evidence_sentence_ids\"],\n",
    "        }\n",
    "\n",
    "# Display summary\n",
    "print(\"=\"*80)\n",
    "print(f\"LOADED {len(test_suite)} TEST QUESTIONS\")\n",
    "print(\"=\"*80)\n",
    "for qid in SELECTED_IDS:\n",
    "    q = test_suite[qid]\n",
    "    print(f\"\\n{qid}:\")\n",
    "    print(f\"  Companies: {', '.join(q['companies'])}\")\n",
    "    print(f\"  Years: {q['years']}\")\n",
    "    print(f\"  Scope: {q['retrieval_scope']} | Difficulty: {q['difficulty']} | Evidence: {q['evidence_count']} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548436d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BATCH EVALUATION: 6 QUESTIONS\n",
      "================================================================================\n",
      "Model: Claude Sonnet 4.5\n",
      "Rate limit protection: 70s delay between queries\n",
      "Estimated total runtime: ~8.0 min\n",
      "\n",
      "\n",
      "[1/6] Processing P3V3-Q001...\n",
      "  - Running synthesis...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  - Computing metrics...\n",
      "  ✓ BERTScore: 0.842 | BLEURT: 0.501 | Time: 7.7s\n",
      "  - Cooling down (70s)...\n",
      "\n",
      "[2/6] Processing P3V2-Q006...\n",
      "  - Running synthesis...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  - Computing metrics...\n",
      "  ✓ BERTScore: 0.848 | BLEURT: 0.437 | Time: 2.6s\n",
      "  - Cooling down (70s)...\n",
      "\n",
      "[3/6] Processing P3V3-Q004...\n",
      "  - Running synthesis...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  - Computing metrics...\n",
      "  ✓ BERTScore: 0.826 | BLEURT: 0.438 | Time: 3.3s\n",
      "  - Cooling down (70s)...\n",
      "\n",
      "[4/6] Processing P3V3-Q007...\n",
      "  - Running synthesis...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  - Computing metrics...\n",
      "  ✓ BERTScore: 0.804 | BLEURT: 0.444 | Time: 2.5s\n",
      "  - Cooling down (70s)...\n",
      "\n",
      "[5/6] Processing P3V3-Q002...\n",
      "  - Running synthesis...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  - Computing metrics...\n",
      "  ✓ BERTScore: 0.832 | BLEURT: 0.449 | Time: 2.9s\n",
      "  - Cooling down (70s)...\n",
      "\n",
      "[6/6] Processing P3V2-Q001...\n",
      "  - Running synthesis...\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ FilterExtractor initialized with 21 companies\n",
      "  Using: finrag_dim_companies_21.parquet\n",
      "✓ KPI-JSON: Loaded 527 metric records\n",
      "✓ KPI-JSON: Unique tickers: 2\n",
      "✓ KPI-JSON: Year range: 2010-2025\n",
      "  - Computing metrics...\n",
      "  ✓ BERTScore: 0.802 | BLEURT: 0.409 | Time: 2.2s\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>question_id</th><th>company</th><th>scope</th><th>difficulty</th><th>rouge_l</th><th>bertscore_f1</th><th>cosine_sim</th><th>bleurt</th><th>interpretation</th><th>eval_time_ms</th><th>synthesis_tokens</th><th>synthesis_cost</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;P3V3-Q001&quot;</td><td>&quot;Walmart Inc.&quot;</td><td>&quot;cross_year&quot;</td><td>&quot;medium&quot;</td><td>0.103</td><td>0.842</td><td>0.842</td><td>0.501</td><td>&quot;Strong similarity&quot;</td><td>7675.3</td><td>6287</td><td>0.029649</td></tr><tr><td>&quot;P3V2-Q006&quot;</td><td>&quot;MICROSOFT CORP&quot;</td><td>&quot;local&quot;</td><td>&quot;medium&quot;</td><td>0.127</td><td>0.848</td><td>0.733</td><td>0.437</td><td>&quot;Strong similarity&quot;</td><td>2627.4</td><td>7394</td><td>0.027006</td></tr><tr><td>&quot;P3V3-Q004&quot;</td><td>&quot;RADIAN GROUP INC, NETFLIX INC,…</td><td>&quot;cross_company&quot;</td><td>&quot;medium&quot;</td><td>0.088</td><td>0.826</td><td>0.573</td><td>0.438</td><td>&quot;Strong similarity&quot;</td><td>3321.2</td><td>5946</td><td>0.026034</td></tr><tr><td>&quot;P3V3-Q007&quot;</td><td>&quot;Tesla, Inc.&quot;</td><td>&quot;local&quot;</td><td>&quot;easy&quot;</td><td>0.099</td><td>0.804</td><td>0.684</td><td>0.444</td><td>&quot;Strong similarity&quot;</td><td>2467.1</td><td>6262</td><td>0.022398</td></tr><tr><td>&quot;P3V3-Q002&quot;</td><td>&quot;Meta Platforms, Inc.&quot;</td><td>&quot;cross_year&quot;</td><td>&quot;hard&quot;</td><td>0.066</td><td>0.832</td><td>0.737</td><td>0.449</td><td>&quot;Strong similarity&quot;</td><td>2858.2</td><td>11541</td><td>0.054807</td></tr><tr><td>&quot;P3V2-Q001&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>&quot;local&quot;</td><td>&quot;easy&quot;</td><td>0.109</td><td>0.802</td><td>0.483</td><td>0.409</td><td>&quot;Strong similarity&quot;</td><td>2238.0</td><td>5976</td><td>0.020724</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 12)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ question_ ┆ company   ┆ scope     ┆ difficult ┆ … ┆ interpret ┆ eval_time ┆ synthesis ┆ synthesi │\n",
       "│ id        ┆ ---       ┆ ---       ┆ y         ┆   ┆ ation     ┆ _ms       ┆ _tokens   ┆ s_cost   │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆           ┆           ┆ str       ┆   ┆ str       ┆ f64       ┆ i64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ P3V3-Q001 ┆ Walmart   ┆ cross_yea ┆ medium    ┆ … ┆ Strong    ┆ 7675.3    ┆ 6287      ┆ 0.029649 │\n",
       "│           ┆ Inc.      ┆ r         ┆           ┆   ┆ similarit ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ y         ┆           ┆           ┆          │\n",
       "│ P3V2-Q006 ┆ MICROSOFT ┆ local     ┆ medium    ┆ … ┆ Strong    ┆ 2627.4    ┆ 7394      ┆ 0.027006 │\n",
       "│           ┆ CORP      ┆           ┆           ┆   ┆ similarit ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ y         ┆           ┆           ┆          │\n",
       "│ P3V3-Q004 ┆ RADIAN    ┆ cross_com ┆ medium    ┆ … ┆ Strong    ┆ 3321.2    ┆ 5946      ┆ 0.026034 │\n",
       "│           ┆ GROUP     ┆ pany      ┆           ┆   ┆ similarit ┆           ┆           ┆          │\n",
       "│           ┆ INC,      ┆           ┆           ┆   ┆ y         ┆           ┆           ┆          │\n",
       "│           ┆ NETFLIX   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ INC,…     ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ P3V3-Q007 ┆ Tesla,    ┆ local     ┆ easy      ┆ … ┆ Strong    ┆ 2467.1    ┆ 6262      ┆ 0.022398 │\n",
       "│           ┆ Inc.      ┆           ┆           ┆   ┆ similarit ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ y         ┆           ┆           ┆          │\n",
       "│ P3V3-Q002 ┆ Meta Plat ┆ cross_yea ┆ hard      ┆ … ┆ Strong    ┆ 2858.2    ┆ 11541     ┆ 0.054807 │\n",
       "│           ┆ forms,    ┆ r         ┆           ┆   ┆ similarit ┆           ┆           ┆          │\n",
       "│           ┆ Inc.      ┆           ┆           ┆   ┆ y         ┆           ┆           ┆          │\n",
       "│ P3V2-Q001 ┆ EXXON     ┆ local     ┆ easy      ┆ … ┆ Strong    ┆ 2238.0    ┆ 5976      ┆ 0.020724 │\n",
       "│           ┆ MOBIL     ┆           ┆           ┆   ┆ similarit ┆           ┆           ┆          │\n",
       "│           ┆ CORP      ┆           ┆           ┆   ┆ y         ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Batch Evaluation - Run Selected Questions & Compute Metrics\n",
    "# ============================================================================\n",
    "\n",
    "# CRITICAL: Suppress verbose logging for clean output\n",
    "import logging\n",
    "logging.getLogger(\"finrag_ml_tg1\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "import polars as pl\n",
    "import time\n",
    "from finrag_ml_tg1.rag_modules_src.synthesis_pipeline.orchestrator import answer_query\n",
    "from finrag_ml_tg1.rag_modules_src.utilities.evaluation_metrics import evaluate_answer\n",
    "\n",
    "# Configure which questions to test\n",
    "TEST_QUESTION_IDS = [\n",
    "    # TIER 1: EXCELLENT QUESTIONS (5)\n",
    "    \"P3V3-Q001\",  # Walmart Debt Strategy 2018-2020 (cross-year, medium, 4 evidence)\n",
    "    \"P3V2-Q006\",  # Microsoft Intelligent Cloud 2017 (local, medium, 1 evidence)\n",
    "    \"P3V3-Q004\",  # Cross-Company Cyber 2009 (cross-company, medium, 3 evidence)\n",
    "    \"P3V3-Q007\",  # Tesla Adjusted EBITDA 2022 (local, easy, 1 evidence)\n",
    "    \"P3V3-Q002\",  # Meta Regulatory Evolution 2019-2024 (cross-year, hard, 4 evidence)\n",
    "    \n",
    "    # TIER 3: BAD QUESTION (1 - for comparison)\n",
    "    \"P3V2-Q001\",  # Exxon Revenue 2008 (BAD: cross-reference answer, likely missing data)\n",
    "]\n",
    "\n",
    "# Rate limit protection\n",
    "SLEEP_BETWEEN_QUERIES = 70  # 70 seconds = safe for any rate limit\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"BATCH EVALUATION: {len(TEST_QUESTION_IDS)} QUESTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: Claude Sonnet 4.5\")\n",
    "print(f\"Rate limit protection: {SLEEP_BETWEEN_QUERIES}s delay between queries\")\n",
    "print(f\"Estimated total runtime: ~{(len(TEST_QUESTION_IDS) * SLEEP_BETWEEN_QUERIES + len(TEST_QUESTION_IDS) * 10)/60:.1f} min\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, qid in enumerate(TEST_QUESTION_IDS, 1):\n",
    "    print(f\"\\n[{i}/{len(TEST_QUESTION_IDS)}] Processing {qid}...\")\n",
    "    \n",
    "    # Get question data\n",
    "    question_data = test_suite[qid]\n",
    "    query = question_data[\"question_text\"]\n",
    "    gold_answer = question_data[\"gold_answer\"]\n",
    "    \n",
    "    # Run synthesis\n",
    "    print(f\"  - Running synthesis...\")\n",
    "    result = answer_query(\n",
    "        query=query,\n",
    "        model_root=model_root,\n",
    "        include_kpi=True,\n",
    "        include_rag=True,\n",
    "        model_key=\"development_CL_SONN_4_5\",  # Sonnet 4.5\n",
    "        export_context=True,\n",
    "        export_response=True\n",
    "    )\n",
    "    \n",
    "    # Extract answer\n",
    "    if result.get('error'):\n",
    "        print(f\"  - ERROR: {result['error']}\")\n",
    "        continue\n",
    "    \n",
    "    llm_answer = result['answer']\n",
    "    llm_meta = result['metadata']['llm']\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    print(f\"  - Computing metrics...\")\n",
    "    eval_scores = evaluate_answer(\n",
    "        gold_answer=gold_answer,\n",
    "        synthesis_answer=llm_answer,\n",
    "        include_bleurt=True,\n",
    "        include_timing=True\n",
    "    )\n",
    "    \n",
    "    # Collect results\n",
    "    results.append({\n",
    "        \"question_id\": qid,\n",
    "        \"company\": \", \".join(question_data[\"companies\"]),\n",
    "        \"scope\": question_data[\"retrieval_scope\"],\n",
    "        \"difficulty\": question_data[\"difficulty\"],\n",
    "        \"rouge_l\": eval_scores[\"rouge_l\"],\n",
    "        \"bertscore_f1\": eval_scores[\"bertscore_f1\"],\n",
    "        \"cosine_sim\": eval_scores[\"cosine_sim\"],\n",
    "        \"bleurt\": eval_scores[\"bleurt\"],\n",
    "        \"interpretation\": eval_scores[\"interpretation\"],\n",
    "        \"eval_time_ms\": eval_scores[\"timing\"][\"total_ms\"],\n",
    "        \"synthesis_tokens\": llm_meta[\"total_tokens\"],\n",
    "        \"synthesis_cost\": llm_meta[\"cost\"],\n",
    "    })\n",
    "    \n",
    "    print(f\"  ✓ BERTScore: {eval_scores['bertscore_f1']:.3f} | BLEURT: {eval_scores['bleurt']:.3f} | Time: {eval_scores['timing']['total_ms']/1000:.1f}s\")\n",
    "    \n",
    "    # Rate limit protection (skip after last question)\n",
    "    if i < len(TEST_QUESTION_IDS):\n",
    "        print(f\"  - Cooling down ({SLEEP_BETWEEN_QUERIES}s)...\")\n",
    "        time.sleep(SLEEP_BETWEEN_QUERIES)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pl.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Display results\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2fda991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "Questions Evaluated: 6\n",
      "Average BERTScore F1: 0.826\n",
      "Average BLEURT: 0.446\n",
      "Average ROUGE-L: 0.099\n",
      "Average Cosine Similarity: 0.675\n",
      "Average Eval Time: 3.5s\n",
      "Total Synthesis Cost: $0.1806\n",
      "Total Runtime: ~7.4 min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Summary statistics\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Questions Evaluated: {df_results.height}\")\n",
    "print(f\"Average BERTScore F1: {df_results['bertscore_f1'].mean():.3f}\")\n",
    "print(f\"Average BLEURT: {df_results['bleurt'].mean():.3f}\")\n",
    "print(f\"Average ROUGE-L: {df_results['rouge_l'].mean():.3f}\")\n",
    "print(f\"Average Cosine Similarity: {df_results['cosine_sim'].mean():.3f}\")\n",
    "print(f\"Average Eval Time: {df_results['eval_time_ms'].mean()/1000:.1f}s\")\n",
    "print(f\"Total Synthesis Cost: ${df_results['synthesis_cost'].sum():.4f}\")\n",
    "print(f\"Total Runtime: ~{(len(TEST_QUESTION_IDS) * SLEEP_BETWEEN_QUERIES + df_results['eval_time_ms'].sum()/1000)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e4dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
