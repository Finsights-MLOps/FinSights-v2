{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a419a8",
   "metadata": {},
   "source": [
    "### Manual Test and Replace Scripts, Analytics, Ad-hoc test analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0087b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 1: Configuration\n",
    "INCR_PATH = \"s3://sentence-data-ingestion/DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\"\n",
    "# ↑ Swap this line to test different incremental sources\n",
    "\n",
    "# Cell 2: Load Config & Setup\n",
    "config = ETLConfig()  # Gets hist_path, bucket, credentials\n",
    "s3 = config.get_s3_client()\n",
    "storage_options = config.get_storage_options()\n",
    "\n",
    "# Cell 3: Pre-flight Validation\n",
    "# Check HIST exists, Check INCR exists, Verify S3 access\n",
    "\n",
    "# Cell 4: Duplicate Analysis  \n",
    "# Load both files, count internal dupes, check overlap\n",
    "# Show final comparison DataFrame\n",
    "\n",
    "# Cell 5: Schema Inspector\n",
    "# Compare columns, show mapping applied\n",
    "# Display schema comparison DataFrame\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a658f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with incremental path:\n",
      "  Full URI: s3://sentence-data-ingestion/DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\n",
      "  S3 Key: DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick-swap incremental path for testing\n",
    "Provide full S3 URI - we'll extract the key automatically\n",
    "\"\"\"\n",
    "\n",
    "# === SWAP THIS PATH TO TEST DIFFERENT INCREMENTAL SOURCES ===\n",
    "INCR_PATH_URI = \"s3://sentence-data-ingestion/DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\"\n",
    "\n",
    "# Alternative paths (uncomment to test):\n",
    "# INCR_PATH_URI = \"s3://sentence-data-ingestion/DATA_MERGE_ASSETS/INCREMENTAL_DATA/finrag_sec_incremental_stg_data.parquet\"\n",
    "\n",
    "# Extract S3 key from URI (remove s3://bucket-name/ prefix)\n",
    "INCR_PATH = INCR_PATH_URI.replace(f\"s3://{config.bucket}/\", \"\") if \"s3://\" in INCR_PATH_URI else INCR_PATH_URI\n",
    "\n",
    "print(f\"Testing with incremental path:\")\n",
    "print(f\"  Full URI: {INCR_PATH_URI}\")\n",
    "print(f\"  S3 Key: {INCR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ca02f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Modular file exists but empty/invalid\n",
      "[DEBUG] Trying root fallback: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\DataPipeline\\.env\n",
      "[DEBUG] ✓ Credentials loaded from root .env\n",
      "Configuration loaded:\n",
      "  Bucket: sentence-data-ingestion\n",
      "  Historical: DATA_MERGE_ASSETS/HISTORICAL_DATA/finrag_sec_fact_historical.parquet\n",
      "  Incremental: DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load ETL config and AWS clients\n",
    "Historical path comes from config, incremental path from Cell 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import boto3\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd().parent.parent if 'notebooks' in str(Path.cwd()) else Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src_aws_etl.etl.config_loader import ETLConfig\n",
    "\n",
    "# Load config (gets historical path, bucket, credentials)\n",
    "config = ETLConfig()\n",
    "s3 = config.get_s3_client()\n",
    "storage_options = config.get_storage_options()\n",
    "\n",
    "# Paths\n",
    "HIST_PATH = config.hist_path  # From config (always same)\n",
    "BUCKET = config.bucket\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Bucket: {BUCKET}\")\n",
    "print(f\"  Historical: {HIST_PATH}\")\n",
    "print(f\"  Incremental: {INCR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5fb01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-flight Validation\n",
      "\n",
      "✓ Historical: 13.41 MB\n",
      "✓ Incremental: 8.88 MB\n",
      "✓ S3 permissions: OK\n",
      "\n",
      "Ready to proceed with analysis\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validate both files exist and are accessible\n",
    "\"\"\"\n",
    "\n",
    "def check_s3_file(s3_client, bucket, key):\n",
    "    \"\"\"Check if S3 file exists and return size in MB\"\"\"\n",
    "    response = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "    size_mb = response['ContentLength'] / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "print(\"Pre-flight Validation\\n\")\n",
    "\n",
    "# Check Historical\n",
    "hist_size = check_s3_file(s3, BUCKET, HIST_PATH)\n",
    "print(f\"✓ Historical: {hist_size:.2f} MB\")\n",
    "\n",
    "# Check Incremental\n",
    "incr_size = check_s3_file(s3, BUCKET, INCR_PATH)\n",
    "print(f\"✓ Incremental: {incr_size:.2f} MB\")\n",
    "\n",
    "# Verify S3 list permissions\n",
    "s3.list_objects_v2(Bucket=BUCKET, Prefix=\"DATA_MERGE_ASSETS/\", MaxKeys=1)\n",
    "print(f\"✓ S3 permissions: OK\")\n",
    "\n",
    "print(f\"\\nReady to proceed with analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66813c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with file paths:\n",
      "  Historical: https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/HISTORICAL_DATA/finrag_sec_fact_historical.parquet\n",
      "  Incremental (SDK): https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\n",
      "  Incremental (Crawled): https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/INCREMENTAL_DATA/finrag_sec_incremental_stg_data.parquet\n",
      "  Final: https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/FINRAG_FACT_SENTENCES/finrag_fact_sentences.parquet\n",
      "Loading files and analyzing...\n",
      "\n",
      "✓ Historical: 287,066 rows (13.4 MB)\n",
      "✓ Incremental (SDK): 90,072 rows (8.9 MB)\n",
      "✓ Incremental (Crawled): 36,999 rows (3.2 MB)\n",
      "✓ Final: 469,252 rows (23.1 MB)\n",
      "\n",
      "Duplicate Analysis Summary\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>File</th><th>File Size</th><th>Total Rows</th><th>Unique IDs</th><th>Internal Dupes</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Historical&quot;</td><td>&quot;13.4 MB&quot;</td><td>&quot;287,066&quot;</td><td>&quot;287,066&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;Incremental (SDK)&quot;</td><td>&quot;8.9 MB&quot;</td><td>&quot;90,072&quot;</td><td>&quot;86,654&quot;</td><td>&quot;3,418&quot;</td></tr><tr><td>&quot;Incremental (Crawled)&quot;</td><td>&quot;3.2 MB&quot;</td><td>&quot;36,999&quot;</td><td>&quot;36,999&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;Final&quot;</td><td>&quot;23.1 MB&quot;</td><td>&quot;469,252&quot;</td><td>&quot;469,252&quot;</td><td>&quot;0&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 5)\n",
       "┌───────────────────────┬───────────┬────────────┬────────────┬────────────────┐\n",
       "│ File                  ┆ File Size ┆ Total Rows ┆ Unique IDs ┆ Internal Dupes │\n",
       "│ ---                   ┆ ---       ┆ ---        ┆ ---        ┆ ---            │\n",
       "│ str                   ┆ str       ┆ str        ┆ str        ┆ str            │\n",
       "╞═══════════════════════╪═══════════╪════════════╪════════════╪════════════════╡\n",
       "│ Historical            ┆ 13.4 MB   ┆ 287,066    ┆ 287,066    ┆ 0              │\n",
       "│ Incremental (SDK)     ┆ 8.9 MB    ┆ 90,072     ┆ 86,654     ┆ 3,418          │\n",
       "│ Incremental (Crawled) ┆ 3.2 MB    ┆ 36,999     ┆ 36,999     ┆ 0              │\n",
       "│ Final                 ┆ 23.1 MB   ┆ 469,252    ┆ 469,252    ┆ 0              │\n",
       "└───────────────────────┴───────────┴────────────┴────────────┴────────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick-swap paths for testing\n",
    "Set all 4 file paths here\n",
    "\"\"\"\n",
    "\n",
    "# === FILE PATHS - CHANGE THESE TO TEST DIFFERENT SOURCES ===\n",
    "HIST_PATH_URI = \"https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/HISTORICAL_DATA/finrag_sec_fact_historical.parquet\"\n",
    "INCR_SDK_URI = \"https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/INCREMENTAL_DATA_SDK/finrag_sec_incremental_stg_data.parquet\"\n",
    "INCR_CRAWL_URI = \"https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/INCREMENTAL_DATA/finrag_sec_incremental_stg_data.parquet\"\n",
    "FINAL_PATH_URI = \"https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/DATA_MERGE_ASSETS/FINRAG_FACT_SENTENCES/finrag_fact_sentences.parquet\"\n",
    "\n",
    "print(\"Testing with file paths:\")\n",
    "print(f\"  Historical: {HIST_PATH_URI}\")\n",
    "print(f\"  Incremental (SDK): {INCR_SDK_URI}\")\n",
    "print(f\"  Incremental (Crawled): {INCR_CRAWL_URI}\")\n",
    "print(f\"  Final: {FINAL_PATH_URI}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Analyze duplicates across all 4 files with size display\n",
    "No cross-file overlap - just internal duplicate counts\n",
    "\"\"\"\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Convert HTTPS URLs to S3 URIs for Polars\n",
    "def https_to_s3(url):\n",
    "    \"\"\"Convert S3 HTTPS URL to s3:// URI\"\"\"\n",
    "    return url.replace(\"https://sentence-data-ingestion.s3.us-east-1.amazonaws.com/\", \"s3://sentence-data-ingestion/\")\n",
    "\n",
    "def get_file_size_mb(s3_client, bucket, url):\n",
    "    \"\"\"Get file size from S3 in MB\"\"\"\n",
    "    # Extract key from URL\n",
    "    key = url.replace(f\"https://{bucket}.s3.us-east-1.amazonaws.com/\", \"\")\n",
    "    response = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "    return response['ContentLength'] / (1024 * 1024)\n",
    "\n",
    "# File definitions\n",
    "files = {\n",
    "    'Historical': HIST_PATH_URI,\n",
    "    'Incremental (SDK)': INCR_SDK_URI,\n",
    "    'Incremental (Crawled)': INCR_CRAWL_URI,\n",
    "    'Final': FINAL_PATH_URI\n",
    "}\n",
    "\n",
    "print(\"Loading files and analyzing...\\n\")\n",
    "\n",
    "# Collect analysis results\n",
    "results = []\n",
    "\n",
    "for file_name, url in files.items():\n",
    "    # Get file size\n",
    "    file_size_mb = get_file_size_mb(s3, BUCKET, url)\n",
    "    \n",
    "    # Load data\n",
    "    s3_uri = https_to_s3(url)\n",
    "    df = pl.read_parquet(s3_uri, storage_options=storage_options)\n",
    "    \n",
    "    # Analyze duplicates\n",
    "    total_rows = len(df)\n",
    "    unique_ids = df['sentenceID'].n_unique()\n",
    "    internal_dupes = total_rows - unique_ids\n",
    "    \n",
    "    results.append({\n",
    "        'File': file_name,\n",
    "        'File Size': f\"{file_size_mb:.1f} MB\",\n",
    "        'Total Rows': f\"{total_rows:,}\",\n",
    "        'Unique IDs': f\"{unique_ids:,}\",\n",
    "        'Internal Dupes': f\"{internal_dupes:,}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ {file_name}: {total_rows:,} rows ({file_size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"\\nDuplicate Analysis Summary\\n\")\n",
    "pl.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67329add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 90,072 rows from Incremental (SDK)\n",
      "Found 86,654 unique IDs\n",
      "Total duplicates: 3,418\n",
      "\n",
      "Analyzing dup rows: 6,836 :/ \n",
      "\n",
      "Duplicate Breakdown by Year and Company\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>report_year</th><th>name</th><th>Duplicate Rows</th><th>Unique IDs Affected</th></tr><tr><td>i64</td><td>str</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>2021</td><td>&quot;Alphabet Inc.&quot;</td><td>1842</td><td>921</td></tr><tr><td>2022</td><td>&quot;Alphabet Inc.&quot;</td><td>1678</td><td>839</td></tr><tr><td>2023</td><td>&quot;Alphabet Inc.&quot;</td><td>1566</td><td>783</td></tr><tr><td>2024</td><td>&quot;Alphabet Inc.&quot;</td><td>1750</td><td>875</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌─────────────┬───────────────┬────────────────┬─────────────────────┐\n",
       "│ report_year ┆ name          ┆ Duplicate Rows ┆ Unique IDs Affected │\n",
       "│ ---         ┆ ---           ┆ ---            ┆ ---                 │\n",
       "│ i64         ┆ str           ┆ u32            ┆ u32                 │\n",
       "╞═════════════╪═══════════════╪════════════════╪═════════════════════╡\n",
       "│ 2021        ┆ Alphabet Inc. ┆ 1842           ┆ 921                 │\n",
       "│ 2022        ┆ Alphabet Inc. ┆ 1678           ┆ 839                 │\n",
       "│ 2023        ┆ Alphabet Inc. ┆ 1566           ┆ 783                 │\n",
       "│ 2024        ┆ Alphabet Inc. ┆ 1750           ┆ 875                 │\n",
       "└─────────────┴───────────────┴────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyze duplicates in SDK file by report_year and company name\n",
    "Shows where the 3,418 duplicates are concentrated\n",
    "\"\"\"\n",
    "\n",
    "# Load SDK file (the one with duplicates)\n",
    "s3_uri_sdk = https_to_s3(INCR_SDK_URI)\n",
    "df_sdk = pl.read_parquet(s3_uri_sdk, storage_options=storage_options)\n",
    "\n",
    "print(f\"Analyzing {len(df_sdk):,} rows from Incremental (SDK)\")\n",
    "print(f\"Found {df_sdk['sentenceID'].n_unique():,} unique IDs\")\n",
    "print(f\"Total duplicates: {len(df_sdk) - df_sdk['sentenceID'].n_unique():,}\\n\")\n",
    "\n",
    "# Identify duplicate sentenceIDs\n",
    "duplicate_ids = (\n",
    "    df_sdk\n",
    "    .group_by('sentenceID')\n",
    "    .agg(pl.len().alias('count'))\n",
    "    .filter(pl.col('count') > 1)\n",
    "    .select('sentenceID')\n",
    ")\n",
    "\n",
    "# Filter to only duplicate rows\n",
    "duplicates_df = df_sdk.join(duplicate_ids, on='sentenceID', how='inner')\n",
    "\n",
    "print(f\"Analyzing dup rows: {len(duplicates_df):,} :/ \\n\")\n",
    "\n",
    "# Breakdown by Year and Company\n",
    "breakdown = (\n",
    "    duplicates_df\n",
    "    .group_by(['report_year', 'name'])\n",
    "    .agg([\n",
    "        pl.len().alias('Duplicate Rows'),\n",
    "        pl.col('sentenceID').n_unique().alias('Unique IDs Affected')\n",
    "    ])\n",
    "    .sort(['report_year', 'Duplicate Rows'], descending=[False, True])\n",
    ")\n",
    "\n",
    "print(\"Duplicate Breakdown by Year and Company\\n\")\n",
    "breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd073c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year-Level Summary\n",
      "\n",
      "shape: (4, 3)\n",
      "┌─────────────┬──────────────────┬────────────────────┐\n",
      "│ report_year ┆ Total Duplicates ┆ Companies Affected │\n",
      "│ ---         ┆ ---              ┆ ---                │\n",
      "│ i64         ┆ u32              ┆ u32                │\n",
      "╞═════════════╪══════════════════╪════════════════════╡\n",
      "│ 2021        ┆ 1842             ┆ 1                  │\n",
      "│ 2022        ┆ 1678             ┆ 1                  │\n",
      "│ 2023        ┆ 1566             ┆ 1                  │\n",
      "│ 2024        ┆ 1750             ┆ 1                  │\n",
      "└─────────────┴──────────────────┴────────────────────┘\n",
      "\n",
      "==================================================\n",
      "\n",
      "Top 10 Companies with Most Duplicates\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>Total Duplicates</th><th>Years Affected</th></tr><tr><td>str</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Alphabet Inc.&quot;</td><td>6836</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "┌───────────────┬──────────────────┬────────────────┐\n",
       "│ name          ┆ Total Duplicates ┆ Years Affected │\n",
       "│ ---           ┆ ---              ┆ ---            │\n",
       "│ str           ┆ u32              ┆ u32            │\n",
       "╞═══════════════╪══════════════════╪════════════════╡\n",
       "│ Alphabet Inc. ┆ 6836             ┆ 4              │\n",
       "└───────────────┴──────────────────┴────────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick summary of duplicate distribution\n",
    "\"\"\"\n",
    "\n",
    "# Year-level summary\n",
    "year_summary = (\n",
    "    duplicates_df\n",
    "    .group_by('report_year')\n",
    "    .agg([\n",
    "        pl.len().alias('Total Duplicates'),\n",
    "        pl.col('name').n_unique().alias('Companies Affected')\n",
    "    ])\n",
    "    .sort('report_year')\n",
    ")\n",
    "\n",
    "print(\"Year-Level Summary\\n\")\n",
    "print(year_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Company-level summary (top 10)\n",
    "company_summary = (\n",
    "    duplicates_df\n",
    "    .group_by('name')\n",
    "    .agg([\n",
    "        pl.len().alias('Total Duplicates'),\n",
    "        pl.col('report_year').n_unique().alias('Years Affected')\n",
    "    ])\n",
    "    .sort('Total Duplicates', descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 Companies with Most Duplicates\\n\")\n",
    "company_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare schemas between historical and incremental\n",
    "\"\"\"\n",
    "\n",
    "# Column mapping rules (if incremental uses different names)\n",
    "COLUMN_MAPPINGS = {\n",
    "    'SIC': 'sic',\n",
    "    'section_item': 'section_name',\n",
    "}\n",
    "\n",
    "# Derived columns (computed during merge, OK to differ)\n",
    "DERIVED_COLUMNS = {\n",
    "    'cik_int', 'has_comparison', 'has_numbers', 'likely_kpi',\n",
    "    'row_hash', 'tickers', 'sentence_index',\n",
    "}\n",
    "\n",
    "# Read schemas (just 1 row for speed)\n",
    "print(\"Reading schemas...\\n\")\n",
    "hist_schema_df = pl.read_parquet(hist_uri, n_rows=1, storage_options=storage_options)\n",
    "incr_schema_df = pl.read_parquet(incr_uri, n_rows=1, storage_options=storage_options)\n",
    "\n",
    "hist_schema = hist_schema_df.schema\n",
    "incr_schema = incr_schema_df.schema\n",
    "\n",
    "print(f\"Historical: {len(hist_schema)} columns\")\n",
    "print(f\"Incremental: {len(incr_schema)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2a8c2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading schemas...\n",
      "\n",
      "Historical: 24 columns\n",
      "Incremental (SDK): 20 columns\n",
      "\n",
      "Column Mapping Rules Applied:\n",
      "  SIC                  → sic                 \n",
      "  section_item         → section_name        \n",
      "\n",
      "Schema Comparison: Historical vs Incremental (SDK)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (25, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Column</th><th>Historical Type</th><th>Incremental Type</th><th>Status</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;cik&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;cik_int&quot;</td><td>&quot;Int32&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;✓ Derived&quot;</td></tr><tr><td>&quot;docID&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;filingDate&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;form&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;has_comparison&quot;</td><td>&quot;Boolean&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;✓ Derived&quot;</td></tr><tr><td>&quot;has_numbers&quot;</td><td>&quot;Boolean&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;✓ Derived&quot;</td></tr><tr><td>&quot;last_modified_date&quot;</td><td>&quot;Datetime(time_unit=&#x27;us&#x27;, time_zone=&#x27;UTC&#x27;)&quot;</td><td>&quot;Datetime(time_unit=&#x27;ns&#x27;, time_zone=None)&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;likely_kpi&quot;</td><td>&quot;Boolean&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;✓ Derived&quot;</td></tr><tr><td>&quot;load_method&quot;</td><td>&quot;String&quot;</td><td>&quot;Null&quot;</td><td>&quot;❌ Type Diff&quot;</td></tr><tr><td>&quot;name&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;reportDate&quot;</td><td>&quot;String&quot;</td><td>&quot;Null&quot;</td><td>&quot;❌ Type Diff&quot;</td></tr><tr><td>&quot;report_year&quot;</td><td>&quot;Int64&quot;</td><td>&quot;Int64&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;row_hash&quot;</td><td>&quot;String&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;✓ Derived&quot;</td></tr><tr><td>&quot;sample_created_at&quot;</td><td>&quot;Datetime(time_unit=&#x27;us&#x27;, time_zone=&#x27;UTC&#x27;)&quot;</td><td>&quot;Datetime(time_unit=&#x27;ns&#x27;, time_zone=None)&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;sample_version&quot;</td><td>&quot;String&quot;</td><td>&quot;Null&quot;</td><td>&quot;❌ Type Diff&quot;</td></tr><tr><td>&quot;section_ID&quot;</td><td>&quot;Int64&quot;</td><td>&quot;Int64&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;section_name (section_item→)&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;sentence&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;sentenceID&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;sentence_index&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;Int64&quot;</td><td>&quot;✓ Derived&quot;</td></tr><tr><td>&quot;sic (SIC→)&quot;</td><td>&quot;String&quot;</td><td>&quot;String&quot;</td><td>&quot;✓ Match&quot;</td></tr><tr><td>&quot;source_file_path&quot;</td><td>&quot;String&quot;</td><td>&quot;Null&quot;</td><td>&quot;❌ Type Diff&quot;</td></tr><tr><td>&quot;temporal_bin&quot;</td><td>&quot;String&quot;</td><td>&quot;Null&quot;</td><td>&quot;❌ Type Diff&quot;</td></tr><tr><td>&quot;tickers&quot;</td><td>&quot;List(String)&quot;</td><td>&quot;MISSING&quot;</td><td>&quot;✓ Derived&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (25, 4)\n",
       "┌──────────────────────────────┬───────────────────────────────────────────┬──────────────────────────────────────────┬──────────────┐\n",
       "│ Column                       ┆ Historical Type                           ┆ Incremental Type                         ┆ Status       │\n",
       "│ ---                          ┆ ---                                       ┆ ---                                      ┆ ---          │\n",
       "│ str                          ┆ str                                       ┆ str                                      ┆ str          │\n",
       "╞══════════════════════════════╪═══════════════════════════════════════════╪══════════════════════════════════════════╪══════════════╡\n",
       "│ cik                          ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ cik_int                      ┆ Int32                                     ┆ MISSING                                  ┆ ✓ Derived    │\n",
       "│ docID                        ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ filingDate                   ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ form                         ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ has_comparison               ┆ Boolean                                   ┆ MISSING                                  ┆ ✓ Derived    │\n",
       "│ has_numbers                  ┆ Boolean                                   ┆ MISSING                                  ┆ ✓ Derived    │\n",
       "│ last_modified_date           ┆ Datetime(time_unit='us', time_zone='UTC') ┆ Datetime(time_unit='ns', time_zone=None) ┆ ✓ Match      │\n",
       "│ likely_kpi                   ┆ Boolean                                   ┆ MISSING                                  ┆ ✓ Derived    │\n",
       "│ load_method                  ┆ String                                    ┆ Null                                     ┆ ❌ Type Diff │\n",
       "│ name                         ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ reportDate                   ┆ String                                    ┆ Null                                     ┆ ❌ Type Diff │\n",
       "│ report_year                  ┆ Int64                                     ┆ Int64                                    ┆ ✓ Match      │\n",
       "│ row_hash                     ┆ String                                    ┆ MISSING                                  ┆ ✓ Derived    │\n",
       "│ sample_created_at            ┆ Datetime(time_unit='us', time_zone='UTC') ┆ Datetime(time_unit='ns', time_zone=None) ┆ ✓ Match      │\n",
       "│ sample_version               ┆ String                                    ┆ Null                                     ┆ ❌ Type Diff │\n",
       "│ section_ID                   ┆ Int64                                     ┆ Int64                                    ┆ ✓ Match      │\n",
       "│ section_name (section_item→) ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ sentence                     ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ sentenceID                   ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ sentence_index               ┆ MISSING                                   ┆ Int64                                    ┆ ✓ Derived    │\n",
       "│ sic (SIC→)                   ┆ String                                    ┆ String                                   ┆ ✓ Match      │\n",
       "│ source_file_path             ┆ String                                    ┆ Null                                     ┆ ❌ Type Diff │\n",
       "│ temporal_bin                 ┆ String                                    ┆ Null                                     ┆ ❌ Type Diff │\n",
       "│ tickers                      ┆ List(String)                              ┆ MISSING                                  ┆ ✓ Derived    │\n",
       "└──────────────────────────────┴───────────────────────────────────────────┴──────────────────────────────────────────┴──────────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schema Inspector - Historical vs Incremental (SDK)\n",
    "Smart comparison with column mapping rules and derived column handling\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# POLARS DISPLAY CONFIGURATION - Show Full DataFrame\n",
    "# ============================================================\n",
    "pl.Config.set_tbl_rows(-1)  # Show all rows (no truncation)\n",
    "pl.Config.set_tbl_cols(-1)  # Show all columns\n",
    "pl.Config.set_fmt_str_lengths(100)  # Allow longer string display\n",
    "pl.Config.set_tbl_width_chars(1000)  # Wider table display\n",
    "\n",
    "# Column mapping rules (if incremental uses different names)\n",
    "COLUMN_MAPPINGS = {\n",
    "    'SIC': 'sic',\n",
    "    'section_item': 'section_name',\n",
    "}\n",
    "\n",
    "# Derived columns (can be computed during merge, OK to differ)\n",
    "DERIVED_COLUMNS = {\n",
    "    'cik_int', 'has_comparison', 'has_numbers', 'likely_kpi',\n",
    "    'row_hash', 'tickers', 'sentence_index',\n",
    "}\n",
    "\n",
    "# Load schemas\n",
    "hist_uri = f\"s3://{BUCKET}/{HIST_PATH}\"\n",
    "incr_uri = https_to_s3(INCR_SDK_URI)\n",
    "\n",
    "print(\"Reading schemas...\\n\")\n",
    "hist_df = pl.read_parquet(hist_uri, n_rows=1, storage_options=storage_options)\n",
    "incr_df = pl.read_parquet(incr_uri, n_rows=1, storage_options=storage_options)\n",
    "\n",
    "hist_schema = hist_df.schema\n",
    "incr_schema = incr_df.schema\n",
    "\n",
    "print(f\"Historical: {len(hist_schema)} columns\")\n",
    "print(f\"Incremental (SDK): {len(incr_schema)} columns\\n\")\n",
    "\n",
    "# Apply column mappings to incremental\n",
    "incr_mapped = {}\n",
    "for col, dtype in incr_schema.items():\n",
    "    mapped_col = COLUMN_MAPPINGS.get(col, col)\n",
    "    incr_mapped[mapped_col] = (col, dtype)\n",
    "\n",
    "# Show mapping rules applied\n",
    "print(\"Column Mapping Rules Applied:\")\n",
    "for incr_col, hist_col in COLUMN_MAPPINGS.items():\n",
    "    if incr_col in incr_schema:\n",
    "        print(f\"  {incr_col:20s} → {hist_col:20s}\")\n",
    "print()\n",
    "\n",
    "# Get all unique columns (after mapping)\n",
    "all_cols = sorted(set(hist_schema.keys()) | set(incr_mapped.keys()))\n",
    "\n",
    "# Build comparison\n",
    "comparison_rows = []\n",
    "matches = []\n",
    "hist_only = []\n",
    "incr_only = []\n",
    "type_diffs = []\n",
    "\n",
    "for col in all_cols:\n",
    "    hist_type = str(hist_schema.get(col, \"MISSING\"))\n",
    "    \n",
    "    # Check if incremental has this column (after mapping)\n",
    "    incr_orig_col, incr_type_val = incr_mapped.get(col, (None, None))\n",
    "    incr_type = str(incr_type_val) if incr_type_val else \"MISSING\"\n",
    "    \n",
    "    # Normalize datetime types for comparison\n",
    "    hist_norm = hist_type.replace(\"time_unit='us'\", \"TU\").replace(\"time_unit='ns'\", \"TU\")\n",
    "    incr_norm = incr_type.replace(\"time_unit='us'\", \"TU\").replace(\"time_unit='ns'\", \"TU\")\n",
    "    hist_norm = hist_norm.replace(\"time_zone='UTC'\", \"TZ\").replace(\"time_zone=None\", \"TZ\")\n",
    "    incr_norm = incr_norm.replace(\"time_zone='UTC'\", \"TZ\").replace(\"time_zone=None\", \"TZ\")\n",
    "    \n",
    "    # Determine status\n",
    "    if col not in hist_schema:\n",
    "        status = \"✓ Derived\" if col in DERIVED_COLUMNS else \"⚠ Incr Only\"\n",
    "        incr_only.append(col)\n",
    "    elif incr_orig_col is None:\n",
    "        status = \"✓ Derived\" if col in DERIVED_COLUMNS else \"⚠ Hist Only\"\n",
    "        hist_only.append(col)\n",
    "    elif hist_norm == incr_norm:\n",
    "        status = \"✓ Match\"\n",
    "        matches.append(col)\n",
    "    else:\n",
    "        if 'Datetime' in hist_type and 'Datetime' in incr_type:\n",
    "            status = \"✓ Match (datetime)\"\n",
    "            matches.append(col)\n",
    "        else:\n",
    "            status = \"❌ Type Diff\"\n",
    "            type_diffs.append(col)\n",
    "    \n",
    "    # Show mapped name if different\n",
    "    display_col = f\"{col} ({incr_orig_col}→)\" if incr_orig_col and incr_orig_col != col else col\n",
    "    \n",
    "    comparison_rows.append({\n",
    "        'Column': display_col,\n",
    "        'Historical Type': hist_type,\n",
    "        'Incremental Type': incr_type,\n",
    "        'Status': status\n",
    "    })\n",
    "\n",
    "schema_comparison = pl.DataFrame(comparison_rows)\n",
    "\n",
    "print(\"Schema Comparison: Historical vs Incremental (SDK)\\n\")\n",
    "schema_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bece398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full datasets for null analysis...\n",
      "\n",
      "Historical file: 287,066 total rows\n",
      "Incremental (SDK) file: 90,072 total rows\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Null Value Analysis (Full Data)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Column</th><th>Hist: Total</th><th>Hist: Not Null</th><th>Hist: Null</th><th>Hist: Null %</th><th>Incr: Total</th><th>Incr: Not Null</th><th>Incr: Null</th><th>Incr: Null %</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;load_method&quot;</td><td>&quot;287,066&quot;</td><td>&quot;287,066&quot;</td><td>&quot;0&quot;</td><td>&quot;0.0%&quot;</td><td>&quot;90,072&quot;</td><td>&quot;0&quot;</td><td>&quot;90,072&quot;</td><td>&quot;100.0%&quot;</td></tr><tr><td>&quot;reportDate&quot;</td><td>&quot;287,066&quot;</td><td>&quot;287,066&quot;</td><td>&quot;0&quot;</td><td>&quot;0.0%&quot;</td><td>&quot;90,072&quot;</td><td>&quot;0&quot;</td><td>&quot;90,072&quot;</td><td>&quot;100.0%&quot;</td></tr><tr><td>&quot;sample_version&quot;</td><td>&quot;287,066&quot;</td><td>&quot;287,066&quot;</td><td>&quot;0&quot;</td><td>&quot;0.0%&quot;</td><td>&quot;90,072&quot;</td><td>&quot;0&quot;</td><td>&quot;90,072&quot;</td><td>&quot;100.0%&quot;</td></tr><tr><td>&quot;source_file_path&quot;</td><td>&quot;287,066&quot;</td><td>&quot;287,066&quot;</td><td>&quot;0&quot;</td><td>&quot;0.0%&quot;</td><td>&quot;90,072&quot;</td><td>&quot;0&quot;</td><td>&quot;90,072&quot;</td><td>&quot;100.0%&quot;</td></tr><tr><td>&quot;temporal_bin&quot;</td><td>&quot;287,066&quot;</td><td>&quot;287,066&quot;</td><td>&quot;0&quot;</td><td>&quot;0.0%&quot;</td><td>&quot;90,072&quot;</td><td>&quot;0&quot;</td><td>&quot;90,072&quot;</td><td>&quot;100.0%&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌──────────────────┬─────────────┬────────────────┬────────────┬──────────────┬─────────────┬────────────────┬────────────┬──────────────┐\n",
       "│ Column           ┆ Hist: Total ┆ Hist: Not Null ┆ Hist: Null ┆ Hist: Null % ┆ Incr: Total ┆ Incr: Not Null ┆ Incr: Null ┆ Incr: Null % │\n",
       "│ ---              ┆ ---         ┆ ---            ┆ ---        ┆ ---          ┆ ---         ┆ ---            ┆ ---        ┆ ---          │\n",
       "│ str              ┆ str         ┆ str            ┆ str        ┆ str          ┆ str         ┆ str            ┆ str        ┆ str          │\n",
       "╞══════════════════╪═════════════╪════════════════╪════════════╪══════════════╪═════════════╪════════════════╪════════════╪══════════════╡\n",
       "│ load_method      ┆ 287,066     ┆ 287,066        ┆ 0          ┆ 0.0%         ┆ 90,072      ┆ 0              ┆ 90,072     ┆ 100.0%       │\n",
       "│ reportDate       ┆ 287,066     ┆ 287,066        ┆ 0          ┆ 0.0%         ┆ 90,072      ┆ 0              ┆ 90,072     ┆ 100.0%       │\n",
       "│ sample_version   ┆ 287,066     ┆ 287,066        ┆ 0          ┆ 0.0%         ┆ 90,072      ┆ 0              ┆ 90,072     ┆ 100.0%       │\n",
       "│ source_file_path ┆ 287,066     ┆ 287,066        ┆ 0          ┆ 0.0%         ┆ 90,072      ┆ 0              ┆ 90,072     ┆ 100.0%       │\n",
       "│ temporal_bin     ┆ 287,066     ┆ 287,066        ┆ 0          ┆ 0.0%         ┆ 90,072      ┆ 0              ┆ 90,072     ┆ 100.0%       │\n",
       "└──────────────────┴─────────────┴────────────────┴────────────┴──────────────┴─────────────┴────────────────┴────────────┴──────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyze null values in columns showing Type Diff\n",
    "Load FULL datasets for accurate null analysis\n",
    "\"\"\"\n",
    "\n",
    "# Columns with Type Diff (String vs Null)\n",
    "problem_columns = [\n",
    "    'load_method',\n",
    "    'reportDate', \n",
    "    'sample_version',\n",
    "    'source_file_path',\n",
    "    'temporal_bin'\n",
    "]\n",
    "\n",
    "# Load FULL datasets (not just 1 row)\n",
    "print(\"Loading full datasets for null analysis...\\n\")\n",
    "\n",
    "hist_uri = f\"s3://{BUCKET}/{HIST_PATH}\"\n",
    "incr_uri = https_to_s3(INCR_SDK_URI)\n",
    "\n",
    "hist_full = pl.read_parquet(hist_uri, storage_options=storage_options)\n",
    "incr_full = pl.read_parquet(incr_uri, storage_options=storage_options)\n",
    "\n",
    "print(f\"Historical file: {len(hist_full):,} total rows\")\n",
    "print(f\"Incremental (SDK) file: {len(incr_full):,} total rows\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze each column\n",
    "results = []\n",
    "\n",
    "for col in problem_columns:\n",
    "    # Historical stats\n",
    "    hist_exists = col in hist_full.columns\n",
    "    if hist_exists:\n",
    "        hist_total = len(hist_full)\n",
    "        hist_null = hist_full[col].is_null().sum()\n",
    "        hist_not_null = hist_total - hist_null\n",
    "        hist_null_pct = (hist_null / hist_total) * 100\n",
    "    else:\n",
    "        hist_total = hist_null = hist_not_null = hist_null_pct = \"N/A\"\n",
    "    \n",
    "    # Incremental stats\n",
    "    incr_exists = col in incr_full.columns\n",
    "    if incr_exists:\n",
    "        incr_total = len(incr_full)\n",
    "        incr_null = incr_full[col].is_null().sum()\n",
    "        incr_not_null = incr_total - incr_null\n",
    "        incr_null_pct = (incr_null / incr_total) * 100\n",
    "    else:\n",
    "        incr_total = incr_null = incr_not_null = incr_null_pct = \"N/A\"\n",
    "    \n",
    "    results.append({\n",
    "        'Column': col,\n",
    "        'Hist: Total': f\"{hist_total:,}\" if isinstance(hist_total, int) else hist_total,\n",
    "        'Hist: Not Null': f\"{hist_not_null:,}\" if isinstance(hist_not_null, int) else hist_not_null,\n",
    "        'Hist: Null': f\"{hist_null:,}\" if isinstance(hist_null, int) else hist_null,\n",
    "        'Hist: Null %': f\"{hist_null_pct:.1f}%\" if isinstance(hist_null_pct, float) else hist_null_pct,\n",
    "        'Incr: Total': f\"{incr_total:,}\" if isinstance(incr_total, int) else incr_total,\n",
    "        'Incr: Not Null': f\"{incr_not_null:,}\" if isinstance(incr_not_null, int) else incr_not_null,\n",
    "        'Incr: Null': f\"{incr_null:,}\" if isinstance(incr_null, int) else incr_null,\n",
    "        'Incr: Null %': f\"{incr_null_pct:.1f}%\" if isinstance(incr_null_pct, float) else incr_null_pct,\n",
    "    })\n",
    "\n",
    "null_analysis = pl.DataFrame(results)\n",
    "\n",
    "print(\"\\nNull Value Analysis (Full Data)\\n\")\n",
    "null_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4242303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Values and Unique Counts from Non-Null Rows\n",
      "\n",
      "======================================================================\n",
      "\n",
      "load_method:\n",
      "----------------------------------------------------------------------\n",
      "  Historical:\n",
      "    - Non-null rows: 287,066\n",
      "    - Unique values: 2\n",
      "    - Sample values:\n",
      "        stratified_sampling\n",
      "        stratified_sampling\n",
      "        stratified_sampling\n",
      "        stratified_sampling\n",
      "        stratified_sampling\n",
      "  Incremental (SDK):\n",
      "    - Non-null rows: 0\n",
      "    - Unique values: 0\n",
      "    - ALL NULL\n",
      "\n",
      "reportDate:\n",
      "----------------------------------------------------------------------\n",
      "  Historical:\n",
      "    - Non-null rows: 287,066\n",
      "    - Unique values: 124\n",
      "    - Sample values:\n",
      "        2020-12-31\n",
      "        2020-12-31\n",
      "        2020-12-31\n",
      "        2020-12-31\n",
      "        2020-12-31\n",
      "  Incremental (SDK):\n",
      "    - Non-null rows: 0\n",
      "    - Unique values: 0\n",
      "    - ALL NULL\n",
      "\n",
      "sample_version:\n",
      "----------------------------------------------------------------------\n",
      "  Historical:\n",
      "    - Non-null rows: 287,066\n",
      "    - Unique values: 2\n",
      "    - Sample values:\n",
      "        v1.0_75companies_1M\n",
      "        v1.0_75companies_1M\n",
      "        v1.0_75companies_1M\n",
      "        v1.0_75companies_1M\n",
      "        v1.0_75companies_1M\n",
      "  Incremental (SDK):\n",
      "    - Non-null rows: 0\n",
      "    - Unique values: 0\n",
      "    - ALL NULL\n",
      "\n",
      "source_file_path:\n",
      "----------------------------------------------------------------------\n",
      "  Historical:\n",
      "    - Non-null rows: 287,066\n",
      "    - Unique values: 6\n",
      "    - Sample values:\n",
      "        D:/JoelDesktop folds_24/NEU FALL2025/MLops IE7374 Project/finrag-insights-mlops/data/exports/sec_filings_large_full.parquet\n",
      "        D:/JoelDesktop folds_24/NEU FALL2025/MLops IE7374 Project/finrag-insights-mlops/data/exports/sec_filings_large_full.parquet\n",
      "        D:/JoelDesktop folds_24/NEU FALL2025/MLops IE7374 Project/finrag-insights-mlops/data/exports/sec_filings_large_full.parquet\n",
      "        D:/JoelDesktop folds_24/NEU FALL2025/MLops IE7374 Project/finrag-insights-mlops/data/exports/sec_filings_large_full.parquet\n",
      "        D:/JoelDesktop folds_24/NEU FALL2025/MLops IE7374 Project/finrag-insights-mlops/data/exports/sec_filings_large_full.parquet\n",
      "  Incremental (SDK):\n",
      "    - Non-null rows: 0\n",
      "    - Unique values: 0\n",
      "    - ALL NULL\n",
      "\n",
      "temporal_bin:\n",
      "----------------------------------------------------------------------\n",
      "  Historical:\n",
      "    - Non-null rows: 287,066\n",
      "    - Unique values: 3\n",
      "    - Sample values:\n",
      "        bin_2016_2020\n",
      "        bin_2016_2020\n",
      "        bin_2016_2020\n",
      "        bin_2016_2020\n",
      "        bin_2016_2020\n",
      "  Incremental (SDK):\n",
      "    - Non-null rows: 0\n",
      "    - Unique values: 0\n",
      "    - ALL NULL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show sample values and unique value counts from full dataset\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Values and Unique Counts from Non-Null Rows\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in problem_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Historical samples\n",
    "    if col in hist_full.columns:\n",
    "        hist_not_null = hist_full.filter(pl.col(col).is_not_null())\n",
    "        hist_unique = hist_not_null[col].n_unique() if len(hist_not_null) > 0 else 0\n",
    "        \n",
    "        print(f\"  Historical:\")\n",
    "        print(f\"    - Non-null rows: {len(hist_not_null):,}\")\n",
    "        print(f\"    - Unique values: {hist_unique:,}\")\n",
    "        \n",
    "        if len(hist_not_null) > 0:\n",
    "            hist_samples = hist_not_null[col].head(5)\n",
    "            print(f\"    - Sample values:\")\n",
    "            for val in hist_samples:\n",
    "                print(f\"        {val}\")\n",
    "        else:\n",
    "            print(f\"    - ALL NULL\")\n",
    "    else:\n",
    "        print(f\"  Historical: COLUMN MISSING\")\n",
    "    \n",
    "    # Incremental samples\n",
    "    if col in incr_full.columns:\n",
    "        incr_not_null = incr_full.filter(pl.col(col).is_not_null())\n",
    "        incr_unique = incr_not_null[col].n_unique() if len(incr_not_null) > 0 else 0\n",
    "        \n",
    "        print(f\"  Incremental (SDK):\")\n",
    "        print(f\"    - Non-null rows: {len(incr_not_null):,}\")\n",
    "        print(f\"    - Unique values: {incr_unique:,}\")\n",
    "        \n",
    "        if len(incr_not_null) > 0:\n",
    "            incr_samples = incr_not_null[col].head(5)\n",
    "            print(f\"    - Sample values:\")\n",
    "            for val in incr_samples:\n",
    "                print(f\"        {val}\")\n",
    "        else:\n",
    "            print(f\"    - ALL NULL\")\n",
    "    else:\n",
    "        print(f\"  Incremental: COLUMN MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98cdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentence positions from sentenceID...\n",
      "\n",
      "Malformed sentenceIDs: 0\n",
      "Valid positions: 90,072\n",
      "\n",
      "Testing contiguity within document-section groups...\n",
      "\n",
      "Document-Section Groups Analysis:\n",
      "  Total groups: 334\n",
      "  Contiguous groups: 334 (100.0%)\n",
      "  Broken groups (gaps): 0 (0.0%)\n",
      "\n",
      "Sample Contiguous Groups:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>docID</th><th>section_name</th><th>total_sentences</th><th>min_pos</th><th>max_pos</th><th>unique_positions</th><th>expected_if_contiguous</th><th>is_contiguous</th></tr><tr><td>str</td><td>str</td><td>u32</td><td>i16</td><td>i16</td><td>u32</td><td>i16</td><td>bool</td></tr></thead><tbody><tr><td>&quot;1018724_10-K_2021&quot;</td><td>&quot;Business&quot;</td><td>60</td><td>0</td><td>59</td><td>60</td><td>60</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2021&quot;</td><td>&quot;Management &amp; Discussion and Analysis (MD&amp;A)&quot;</td><td>220</td><td>0</td><td>219</td><td>220</td><td>220</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2021&quot;</td><td>&quot;Quantitative and Qualitative Disclosures About Market Risk&quot;</td><td>26</td><td>0</td><td>25</td><td>26</td><td>26</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2021&quot;</td><td>&quot;Risk Factors&quot;</td><td>186</td><td>0</td><td>185</td><td>186</td><td>186</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2022&quot;</td><td>&quot;Business&quot;</td><td>62</td><td>0</td><td>61</td><td>62</td><td>62</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2022&quot;</td><td>&quot;Management &amp; Discussion and Analysis (MD&amp;A)&quot;</td><td>205</td><td>0</td><td>204</td><td>205</td><td>205</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2022&quot;</td><td>&quot;Quantitative and Qualitative Disclosures About Market Risk&quot;</td><td>27</td><td>0</td><td>26</td><td>27</td><td>27</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2022&quot;</td><td>&quot;Risk Factors&quot;</td><td>193</td><td>0</td><td>192</td><td>193</td><td>193</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2023&quot;</td><td>&quot;Business&quot;</td><td>65</td><td>0</td><td>64</td><td>65</td><td>65</td><td>true</td></tr><tr><td>&quot;1018724_10-K_2023&quot;</td><td>&quot;Management &amp; Discussion and Analysis (MD&amp;A)&quot;</td><td>206</td><td>0</td><td>205</td><td>206</td><td>206</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 8)\n",
       "┌───────────────────┬────────────────────────────────────────────────────────────┬─────────────────┬─────────┬─────────┬──────────────────┬────────────────────────┬───────────────┐\n",
       "│ docID             ┆ section_name                                               ┆ total_sentences ┆ min_pos ┆ max_pos ┆ unique_positions ┆ expected_if_contiguous ┆ is_contiguous │\n",
       "│ ---               ┆ ---                                                        ┆ ---             ┆ ---     ┆ ---     ┆ ---              ┆ ---                    ┆ ---           │\n",
       "│ str               ┆ str                                                        ┆ u32             ┆ i16     ┆ i16     ┆ u32              ┆ i16                    ┆ bool          │\n",
       "╞═══════════════════╪════════════════════════════════════════════════════════════╪═════════════════╪═════════╪═════════╪══════════════════╪════════════════════════╪═══════════════╡\n",
       "│ 1018724_10-K_2021 ┆ Business                                                   ┆ 60              ┆ 0       ┆ 59      ┆ 60               ┆ 60                     ┆ true          │\n",
       "│ 1018724_10-K_2021 ┆ Management & Discussion and Analysis (MD&A)                ┆ 220             ┆ 0       ┆ 219     ┆ 220              ┆ 220                    ┆ true          │\n",
       "│ 1018724_10-K_2021 ┆ Quantitative and Qualitative Disclosures About Market Risk ┆ 26              ┆ 0       ┆ 25      ┆ 26               ┆ 26                     ┆ true          │\n",
       "│ 1018724_10-K_2021 ┆ Risk Factors                                               ┆ 186             ┆ 0       ┆ 185     ┆ 186              ┆ 186                    ┆ true          │\n",
       "│ 1018724_10-K_2022 ┆ Business                                                   ┆ 62              ┆ 0       ┆ 61      ┆ 62               ┆ 62                     ┆ true          │\n",
       "│ 1018724_10-K_2022 ┆ Management & Discussion and Analysis (MD&A)                ┆ 205             ┆ 0       ┆ 204     ┆ 205              ┆ 205                    ┆ true          │\n",
       "│ 1018724_10-K_2022 ┆ Quantitative and Qualitative Disclosures About Market Risk ┆ 27              ┆ 0       ┆ 26      ┆ 27               ┆ 27                     ┆ true          │\n",
       "│ 1018724_10-K_2022 ┆ Risk Factors                                               ┆ 193             ┆ 0       ┆ 192     ┆ 193              ┆ 193                    ┆ true          │\n",
       "│ 1018724_10-K_2023 ┆ Business                                                   ┆ 65              ┆ 0       ┆ 64      ┆ 65               ┆ 65                     ┆ true          │\n",
       "│ 1018724_10-K_2023 ┆ Management & Discussion and Analysis (MD&A)                ┆ 206             ┆ 0       ┆ 205     ┆ 206              ┆ 206                    ┆ true          │\n",
       "└───────────────────┴────────────────────────────────────────────────────────────┴─────────────────┴─────────┴─────────┴──────────────────┴────────────────────────┴───────────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract sentence_pos from sentenceID and validate contiguity\n",
    "Tests if sentence positions are sequential within each document-section group\n",
    "\"\"\"\n",
    "\n",
    "def extract_sentence_position(df: pl.DataFrame, sentenceid_col: str = 'sentenceID') -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract sentence position from sentenceID string.\n",
    "    sentenceID format: {docID}_{section}_{sequence}\n",
    "    Example: \"0001045810_10-K_2020_section_1A_45\" → pos=45\n",
    "    \"\"\"\n",
    "    return df.with_columns([\n",
    "        pl.col(sentenceid_col)\n",
    "          .str.split('_')\n",
    "          .list.last()\n",
    "          .cast(pl.Int16, strict=False)  # NULL on cast failure\n",
    "          .fill_null(-1)                  # -1 = malformed ID\n",
    "          .alias('sentence_pos')\n",
    "    ])\n",
    "\n",
    "# Extract sentence positions for incremental SDK data\n",
    "print(\"Extracting sentence positions from sentenceID...\\n\")\n",
    "incr_with_pos = extract_sentence_position(incr_full)\n",
    "\n",
    "# Check for malformed IDs\n",
    "malformed_count = incr_with_pos.filter(pl.col('sentence_pos') == -1).shape[0]\n",
    "print(f\"Malformed sentenceIDs: {malformed_count:,}\")\n",
    "print(f\"Valid positions: {len(incr_with_pos) - malformed_count:,}\")\n",
    "\n",
    "# Group key: docID + section_name (sentences should be contiguous within these groups)\n",
    "print(\"\\nTesting contiguity within document-section groups...\\n\")\n",
    "\n",
    "contiguity_test = (\n",
    "    incr_with_pos\n",
    "    .filter(pl.col('sentence_pos') != -1)  # Exclude malformed\n",
    "    .group_by(['docID', 'section_name'])\n",
    "    .agg([\n",
    "        pl.len().alias('total_sentences'),\n",
    "        pl.col('sentence_pos').min().alias('min_pos'),\n",
    "        pl.col('sentence_pos').max().alias('max_pos'),\n",
    "        pl.col('sentence_pos').n_unique().alias('unique_positions')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Expected count if contiguous: max - min + 1\n",
    "        (pl.col('max_pos') - pl.col('min_pos') + 1).alias('expected_if_contiguous'),\n",
    "        # Are they contiguous?\n",
    "        (pl.col('unique_positions') == (pl.col('max_pos') - pl.col('min_pos') + 1)).alias('is_contiguous')\n",
    "    ])\n",
    "    .sort(['docID', 'section_name'])\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "total_groups = len(contiguity_test)\n",
    "contiguous_groups = contiguity_test.filter(pl.col('is_contiguous')).shape[0]\n",
    "broken_groups = total_groups - contiguous_groups\n",
    "\n",
    "print(f\"Document-Section Groups Analysis:\")\n",
    "print(f\"  Total groups: {total_groups:,}\")\n",
    "print(f\"  Contiguous groups: {contiguous_groups:,} ({contiguous_groups/total_groups*100:.1f}%)\")\n",
    "print(f\"  Broken groups (gaps): {broken_groups:,} ({broken_groups/total_groups*100:.1f}%)\")\n",
    "\n",
    "# Show sample of contiguous groups\n",
    "print(\"\\nSample Contiguous Groups:\\n\")\n",
    "contiguous_sample = (\n",
    "    contiguity_test\n",
    "    .filter(pl.col('is_contiguous'))\n",
    "    .head(10)\n",
    ")\n",
    "contiguous_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "045a671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NO GAPS FOUND - All groups are perfectly contiguous!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyze groups with gaps in sentence positions\n",
    "Shows which document-sections have non-contiguous sequences\n",
    "\"\"\"\n",
    "\n",
    "# Filter to groups with gaps\n",
    "broken_analysis = (\n",
    "    contiguity_test\n",
    "    .filter(~pl.col('is_contiguous'))\n",
    "    .with_columns([\n",
    "        (pl.col('expected_if_contiguous') - pl.col('unique_positions')).alias('missing_positions')\n",
    "    ])\n",
    "    .sort('missing_positions', descending=True)\n",
    ")\n",
    "\n",
    "if len(broken_analysis) > 0:\n",
    "    print(f\"Groups with Gaps ({len(broken_analysis):,} total):\\n\")\n",
    "    print(\"Top 10 groups with most missing positions:\\n\")\n",
    "    broken_analysis.head(10)\n",
    "else:\n",
    "    print(\"✓ NO GAPS FOUND - All groups are perfectly contiguous!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e986567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gaps to analyze - all sequences are contiguous!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show actual sentence positions for a broken group to visualize the gap\n",
    "\"\"\"\n",
    "\n",
    "if len(broken_analysis) > 0:\n",
    "    # Pick the worst offender\n",
    "    worst_group = broken_analysis.head(1)\n",
    "    worst_docid = worst_group['docID'][0]\n",
    "    worst_section = worst_group['section_name'][0]\n",
    "    \n",
    "    print(f\"Detailed Gap Analysis for Worst Case:\")\n",
    "    print(f\"  docID: {worst_docid}\")\n",
    "    print(f\"  section: {worst_section}\\n\")\n",
    "    \n",
    "    # Get all positions for this group\n",
    "    gap_detail = (\n",
    "        incr_with_pos\n",
    "        .filter(\n",
    "            (pl.col('docID') == worst_docid) & \n",
    "            (pl.col('section_name') == worst_section) &\n",
    "            (pl.col('sentence_pos') != -1)\n",
    "        )\n",
    "        .select(['sentenceID', 'sentence_pos', 'sentence'])\n",
    "        .sort('sentence_pos')\n",
    "    )\n",
    "    \n",
    "    positions = gap_detail['sentence_pos'].to_list()\n",
    "    min_pos = min(positions)\n",
    "    max_pos = max(positions)\n",
    "    expected_range = set(range(min_pos, max_pos + 1))\n",
    "    actual_positions = set(positions)\n",
    "    missing = sorted(expected_range - actual_positions)\n",
    "    \n",
    "    print(f\"Position range: {min_pos} to {max_pos}\")\n",
    "    print(f\"Expected positions: {len(expected_range)}\")\n",
    "    print(f\"Actual positions: {len(actual_positions)}\")\n",
    "    print(f\"Missing positions: {missing[:20]}\")  # Show first 20 gaps\n",
    "    \n",
    "    print(f\"\\nSample sentences from this group:\\n\")\n",
    "    gap_detail.head(10)\n",
    "else:\n",
    "    print(\"No gaps to analyze - all sequences are contiguous!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485ad0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22935ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting reportDate column...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "HISTORICAL DATA:\n",
      "----------------------------------------------------------------------\n",
      "Sample of 10 rows:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cik</th><th>name</th><th>report_year</th><th>reportDate</th><th>filingDate</th><th>docID</th><th>sentenceID</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_0&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_1&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_10&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_11&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_12&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_13&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_14&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_15&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_16&quot;</td></tr><tr><td>&quot;0000034088&quot;</td><td>&quot;EXXON MOBIL CORP&quot;</td><td>2020</td><td>&quot;2020-12-31&quot;</td><td>&quot;2021-02-24&quot;</td><td>&quot;0000034088_10-K_2020&quot;</td><td>&quot;0000034088_10-K_2020_section_1_17&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 7)\n",
       "┌────────────┬──────────────────┬─────────────┬────────────┬────────────┬──────────────────────┬───────────────────────────────────┐\n",
       "│ cik        ┆ name             ┆ report_year ┆ reportDate ┆ filingDate ┆ docID                ┆ sentenceID                        │\n",
       "│ ---        ┆ ---              ┆ ---         ┆ ---        ┆ ---        ┆ ---                  ┆ ---                               │\n",
       "│ str        ┆ str              ┆ i64         ┆ str        ┆ str        ┆ str                  ┆ str                               │\n",
       "╞════════════╪══════════════════╪═════════════╪════════════╪════════════╪══════════════════════╪═══════════════════════════════════╡\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_0  │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_1  │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_10 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_11 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_12 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_13 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_14 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_15 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_16 │\n",
       "│ 0000034088 ┆ EXXON MOBIL CORP ┆ 2020        ┆ 2020-12-31 ┆ 2021-02-24 ┆ 0000034088_10-K_2020 ┆ 0000034088_10-K_2020_section_1_17 │\n",
       "└────────────┴──────────────────┴─────────────┴────────────┴────────────┴──────────────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple inspection of reportDate column to understand the null issue\n",
    "Check both Historical and Incremental (SDK) data\n",
    "\"\"\"\n",
    "\n",
    "print(\"Inspecting reportDate column...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Historical - Sample rows with reportDate\n",
    "print(\"\\nHISTORICAL DATA:\")\n",
    "print(\"-\"*70)\n",
    "hist_sample = hist_full.select([\n",
    "    'cik',\n",
    "    'name', \n",
    "    'report_year',\n",
    "    'reportDate',\n",
    "    'filingDate',\n",
    "    'docID',\n",
    "    'sentenceID'\n",
    "]).head(10)\n",
    "\n",
    "print(f\"Sample of {len(hist_sample)} rows:\\n\")\n",
    "hist_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "INCREMENTAL (SDK) DATA:\n",
      "----------------------------------------------------------------------\n",
      "Sample of 10 rows:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cik</th><th>name</th><th>report_year</th><th>reportDate</th><th>filingDate</th><th>docID</th><th>sentenceID</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>null</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_0&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_1&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_2&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_3&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_4&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_5&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_6&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_7&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_8&quot;</td></tr><tr><td>&quot;320193&quot;</td><td>&quot;Apple Inc.&quot;</td><td>2024</td><td>null</td><td>&quot;2024-11-01&quot;</td><td>&quot;320193_10-K_2024&quot;</td><td>&quot;320193_10-K_2024_section_0_9&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 7)\n",
       "┌────────┬────────────┬─────────────┬────────────┬────────────┬──────────────────┬──────────────────────────────┐\n",
       "│ cik    ┆ name       ┆ report_year ┆ reportDate ┆ filingDate ┆ docID            ┆ sentenceID                   │\n",
       "│ ---    ┆ ---        ┆ ---         ┆ ---        ┆ ---        ┆ ---              ┆ ---                          │\n",
       "│ str    ┆ str        ┆ i64         ┆ null       ┆ str        ┆ str              ┆ str                          │\n",
       "╞════════╪════════════╪═════════════╪════════════╪════════════╪══════════════════╪══════════════════════════════╡\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_0 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_1 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_2 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_3 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_4 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_5 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_6 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_7 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_8 │\n",
       "│ 320193 ┆ Apple Inc. ┆ 2024        ┆ null       ┆ 2024-11-01 ┆ 320193_10-K_2024 ┆ 320193_10-K_2024_section_0_9 │\n",
       "└────────┴────────────┴─────────────┴────────────┴────────────┴──────────────────┴──────────────────────────────┘"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Incremental SDK - Sample rows with reportDate\n",
    "print(\"\\nINCREMENTAL (SDK) DATA:\")\n",
    "print(\"-\"*70)\n",
    "incr_sample = incr_full.select([\n",
    "    'cik',\n",
    "    'name',\n",
    "    'report_year', \n",
    "    'reportDate',\n",
    "    'filingDate',\n",
    "    'docID',\n",
    "    'sentenceID'\n",
    "]).head(10)\n",
    "\n",
    "print(f\"Sample of {len(incr_sample)} rows:\\n\")\n",
    "incr_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcefcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6a949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32dcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ad167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c436dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a291d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
