name: Src Metrics CI

on:
  push:
    paths:
      - 'DataPipeline/src_metrics/**'

jobs:
  build-and-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f DataPipeline/src_metrics/requirements.txt ]; then pip install -r DataPipeline/src_metrics/requirements.txt; fi
        pip install flake8 pytest

    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 DataPipeline/src_metrics --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings.
        flake8 DataPipeline/src_metrics --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    # - name: Run Tests
    #   run: |
    #     # Assuming tests are in DataPipeline/tests and we want to run them
    #     # If there are specific tests for src_metrics, we might want to target them more specifically
    #     # For now, running all tests in DataPipeline/tests as a sanity check
    #     if [ -d DataPipeline/tests ]; then
    #       pytest DataPipeline/tests
    #     else
    #       echo "No tests directory found at DataPipeline/tests"
    #     fi

  deploy:
    needs: build-and-test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    
    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f DataPipeline/src_metrics/requirements.txt ]; then pip install -r DataPipeline/src_metrics/requirements.txt; fi

    - name: Run Metrics Pipeline
      env:
        EDGAR_IDENTITY: ${{ secrets.EDGAR_IDENTITY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
        FINRAG_S3_BUCKET: ${{ secrets.FINRAG_S3_BUCKET || 'sentence-data-ingestion' }}
        ANALYTICAL_LAYER_BASE_DIR: 'metrics_output'
        PYTHONPATH: ${{ github.workspace }}/DataPipeline/src_metrics
      run: |
        cd DataPipeline/src_metrics
        python run_pipeline_gh.py